{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13155814,
          "sourceType": "datasetVersion",
          "datasetId": 8335592,
          "isSourceIdPinned": false
        },
        {
          "sourceId": 14156874,
          "sourceType": "datasetVersion",
          "datasetId": 9023315,
          "isSourceIdPinned": false
        },
        {
          "sourceId": 14605080,
          "sourceType": "datasetVersion",
          "datasetId": 9329049,
          "isSourceIdPinned": false
        },
        {
          "sourceId": 14865230,
          "sourceType": "datasetVersion",
          "datasetId": 9509186
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================================\n",
        "# KAGGLE NOTEBOOK: TRUE FL + GA-FELCM + PVTv2-B2 (FUSION) — 12 Clients (3x4 datasets)\n",
        "# DS1/DS2/DS3: kagglehub download\n",
        "# DS4: Kaggle dataset input path (YOU UPLOADED) -> /kaggle/input/datasets/mdzubayerahmadshibly/ds4mine\n",
        "# Outputs: /kaggle/working/outputs/\n",
        "# ======================================================================================\n",
        "\n",
        "import os, time, math, random, sys, subprocess, hashlib\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    log_loss, confusion_matrix, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "def pip_install(pkg):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "# timm\n",
        "try:\n",
        "    import timm\n",
        "except Exception:\n",
        "    pip_install(\"timm\")\n",
        "    import timm\n",
        "\n",
        "# kagglehub\n",
        "try:\n",
        "    import kagglehub\n",
        "except Exception:\n",
        "    pip_install(\"kagglehub\")\n",
        "    import kagglehub\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except Exception:\n",
        "    display = print\n",
        "\n",
        "# -------------------------\n",
        "# Repro / device\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(\"=\" * 92)\n",
        "print(\"KAGGLE: TRUE FL + GA-FELCM + PVTv2-B2 (FUSION) — 12 Clients (3x4 datasets) | AUG=ON\")\n",
        "print(\"=\" * 92)\n",
        "print(f\"DEVICE: {DEVICE} | torch={torch.__version__}\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "CFG = {\n",
        "    \"clients_per_dataset\": 3,\n",
        "    \"clients_total\": 12,\n",
        "    \"rounds\": 12,\n",
        "    \"local_epochs\": 2,\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"warmup_epochs\": 1,\n",
        "    \"label_smoothing\": 0.08,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"fedprox_mu\": 0.01,\n",
        "    \"img_size\": 224 if torch.cuda.is_available() else 160,\n",
        "    \"batch_size\": 20 if torch.cuda.is_available() else 10,\n",
        "    \"num_workers\": 2 if torch.cuda.is_available() else 0,\n",
        "    \"global_val_frac\": 0.15,\n",
        "    \"test_frac\": 0.15,\n",
        "    \"client_val_frac\": 0.12,\n",
        "    \"client_tune_frac\": 0.12,\n",
        "    \"min_per_class_per_client\": 5,\n",
        "    \"dirichlet_alpha\": 0.35,\n",
        "    \"use_preprocessing\": True,\n",
        "    \"use_ga\": True,\n",
        "    \"ga_pop\": 10,\n",
        "    \"ga_gens\": 5,\n",
        "    \"ga_elites\": 3,\n",
        "    \"elite_pool_max\": 18,\n",
        "    \"use_augmentation\": True,\n",
        "    \"cond_dim\": 128,\n",
        "    \"head_dropout\": 0.3,\n",
        "    \"unfreeze_after_round\": 3,\n",
        "    \"unfreeze_lr_mult\": 0.10,\n",
        "    \"unfreeze_tail_frac\": 0.17,\n",
        "    \"quick_hash_subset_per_split\": 300,\n",
        "    \"preproc_val_sample_n\": 500,\n",
        "    \"before_after_n\": 12,\n",
        "}\n",
        "\n",
        "OUTDIR = \"/kaggle/working/outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "MODEL_PATH = os.path.join(OUTDIR, \"FL_GAFELCM_PVTv2B2_FUSION_checkpoint.pth\")\n",
        "CSV_PATH = os.path.join(OUTDIR, \"ALL_OUTPUTS_AND_METRICS.csv\")\n",
        "\n",
        "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\")\n",
        "\n",
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1)\n",
        "\n",
        "ALL_ROWS = []\n",
        "\n",
        "def add_table_to_csv(df, table_name):\n",
        "    df2 = df.copy()\n",
        "    df2.insert(0, \"table_name\", table_name)\n",
        "    for _, row in df2.iterrows():\n",
        "        ALL_ROWS.append(row.to_dict())\n",
        "\n",
        "def print_table(df, title):\n",
        "    print(\"\\n\" + \"-\" * 92)\n",
        "    print(title)\n",
        "    print(\"-\" * 92)\n",
        "    display(df)\n",
        "\n",
        "# -------------------------\n",
        "# Helper: resolve nested roots\n",
        "# -------------------------\n",
        "def pick_probable_root(base_dir: str) -> str:\n",
        "    \"\"\"\n",
        "    Some downloads wrap data inside a single folder. This tries to descend\n",
        "    while there is exactly one subfolder and no images at current level.\n",
        "    \"\"\"\n",
        "    base_dir = str(base_dir)\n",
        "    cur = base_dir\n",
        "    for _ in range(6):\n",
        "        # any images anywhere under cur?\n",
        "        any_img = any(str(p).lower().endswith(IMG_EXTS) for p in Path(cur).rglob(\"*\"))\n",
        "        if any_img:\n",
        "            return cur\n",
        "        subs = [p for p in Path(cur).iterdir() if p.is_dir()]\n",
        "        if len(subs) == 1:\n",
        "            cur = str(subs[0])\n",
        "            continue\n",
        "        return cur\n",
        "    return cur\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 0: DOWNLOAD DS1/DS2/DS3 (kagglehub) + DS4 (Kaggle input path)\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 0: DATASETS (DS1/2/3 via kagglehub) + DS4 from Kaggle input path\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "# NOTE: Kagglehub needs Internet ON in Kaggle notebook settings for downloads.\n",
        "ds1_path = kagglehub.dataset_download(\"alamshihab075/brain-tumor-mri-dataset-for-deep-learning\")\n",
        "ds2_path = kagglehub.dataset_download(\"zehrakucuker/brain-tumor-mri-images-classification-dataset\")\n",
        "ds3_path = kagglehub.dataset_download(\"chubskuy/brain-tumor-image\")\n",
        "\n",
        "# Your uploaded Kaggle dataset path:\n",
        "ds4_path = \"/kaggle/input/datasets/mdzubayerahmadshibly/ds4mine\"\n",
        "\n",
        "ds1_path = pick_probable_root(ds1_path)\n",
        "ds2_path = pick_probable_root(ds2_path)\n",
        "ds3_path = pick_probable_root(ds3_path)\n",
        "ds4_path = pick_probable_root(ds4_path)\n",
        "\n",
        "print(\"✅ DS1:\", ds1_path)\n",
        "print(\"✅ DS2:\", ds2_path)\n",
        "print(\"✅ DS3:\", ds3_path)\n",
        "print(\"✅ DS4:\", ds4_path)\n",
        "\n",
        "CFG[\"ds1_base\"] = ds1_path\n",
        "CFG[\"ds2_base\"] = ds2_path\n",
        "CFG[\"ds3_base\"] = ds3_path\n",
        "CFG[\"ds4_base\"] = ds4_path\n",
        "\n",
        "DATASET_NAMES = [\"ds1\", \"ds2\", \"ds3\", \"ds4\"]\n",
        "DATASET_BASES = {\n",
        "    \"ds1\": CFG[\"ds1_base\"],\n",
        "    \"ds2\": CFG[\"ds2_base\"],\n",
        "    \"ds3\": CFG[\"ds3_base\"],\n",
        "    \"ds4\": CFG[\"ds4_base\"],\n",
        "}\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 1: DISCOVER + MERGE DATASET IMAGES BY CLASS\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 1: DISCOVER + MERGE DATASET IMAGES BY CLASS\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "def norm_label(name: str):\n",
        "    s = str(name).strip().lower().replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "    if \"glioma\" in s:\n",
        "        return \"glioma\"\n",
        "    if \"meningioma\" in s:\n",
        "        return \"meningioma\"\n",
        "    if \"pituitary\" in s:\n",
        "        return \"pituitary\"\n",
        "    if \"normal\" in s or \"no tumor\" in s or \"notumor\" in s or \"no tumour\" in s:\n",
        "        return \"notumor\"\n",
        "    return None\n",
        "\n",
        "def infer_label_from_path(path):\n",
        "    parts = Path(path).parts\n",
        "    for part in reversed(parts):\n",
        "        lab = norm_label(part)\n",
        "        if lab is not None:\n",
        "            return lab\n",
        "    return None\n",
        "\n",
        "def build_df_from_dataset_tree(base_dir, source_name):\n",
        "    rows = []\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for fn in files:\n",
        "            if not fn.lower().endswith(IMG_EXTS):\n",
        "                continue\n",
        "            p = os.path.join(root, fn)\n",
        "            lab = infer_label_from_path(p)\n",
        "            if lab is None:\n",
        "                continue\n",
        "            rows.append({\"path\": p, \"label\": lab, \"source\": source_name})\n",
        "\n",
        "    if not rows:\n",
        "        raise RuntimeError(\n",
        "            f\"No class-labeled images found under {base_dir}. Expected path segments containing \"\n",
        "            f\"glioma/meningioma/pituitary/normal(notumor).\"\n",
        "        )\n",
        "\n",
        "    dfm = pd.DataFrame(rows).dropna().reset_index(drop=True)\n",
        "    dfm[\"path\"] = dfm[\"path\"].astype(str)\n",
        "    dfm[\"label\"] = dfm[\"label\"].astype(str)\n",
        "    dfm[\"source\"] = dfm[\"source\"].astype(str)\n",
        "    dfm = dfm.drop_duplicates(subset=[\"path\"]).reset_index(drop=True)\n",
        "    dfm[\"filename\"] = dfm[\"path\"].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "    counts = dfm[\"label\"].value_counts().reindex([\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"], fill_value=0)\n",
        "    print(f\"{source_name}: total images = {len(dfm)} | \" +\n",
        "          \", \".join([f\"{k}:{int(v)}\" for k, v in counts.items()]))\n",
        "    return dfm\n",
        "\n",
        "all_dfs = {}\n",
        "for ds_name in DATASET_NAMES:\n",
        "    all_dfs[ds_name] = build_df_from_dataset_tree(DATASET_BASES[ds_name], ds_name)\n",
        "\n",
        "df1, df2, df3, df4 = all_dfs[\"ds1\"], all_dfs[\"ds2\"], all_dfs[\"ds3\"], all_dfs[\"ds4\"]\n",
        "\n",
        "labels = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "label2id = {l: i for i, l in enumerate(labels)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "NUM_CLASSES = len(labels)\n",
        "\n",
        "def enforce_labels(df_):\n",
        "    df_ = df_.copy()\n",
        "    df_[\"label\"] = df_[\"label\"].astype(str).str.strip().str.lower()\n",
        "    df_ = df_[df_[\"label\"].isin(set(labels))].reset_index(drop=True)\n",
        "    df_[\"y\"] = df_[\"label\"].map(label2id).astype(int)\n",
        "    return df_\n",
        "\n",
        "df1 = enforce_labels(df1)\n",
        "df2 = enforce_labels(df2)\n",
        "df3 = enforce_labels(df3)\n",
        "df4 = enforce_labels(df4)\n",
        "all_dfs = {\"ds1\": df1, \"ds2\": df2, \"ds3\": df3, \"ds4\": df4}\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 2: TRAIN/VAL/TEST SPLIT\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 2: TRAIN/VAL/TEST SPLIT (PER DATASET)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "def split_dataset(df_):\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df_,\n",
        "        test_size=(CFG[\"global_val_frac\"] + CFG[\"test_frac\"]),\n",
        "        stratify=df_[\"y\"],\n",
        "        random_state=SEED,\n",
        "    )\n",
        "    val_rel = CFG[\"global_val_frac\"] / (CFG[\"global_val_frac\"] + CFG[\"test_frac\"])\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=(1 - val_rel),\n",
        "        stratify=temp_df[\"y\"],\n",
        "        random_state=SEED,\n",
        "    )\n",
        "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
        "\n",
        "train_frames, val_frames, test_frames = {}, {}, {}\n",
        "for ds_name in DATASET_NAMES:\n",
        "    tr, va, te = split_dataset(all_dfs[ds_name])\n",
        "    train_frames[ds_name] = tr\n",
        "    val_frames[ds_name] = va\n",
        "    test_frames[ds_name] = te\n",
        "    print(f\"{ds_name.upper()} TRAIN={len(tr)} | VAL={len(va)} | TEST={len(te)}\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 2.5: LEAKAGE CHECKS\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 2.5: SANITY / LEAKAGE CHECKS (PER DATASET)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "def split_overlap_checks(train_df, val_df, test_df):\n",
        "    tr = set(train_df[\"path\"].tolist())\n",
        "    va = set(val_df[\"path\"].tolist())\n",
        "    te = set(test_df[\"path\"].tolist())\n",
        "    checks = {\n",
        "        \"path_overlap_train_val\": len(tr.intersection(va)),\n",
        "        \"path_overlap_train_test\": len(tr.intersection(te)),\n",
        "        \"path_overlap_val_test\": len(va.intersection(te)),\n",
        "        \"unique_paths_train\": len(tr),\n",
        "        \"unique_paths_val\": len(va),\n",
        "        \"unique_paths_test\": len(te),\n",
        "    }\n",
        "    trf = set(train_df[\"filename\"].tolist())\n",
        "    vaf = set(val_df[\"filename\"].tolist())\n",
        "    tef = set(test_df[\"filename\"].tolist())\n",
        "    checks.update(\n",
        "        {\n",
        "            \"filename_overlap_train_val\": len(trf.intersection(vaf)),\n",
        "            \"filename_overlap_train_test\": len(trf.intersection(tef)),\n",
        "            \"filename_overlap_val_test\": len(vaf.intersection(tef)),\n",
        "        }\n",
        "    )\n",
        "    return checks\n",
        "\n",
        "def md5_file(path, max_bytes=2_000_000):\n",
        "    h = hashlib.md5()\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            h.update(f.read(max_bytes))\n",
        "        return h.hexdigest()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def quick_hash_subset(frame, n=300):\n",
        "    n = min(n, len(frame))\n",
        "    if n <= 0:\n",
        "        return set()\n",
        "    idx = np.random.choice(len(frame), size=n, replace=False)\n",
        "    hashes = []\n",
        "    for i in idx:\n",
        "        hv = md5_file(frame.iloc[i][\"path\"])\n",
        "        if hv is not None:\n",
        "            hashes.append(hv)\n",
        "    return set(hashes)\n",
        "\n",
        "def leakage_report(name, tr, va, te):\n",
        "    over = split_overlap_checks(tr, va, te)\n",
        "    leak_df = pd.DataFrame([over])\n",
        "\n",
        "    n_hash = int(CFG[\"quick_hash_subset_per_split\"])\n",
        "    trh = quick_hash_subset(tr, n_hash)\n",
        "    vah = quick_hash_subset(va, n_hash)\n",
        "    teh = quick_hash_subset(te, n_hash)\n",
        "\n",
        "    hash_over = {\n",
        "        \"subset_hash_train_val\": len(trh.intersection(vah)),\n",
        "        \"subset_hash_train_test\": len(trh.intersection(teh)),\n",
        "        \"subset_hash_val_test\": len(vah.intersection(teh)),\n",
        "        \"subset_hash_n_train\": len(trh),\n",
        "        \"subset_hash_n_val\": len(vah),\n",
        "        \"subset_hash_n_test\": len(teh),\n",
        "    }\n",
        "    leak_df = pd.concat([leak_df, pd.DataFrame([hash_over])], axis=1)\n",
        "    print_table(leak_df, f\"Leakage / Sanity Summary — {name}\")\n",
        "    add_table_to_csv(leak_df, f\"leakage_sanity_{name}\")\n",
        "\n",
        "for ds_name in DATASET_NAMES:\n",
        "    leakage_report(ds_name, train_frames[ds_name], val_frames[ds_name], test_frames[ds_name])\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 3: NON-IID CLIENT PARTITIONING\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 3: NON-IID CLIENT PARTITIONING (3 clients per dataset => 12 total)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "def make_clients_non_iid(train_df, n_clients, num_classes, min_per_class=5, alpha=0.35):\n",
        "    y = train_df[\"y\"].values\n",
        "    idx_by_class = {c: np.where(y == c)[0].tolist() for c in range(num_classes)}\n",
        "    for c in idx_by_class:\n",
        "        random.shuffle(idx_by_class[c])\n",
        "\n",
        "    client_indices = [[] for _ in range(n_clients)]\n",
        "\n",
        "    # guarantee some per-class\n",
        "    for c in range(num_classes):\n",
        "        idxs = idx_by_class[c]\n",
        "        feasible = min(min_per_class, max(1, len(idxs) // n_clients))\n",
        "        for k in range(n_clients):\n",
        "            take = idxs[:feasible]\n",
        "            idxs = idxs[feasible:]\n",
        "            client_indices[k].extend(take)\n",
        "        idx_by_class[c] = idxs\n",
        "\n",
        "    # dirichlet remainder\n",
        "    for c in range(num_classes):\n",
        "        idxs = idx_by_class[c]\n",
        "        if len(idxs) == 0:\n",
        "            continue\n",
        "        props = np.random.dirichlet([alpha] * n_clients)\n",
        "        counts = (props * len(idxs)).astype(int)\n",
        "        diff = len(idxs) - counts.sum()\n",
        "        counts[np.argmax(props)] += diff\n",
        "\n",
        "        start = 0\n",
        "        for k in range(n_clients):\n",
        "            client_indices[k].extend(idxs[start: start + counts[k]])\n",
        "            start += counts[k]\n",
        "\n",
        "    for k in range(n_clients):\n",
        "        random.shuffle(client_indices[k])\n",
        "    return client_indices\n",
        "\n",
        "def robust_client_splits(train_df, indices, val_frac, tune_frac):\n",
        "    idxs = np.array(indices, dtype=int)\n",
        "    if len(idxs) < 3:\n",
        "        return idxs.tolist(), idxs.tolist(), idxs.tolist()\n",
        "\n",
        "    yk = train_df.loc[idxs, \"y\"].values\n",
        "    if len(np.unique(yk)) < 2 or len(idxs) < 20:\n",
        "        n_tune = max(1, int(round(len(idxs) * tune_frac)))\n",
        "        n_tune = min(n_tune, max(1, len(idxs) - 2))\n",
        "        tune_idx = idxs[:n_tune]\n",
        "        rem_idx = idxs[n_tune:]\n",
        "    else:\n",
        "        rem_idx, tune_idx = train_test_split(\n",
        "            idxs, test_size=tune_frac, stratify=yk, random_state=SEED\n",
        "        )\n",
        "\n",
        "    if len(rem_idx) < 2:\n",
        "        return rem_idx.tolist(), tune_idx.tolist(), rem_idx.tolist()\n",
        "\n",
        "    yk2 = train_df.loc[rem_idx, \"y\"].values\n",
        "    if len(np.unique(yk2)) < 2 or len(rem_idx) < 12:\n",
        "        n_val = max(1, int(round(len(rem_idx) * val_frac)))\n",
        "        n_val = min(n_val, max(1, len(rem_idx) - 1))\n",
        "        val_idx = rem_idx[:n_val]\n",
        "        train_idx = rem_idx[n_val:]\n",
        "    else:\n",
        "        train_idx, val_idx = train_test_split(\n",
        "            rem_idx, test_size=val_frac, stratify=yk2, random_state=SEED\n",
        "        )\n",
        "\n",
        "    if len(train_idx) == 0:\n",
        "        train_idx = val_idx[:]\n",
        "    if len(val_idx) == 0:\n",
        "        val_idx = train_idx[:1]\n",
        "    return train_idx.tolist(), tune_idx.tolist(), val_idx.tolist()\n",
        "\n",
        "n_per_ds = CFG[\"clients_per_dataset\"]\n",
        "client_splits = []\n",
        "client_test_splits = []\n",
        "base_gid = 0\n",
        "\n",
        "for ds_name in DATASET_NAMES:\n",
        "    train_df = train_frames[ds_name]\n",
        "    test_df  = test_frames[ds_name]\n",
        "\n",
        "    client_indices = make_clients_non_iid(\n",
        "        train_df,\n",
        "        n_clients=n_per_ds,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        min_per_class=CFG[\"min_per_class_per_client\"],\n",
        "        alpha=CFG[\"dirichlet_alpha\"],\n",
        "    )\n",
        "\n",
        "    for k in range(n_per_ds):\n",
        "        tr, tune, va = robust_client_splits(train_df, client_indices[k], CFG[\"client_val_frac\"], CFG[\"client_tune_frac\"])\n",
        "        gid = base_gid + k\n",
        "        client_splits.append((ds_name, k, gid, tr, tune, va))\n",
        "        print(f\"{ds_name.upper()} Client {k} (gid {gid}): train={len(tr)} tune={len(tune)} val={len(va)}\")\n",
        "\n",
        "    idxs = list(range(len(test_df)))\n",
        "    random.shuffle(idxs)\n",
        "    split = np.array_split(idxs, n_per_ds)\n",
        "    for k in range(n_per_ds):\n",
        "        client_test_splits.append((ds_name, k, base_gid + k, split[k].tolist()))\n",
        "\n",
        "    base_gid += n_per_ds\n",
        "\n",
        "gid_to_ds = {gid: ds_name for (ds_name, _, gid, _, _, _) in client_splits}\n",
        "\n",
        "def client_distribution_table():\n",
        "    dist_rows = []\n",
        "    for (ds_name, local_id, gid, tr_idx, tune_idx, val_idx) in client_splits:\n",
        "        df_src = train_frames[ds_name]\n",
        "        counts = df_src.loc[tr_idx, \"label\"].value_counts().reindex(labels, fill_value=0)\n",
        "        row = {\n",
        "            \"client\": f\"client_{gid}\",\n",
        "            \"dataset\": ds_name,\n",
        "            \"total_train\": len(tr_idx),\n",
        "            \"total_tune\": len(tune_idx),\n",
        "            \"total_val\": len(val_idx),\n",
        "        }\n",
        "        row.update({lab: int(counts[lab]) for lab in labels})\n",
        "        dist_rows.append(row)\n",
        "    return pd.DataFrame(dist_rows)\n",
        "\n",
        "dist_df = client_distribution_table()\n",
        "print_table(dist_df, \"Client class distribution (Non-IID, per dataset)\")\n",
        "add_table_to_csv(dist_df, \"client_distribution\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 4: DATA LOADERS\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 4: DATA LOADERS (AUG ON) + IMAGENET NORM\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "def load_rgb(path):\n",
        "    try:\n",
        "        return Image.open(path).convert(\"RGB\")\n",
        "    except Exception:\n",
        "        return Image.new(\"RGB\", (CFG[\"img_size\"], CFG[\"img_size\"]), (128, 128, 128))\n",
        "\n",
        "EVAL_TFMS = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"img_size\"], CFG[\"img_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "if CFG[\"use_augmentation\"]:\n",
        "    TRAIN_TFMS = transforms.Compose([\n",
        "        transforms.Resize((CFG[\"img_size\"], CFG[\"img_size\"])),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(brightness=0.15, contrast=0.15),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "else:\n",
        "    TRAIN_TFMS = EVAL_TFMS\n",
        "\n",
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, frame, indices=None, tfms=None, source_id=0, client_id=0):\n",
        "        self.df = frame\n",
        "        self.indices = indices if indices is not None else list(range(len(frame)))\n",
        "        self.tfms = tfms\n",
        "        self.source_id = int(source_id)\n",
        "        self.client_id = int(client_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        j = self.indices[i]\n",
        "        row = self.df.iloc[j]\n",
        "        img = load_rgb(row[\"path\"])\n",
        "        x = self.tfms(img) if self.tfms is not None else transforms.ToTensor()(img)\n",
        "        y = int(row[\"y\"])\n",
        "        return x, y, row[\"path\"], self.source_id, self.client_id\n",
        "\n",
        "def make_weighted_sampler(frame, indices, num_classes):\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "    ys = frame.loc[indices, \"y\"].values\n",
        "    class_counts = np.bincount(ys, minlength=num_classes)\n",
        "    class_weights = 1.0 / np.clip(class_counts, 1, None)\n",
        "    sample_weights = class_weights[ys]\n",
        "    return WeightedRandomSampler(\n",
        "        weights=torch.DoubleTensor(sample_weights),\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True,\n",
        "    )\n",
        "\n",
        "def make_loader(frame, indices, bs, tfms, shuffle=False, sampler=None, source_id=0, client_id=0):\n",
        "    ds = MRIDataset(frame, indices=indices, tfms=tfms, source_id=source_id, client_id=client_id)\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=bs,\n",
        "        shuffle=(shuffle and sampler is None),\n",
        "        sampler=sampler,\n",
        "        num_workers=CFG[\"num_workers\"],\n",
        "        pin_memory=(DEVICE.type == \"cuda\"),\n",
        "        drop_last=False,\n",
        "        persistent_workers=(CFG[\"num_workers\"] > 0),\n",
        "    )\n",
        "\n",
        "client_loaders = []\n",
        "for (ds_name, local_id, gid, tr_idx, tune_idx, val_idx) in client_splits:\n",
        "    df_src = train_frames[ds_name]\n",
        "    source_id = DATASET_NAMES.index(ds_name)\n",
        "    sampler = make_weighted_sampler(df_src, tr_idx, NUM_CLASSES)\n",
        "\n",
        "    tr_loader = make_loader(df_src, tr_idx, CFG[\"batch_size\"], TRAIN_TFMS,\n",
        "                            shuffle=(sampler is None), sampler=sampler,\n",
        "                            source_id=source_id, client_id=gid)\n",
        "\n",
        "    tune_loader = make_loader(df_src, tune_idx if len(tune_idx) else tr_idx[:max(1, len(tr_idx))],\n",
        "                              CFG[\"batch_size\"], EVAL_TFMS, shuffle=True,\n",
        "                              source_id=source_id, client_id=gid)\n",
        "\n",
        "    val_loader = make_loader(df_src, val_idx if len(val_idx) else tr_idx[:max(1, min(len(tr_idx), CFG[\"batch_size\"]))],\n",
        "                             CFG[\"batch_size\"], EVAL_TFMS, shuffle=False,\n",
        "                             source_id=source_id, client_id=gid)\n",
        "\n",
        "    client_loaders.append((tr_loader, tune_loader, val_loader))\n",
        "\n",
        "client_test_loaders = []\n",
        "for (ds_name, local_id, gid, test_idx) in client_test_splits:\n",
        "    df_src = test_frames[ds_name]\n",
        "    source_id = DATASET_NAMES.index(ds_name)\n",
        "    t_loader = make_loader(df_src, test_idx, CFG[\"batch_size\"], EVAL_TFMS,\n",
        "                           shuffle=False, source_id=source_id, client_id=gid)\n",
        "    client_test_loaders.append((ds_name, local_id, gid, t_loader))\n",
        "\n",
        "print(f\"Augmentation: {'ON ✅' if CFG['use_augmentation'] else 'OFF'}\")\n",
        "print(f\"Preprocessing: {'ON ✅' if CFG['use_preprocessing'] else 'OFF'}\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 5: GA-TUNED ENHANCED FELCM PREPROCESSOR\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 5: GA-TUNED ENHANCED FELCM PREPROCESSOR\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "THETA_FULLFORMS = {\n",
        "    \"gamma\": \"Power transform exponent (γ)\",\n",
        "    \"alpha\": \"Local contrast weight (α)\",\n",
        "    \"beta\": \"Contrast sharpness (β)\",\n",
        "    \"tau\": \"Robust clipping threshold (τ)\",\n",
        "    \"k\": \"Blur kernel size (k) for local contrast map\",\n",
        "    \"sh\": \"Sharpen strength (sh)\",\n",
        "    \"dn\": \"Denoise strength (dn)\",\n",
        "}\n",
        "\n",
        "class EnhancedFELCM(nn.Module):\n",
        "    def __init__(self, gamma=1.0, alpha=0.35, beta=6.0, tau=2.5, blur_k=7, sharpen=0.0, denoise=0.0):\n",
        "        super().__init__()\n",
        "        self.gamma = float(gamma)\n",
        "        self.alpha = float(alpha)\n",
        "        self.beta = float(beta)\n",
        "        self.tau = float(tau)\n",
        "        self.blur_k = int(blur_k)\n",
        "        self.sharpen = float(sharpen)\n",
        "        self.denoise = float(denoise)\n",
        "\n",
        "        lap = torch.tensor([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype=torch.float32)\n",
        "        self.register_buffer(\"lap\", lap.view(1, 1, 3, 3))\n",
        "\n",
        "        sharp = torch.tensor([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=torch.float32)\n",
        "        self.register_buffer(\"sharp_kernel\", sharp.view(1, 1, 3, 3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        eps = 1e-6\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        if self.denoise > 0:\n",
        "            k = 3\n",
        "            x_blur = F.avg_pool2d(F.pad(x, (1, 1, 1, 1), mode=\"reflect\"), k, 1)\n",
        "            x = x * (1 - self.denoise) + x_blur * self.denoise\n",
        "\n",
        "        mu = x.mean(dim=(2, 3), keepdim=True)\n",
        "        sd = x.std(dim=(2, 3), keepdim=True).clamp_min(eps)\n",
        "        x0 = (x - mu) / sd\n",
        "        x0 = x0.clamp(-self.tau, self.tau)\n",
        "\n",
        "        x1 = torch.sign(x0) * torch.pow(torch.abs(x0).clamp_min(eps), self.gamma)\n",
        "\n",
        "        gray = x1.mean(dim=1, keepdim=True)\n",
        "        lap = F.conv2d(F.pad(gray, (1, 1, 1, 1), mode=\"reflect\"), self.lap)\n",
        "        mag = lap.abs()\n",
        "\n",
        "        k = self.blur_k if self.blur_k % 2 == 1 else self.blur_k + 1\n",
        "        pad = k // 2\n",
        "        blur = F.avg_pool2d(F.pad(mag, (pad, pad, pad, pad), mode=\"reflect\"), k, 1)\n",
        "        C_map = mag / (blur + eps)\n",
        "\n",
        "        x2 = x1 + self.alpha * torch.tanh(self.beta * C_map)\n",
        "\n",
        "        if self.sharpen > 0:\n",
        "            outs = []\n",
        "            for c in range(C):\n",
        "                x_c = x2[:, c: c + 1, :, :]\n",
        "                x_sharp = F.conv2d(F.pad(x_c, (1, 1, 1, 1), mode=\"reflect\"), self.sharp_kernel)\n",
        "                outs.append(x_c * (1 - self.sharpen) + x_sharp * self.sharpen)\n",
        "            x2 = torch.cat(outs, dim=1)\n",
        "\n",
        "        mn = x2.amin(dim=(2, 3), keepdim=True)\n",
        "        mx = x2.amax(dim=(2, 3), keepdim=True)\n",
        "        x3 = (x2 - mn) / (mx - mn + eps)\n",
        "        return x3.clamp(0, 1)\n",
        "\n",
        "def theta_to_module(theta):\n",
        "    return EnhancedFELCM(*theta)\n",
        "\n",
        "def random_theta():\n",
        "    gamma = random.uniform(0.7, 1.4)\n",
        "    alpha = random.uniform(0.15, 0.55)\n",
        "    beta = random.uniform(3.0, 9.0)\n",
        "    tau = random.uniform(1.8, 3.2)\n",
        "    blur_k = random.choice([3, 5, 7])\n",
        "    sharpen = random.uniform(0.0, 0.25)\n",
        "    denoise = random.uniform(0.0, 0.2)\n",
        "    return (gamma, alpha, beta, tau, blur_k, sharpen, denoise)\n",
        "\n",
        "def mutate(theta, p=0.8):\n",
        "    if random.random() > p:\n",
        "        return theta\n",
        "    g, a, b, t, k, sh, dn = theta\n",
        "    g = float(np.clip(g + np.random.normal(0, 0.06), 0.6, 1.5))\n",
        "    a = float(np.clip(a + np.random.normal(0, 0.05), 0.08, 0.7))\n",
        "    b = float(np.clip(b + np.random.normal(0, 0.5), 2.0, 11.0))\n",
        "    t = float(np.clip(t + np.random.normal(0, 0.2), 1.5, 3.8))\n",
        "    if random.random() < 0.3:\n",
        "        k = random.choice([3, 5, 7])\n",
        "    sh = float(np.clip(sh + np.random.normal(0, 0.04), 0.0, 0.35))\n",
        "    dn = float(np.clip(dn + np.random.normal(0, 0.03), 0.0, 0.3))\n",
        "    return (g, a, b, t, int(k), sh, dn)\n",
        "\n",
        "def crossover(t1, t2):\n",
        "    return tuple(random.choice([a, b]) for a, b in zip(t1, t2))\n",
        "\n",
        "def theta_str(th):\n",
        "    if th is None:\n",
        "        return \"None\"\n",
        "    g, a, b, t, k, sh, dn = th\n",
        "    return f\"(γ={g:.2f}, α={a:.2f}, β={b:.1f}, τ={t:.1f}, k={k}, sh={sh:.2f}, dn={dn:.2f})\"\n",
        "\n",
        "IDENTITY_PRE = nn.Identity().to(DEVICE)\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 6: MODEL (PVTv2-B2 + MULTI-SCALE FUSION)\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 6: MODEL (PVTv2-B2 + MULTI-SCALE FUSION)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "BACKBONE_NAME = \"pvt_v2_b2\"\n",
        "\n",
        "class TokenAttentionPooling(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, x):  # x: [B, N, C]\n",
        "        attn = torch.softmax(self.query(x).squeeze(-1), dim=1)  # [B, N]\n",
        "        return (x * attn.unsqueeze(-1)).sum(dim=1)              # [B, C]\n",
        "\n",
        "class MultiScaleFeatureFuser(nn.Module):\n",
        "    def __init__(self, in_channels: List[int], out_dim: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(c, out_dim, kernel_size=1, bias=False),\n",
        "                nn.GroupNorm(8, out_dim),\n",
        "                nn.GELU(),\n",
        "            ) for c in in_channels\n",
        "        ])\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(8, out_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.pool = TokenAttentionPooling(out_dim)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        proj_feats = [p(f) for p, f in zip(self.proj, feats)]\n",
        "        x = proj_feats[-1]\n",
        "        for f in reversed(proj_feats[:-1]):\n",
        "            x = F.interpolate(x, size=f.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "            x = x + f\n",
        "        x = self.fuse(x)\n",
        "        B, C, H, W = x.shape\n",
        "        tokens = x.flatten(2).transpose(1, 2)  # [B, HW, C]\n",
        "        pooled = self.pool(tokens)\n",
        "        return pooled\n",
        "\n",
        "class EnhancedBrainTuner(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.Linear(dim, max(8, dim // 4)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(max(8, dim // 4), dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.refine = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "        self.gate = nn.Parameter(torch.ones(2) / 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = F.softmax(self.gate, dim=0)\n",
        "        out1 = x * self.se(x)\n",
        "        out2 = x + 0.2 * self.refine(x)\n",
        "        return gate[0] * out1 + gate[1] * out2\n",
        "\n",
        "class PVTv2B2_MultiScale(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True, head_dropout=0.3, cond_dim=128, num_clients=6, num_sources=2):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            BACKBONE_NAME,\n",
        "            pretrained=pretrained,\n",
        "            features_only=True,\n",
        "            out_indices=(0, 1, 2, 3),\n",
        "        )\n",
        "        in_channels = self.backbone.feature_info.channels()\n",
        "        out_dim = max(256, in_channels[-1] // 2)\n",
        "\n",
        "        self.fuser = MultiScaleFeatureFuser(in_channels, out_dim)\n",
        "        self.tuner = EnhancedBrainTuner(out_dim, dropout=0.1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Dropout(head_dropout),\n",
        "            nn.Linear(out_dim, max(64, out_dim // 2)),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(head_dropout * 0.5),\n",
        "            nn.Linear(max(64, out_dim // 2), num_classes),\n",
        "        )\n",
        "\n",
        "        self.theta_mlp = nn.Sequential(\n",
        "            nn.Linear(7, cond_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cond_dim, cond_dim),\n",
        "        )\n",
        "        self.source_emb = nn.Embedding(num_sources, cond_dim)\n",
        "        self.client_emb = nn.Embedding(num_clients, cond_dim)\n",
        "        self.cond_norm = nn.LayerNorm(cond_dim)\n",
        "\n",
        "        self.gate_early = nn.Linear(cond_dim, 3)\n",
        "        self.gate_mid = nn.Linear(cond_dim, out_dim)\n",
        "        self.gate_late = nn.Linear(cond_dim, out_dim)\n",
        "\n",
        "    def _cond_vec(self, theta_vec, source_id, client_id):\n",
        "        cond = self.theta_mlp(theta_vec)\n",
        "        cond = cond + self.source_emb(source_id) + self.client_emb(client_id)\n",
        "        return self.cond_norm(cond)\n",
        "\n",
        "    def forward(self, x_raw_n, x_fel_n, theta_vec, source_id, client_id, return_gates=False):\n",
        "        cond = self._cond_vec(theta_vec, source_id, client_id)\n",
        "\n",
        "        g0 = torch.sigmoid(self.gate_early(cond)).view(-1, 3, 1, 1)\n",
        "        x0 = (1 - g0) * x_raw_n + g0 * x_fel_n\n",
        "\n",
        "        feats0 = self.backbone(x0)\n",
        "        feats1 = self.backbone(x_fel_n)\n",
        "\n",
        "        f0 = self.fuser(feats0)\n",
        "        f1 = self.fuser(feats1)\n",
        "\n",
        "        g1 = torch.sigmoid(self.gate_mid(cond))\n",
        "        f_mid = (1 - g1) * f0 + g1 * f1\n",
        "\n",
        "        t0 = self.tuner(f0)\n",
        "        t1 = self.tuner(f1)\n",
        "        t_mid = self.tuner(f_mid)\n",
        "\n",
        "        t_views = 0.5 * (t0 + t1)\n",
        "        g2 = torch.sigmoid(self.gate_late(cond))\n",
        "        t_final = (1 - g2) * t_mid + g2 * t_views\n",
        "\n",
        "        logits = self.classifier(t_final)\n",
        "\n",
        "        if return_gates:\n",
        "            return logits, {\"g0\": g0, \"g1\": g1, \"g2\": g2}\n",
        "        return logits\n",
        "\n",
        "def count_params(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "def set_trainable_for_round(model, rnd):\n",
        "    for p in model.backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "    for n, p in model.named_parameters():\n",
        "        if not n.startswith(\"backbone.\"):\n",
        "            p.requires_grad = True\n",
        "    if rnd >= CFG[\"unfreeze_after_round\"]:\n",
        "        params = list(model.backbone.parameters())\n",
        "        if len(params) > 0:\n",
        "            tail_n = max(1, int(len(params) * CFG[\"unfreeze_tail_frac\"]))\n",
        "            for p in params[-tail_n:]:\n",
        "                p.requires_grad = True\n",
        "\n",
        "def make_optimizer(model):\n",
        "    head_params, bb_params = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if n.startswith(\"backbone.\"):\n",
        "            bb_params.append(p)\n",
        "        else:\n",
        "            head_params.append(p)\n",
        "\n",
        "    groups = []\n",
        "    if head_params:\n",
        "        groups.append({\"params\": head_params, \"lr\": CFG[\"lr\"]})\n",
        "    if bb_params:\n",
        "        groups.append({\"params\": bb_params, \"lr\": CFG[\"lr\"] * CFG[\"unfreeze_lr_mult\"]})\n",
        "    return torch.optim.AdamW(groups, weight_decay=CFG[\"weight_decay\"])\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 7: GA FITNESS (ROBUST)\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 7: GA FITNESS (ROBUST)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "@torch.no_grad()\n",
        "def enhanced_separability_score(emb, y):\n",
        "    eps = 1e-6\n",
        "    y = y.long()\n",
        "    classes = torch.unique(y)\n",
        "    if len(classes) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    centroids = []\n",
        "    within_vars = []\n",
        "    sizes = []\n",
        "\n",
        "    for c in classes:\n",
        "        mask = y == c\n",
        "        e = emb[mask]\n",
        "        if e.size(0) < 2:\n",
        "            continue\n",
        "        mu = e.mean(dim=0)\n",
        "        var = (e - mu).pow(2).sum(dim=1).mean().item()\n",
        "        centroids.append(mu)\n",
        "        within_vars.append(var)\n",
        "        sizes.append(e.size(0))\n",
        "\n",
        "    if len(centroids) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    centroids = torch.stack(centroids, dim=0)\n",
        "    global_mean = centroids.mean(dim=0)\n",
        "    between = sum(n * (c - global_mean).pow(2).sum().item() for c, n in zip(centroids, sizes))\n",
        "    within = float(np.mean(within_vars)) if within_vars else eps\n",
        "    return float(between / (within + eps))\n",
        "\n",
        "@torch.no_grad()\n",
        "def ga_fitness(theta, backbone_frozen, batch_x, batch_y, use_separability=True):\n",
        "    pre = theta_to_module(theta).to(DEVICE)\n",
        "    x = batch_x.to(DEVICE)\n",
        "    y = batch_y.to(DEVICE)\n",
        "\n",
        "    x_p = pre(x)\n",
        "    gray = x_p.mean(dim=1, keepdim=True)\n",
        "    lap = F.conv2d(F.pad(gray, (1, 1, 1, 1), mode=\"reflect\"), pre.lap).abs()\n",
        "    contrast = float(lap.mean().item())\n",
        "    dyn_range = float((x_p.max() - x_p.min()).item())\n",
        "\n",
        "    x_n = (x_p - IMAGENET_MEAN) / IMAGENET_STD\n",
        "\n",
        "    sep = 0.0\n",
        "    if use_separability:\n",
        "        emb = backbone_frozen(x_n)\n",
        "        if isinstance(emb, (list, tuple)):\n",
        "            emb = emb[-1]\n",
        "            emb = emb.mean(dim=(2, 3))\n",
        "        sep = enhanced_separability_score(emb, y)\n",
        "\n",
        "    g, a, b, t, k, sh, dn = theta\n",
        "    cost = (0.03 * abs(g - 1.0) + 0.05 * a + 0.01 * (b / 10.0) +\n",
        "            0.02 * abs(t - 2.5) + 0.02 * sh + 0.02 * dn)\n",
        "\n",
        "    if use_separability:\n",
        "        return 0.35 * contrast + 0.15 * dyn_range + 1.35 * sep - 0.5 * cost\n",
        "    return 0.60 * contrast + 0.35 * dyn_range - 0.5 * cost\n",
        "\n",
        "def _safe_first_batch(dl):\n",
        "    try:\n",
        "        it = iter(dl)\n",
        "        bx, by, *_ = next(it)\n",
        "        return bx, by\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def run_ga_for_client(backbone_frozen, dl_for_eval, elite_pool, use_separability=True):\n",
        "    bx, by = _safe_first_batch(dl_for_eval)\n",
        "    if bx is None:\n",
        "        return None, [], 0.0\n",
        "\n",
        "    pop = []\n",
        "    if elite_pool:\n",
        "        pop.extend(elite_pool[: min(len(elite_pool), CFG[\"ga_pop\"] // 2)])\n",
        "    while len(pop) < CFG[\"ga_pop\"]:\n",
        "        pop.append(random_theta())\n",
        "\n",
        "    bx = bx[: CFG[\"batch_size\"]].contiguous()\n",
        "    by = by[: CFG[\"batch_size\"]].contiguous()\n",
        "\n",
        "    for _ in range(CFG[\"ga_gens\"]):\n",
        "        scored = [(ga_fitness(th, backbone_frozen, bx, by, use_separability), th) for th in pop]\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        elites = [th for _, th in scored[: CFG[\"ga_elites\"]]]\n",
        "\n",
        "        new_pop = elites[:]\n",
        "        while len(new_pop) < CFG[\"ga_pop\"]:\n",
        "            p1, p2 = random.sample(elites + pop[: max(2, CFG[\"ga_pop\"] // 2)], 2)\n",
        "            child = crossover(p1, p2)\n",
        "            child = mutate(child, p=0.75)\n",
        "            new_pop.append(child)\n",
        "        pop = new_pop\n",
        "\n",
        "    scored = [(ga_fitness(th, backbone_frozen, bx, by, use_separability), th) for th in pop]\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_theta = scored[0][1]\n",
        "    best_fit = float(scored[0][0])\n",
        "    top = [th for _, th in scored[: CFG[\"ga_elites\"]]]\n",
        "    return best_theta, top, best_fit\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 8: TRAIN / EVAL UTILITIES\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 8: TRAIN / EVAL UTILITIES (FULL METRICS)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(step):\n",
        "        if step < num_warmup_steps:\n",
        "            return float(step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def preproc_theta_vec(preproc_module, batch_size):\n",
        "    if hasattr(preproc_module, \"gamma\"):\n",
        "        theta = torch.tensor(\n",
        "            [\n",
        "                preproc_module.gamma,\n",
        "                preproc_module.alpha,\n",
        "                preproc_module.beta,\n",
        "                preproc_module.tau,\n",
        "                float(preproc_module.blur_k) / 7.0,\n",
        "                preproc_module.sharpen,\n",
        "                preproc_module.denoise,\n",
        "            ],\n",
        "            device=DEVICE,\n",
        "            dtype=torch.float32,\n",
        "        )\n",
        "    else:\n",
        "        theta = torch.zeros(7, device=DEVICE, dtype=torch.float32)\n",
        "    return theta.unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "def gate_entropy(gate):\n",
        "    eps = 1e-6\n",
        "    p = gate.clamp(eps, 1 - eps)\n",
        "    ent = -(p * torch.log2(p) + (1 - p) * torch.log2(1 - p))\n",
        "    return ent\n",
        "\n",
        "def summarize_gate_stats(gate_stats, num_classes):\n",
        "    gate_metrics = {}\n",
        "    all_gates = {\"g0\": [], \"g1\": [], \"g2\": []}\n",
        "    all_labels = []\n",
        "    for gates, y_cpu in gate_stats:\n",
        "        for k in all_gates:\n",
        "            all_gates[k].append(gates[k].detach().cpu())\n",
        "        all_labels.append(y_cpu)\n",
        "\n",
        "    labels_cat = torch.cat(all_labels, dim=0)\n",
        "    for k in all_gates:\n",
        "        g = torch.cat(all_gates[k], dim=0)\n",
        "        ent = gate_entropy(g).mean(dim=list(range(1, g.ndim)))\n",
        "        gate_metrics[f\"{k}_mean\"] = float(g.mean().item())\n",
        "        gate_metrics[f\"{k}_entropy_mean\"] = float(ent.mean().item())\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            mask = labels_cat == c\n",
        "            if mask.any():\n",
        "                gate_metrics[f\"{k}_mean_c{c}\"] = float(g[mask].mean().item())\n",
        "                gate_metrics[f\"{k}_entropy_c{c}\"] = float(ent[mask].mean().item())\n",
        "    return gate_metrics\n",
        "\n",
        "@torch.no_grad()\n",
        "def _auc_metrics(y_true, p_pred, num_classes):\n",
        "    out = {}\n",
        "    try:\n",
        "        if num_classes == 2:\n",
        "            out[\"auc_roc\"] = float(roc_auc_score(y_true, p_pred[:, 1]))\n",
        "        else:\n",
        "            out[\"auc_roc_macro_ovr\"] = float(roc_auc_score(y_true, p_pred, multi_class=\"ovr\", average=\"macro\"))\n",
        "            for c in range(num_classes):\n",
        "                yc = (y_true == c).astype(int)\n",
        "                if yc.sum() > 0 and yc.sum() < len(yc):\n",
        "                    out[f\"auc_class_{c}\"] = float(roc_auc_score(yc, p_pred[:, c]))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_full(model, loader, preproc_module, return_gates=False):\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    preproc_module.eval()\n",
        "\n",
        "    all_y, all_p, all_loss = [], [], []\n",
        "    gate_stats = []\n",
        "    has_any = False\n",
        "\n",
        "    for x, y, _, source_id, client_id in loader:\n",
        "        has_any = True\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "        source_id = source_id.to(DEVICE, non_blocking=True)\n",
        "        client_id = client_id.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        x_p = preproc_module(x)\n",
        "        x_raw_n = (x - IMAGENET_MEAN) / IMAGENET_STD\n",
        "        x_fel_n = (x_p - IMAGENET_MEAN) / IMAGENET_STD\n",
        "\n",
        "        theta_vec = preproc_theta_vec(preproc_module, x.size(0))\n",
        "        if return_gates:\n",
        "            logits, gates = model(x_raw_n, x_fel_n, theta_vec, source_id, client_id, return_gates=True)\n",
        "            gate_stats.append((gates, y.detach().cpu()))\n",
        "        else:\n",
        "            logits = model(x_raw_n, x_fel_n, theta_vec, source_id, client_id)\n",
        "\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        all_loss.append(float(loss.item()))\n",
        "        all_y.append(y.detach().cpu().numpy())\n",
        "        all_p.append(probs.detach().cpu().numpy())\n",
        "\n",
        "    if not has_any:\n",
        "        met = {\n",
        "            \"loss_ce\": np.nan,\n",
        "            \"acc\": np.nan,\n",
        "            \"precision_macro\": np.nan,\n",
        "            \"recall_macro\": np.nan,\n",
        "            \"f1_macro\": np.nan,\n",
        "            \"precision_weighted\": np.nan,\n",
        "            \"recall_weighted\": np.nan,\n",
        "            \"f1_weighted\": np.nan,\n",
        "            \"log_loss\": np.nan,\n",
        "            \"eval_time_s\": float(time.time() - t0),\n",
        "        }\n",
        "        return met, np.array([]), np.array([])\n",
        "\n",
        "    y_true = np.concatenate(all_y)\n",
        "    p_pred = np.concatenate(all_p)\n",
        "    y_hat = np.argmax(p_pred, axis=1)\n",
        "\n",
        "    met = {\n",
        "        \"loss_ce\": float(np.mean(all_loss)),\n",
        "        \"acc\": float(accuracy_score(y_true, y_hat)),\n",
        "        \"precision_macro\": float(precision_score(y_true, y_hat, average=\"macro\", zero_division=0)),\n",
        "        \"recall_macro\": float(recall_score(y_true, y_hat, average=\"macro\", zero_division=0)),\n",
        "        \"f1_macro\": float(f1_score(y_true, y_hat, average=\"macro\", zero_division=0)),\n",
        "        \"precision_weighted\": float(precision_score(y_true, y_hat, average=\"weighted\", zero_division=0)),\n",
        "        \"recall_weighted\": float(recall_score(y_true, y_hat, average=\"weighted\", zero_division=0)),\n",
        "        \"f1_weighted\": float(f1_score(y_true, y_hat, average=\"weighted\", zero_division=0)),\n",
        "        \"log_loss\": float(log_loss(y_true, p_pred, labels=list(range(NUM_CLASSES)))),\n",
        "        \"eval_time_s\": float(time.time() - t0),\n",
        "    }\n",
        "    met.update(_auc_metrics(y_true, p_pred, NUM_CLASSES))\n",
        "    if return_gates and gate_stats:\n",
        "        met.update(summarize_gate_stats(gate_stats, NUM_CLASSES))\n",
        "    return met, y_true, p_pred\n",
        "\n",
        "def fedprox_term(local_model, global_model):\n",
        "    loss = 0.0\n",
        "    for p_local, p_global in zip(local_model.parameters(), global_model.parameters()):\n",
        "        loss += ((p_local - p_global.detach()) ** 2).sum()\n",
        "    return loss\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, preproc_module, criterion, global_model=None, scheduler=None, scaler=None,\n",
        "                    grad_clip=1.0):\n",
        "    model.train()\n",
        "    preproc_module.eval()\n",
        "    losses, correct, total = [], 0, 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for x, y, _, source_id, client_id in loader:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "        source_id = source_id.to(DEVICE, non_blocking=True)\n",
        "        client_id = client_id.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        with torch.amp.autocast(device_type=DEVICE.type, enabled=(scaler is not None)):\n",
        "            x_p = preproc_module(x)\n",
        "            x_raw_n = (x - IMAGENET_MEAN) / IMAGENET_STD\n",
        "            x_fel_n = (x_p - IMAGENET_MEAN) / IMAGENET_STD\n",
        "            theta_vec = preproc_theta_vec(preproc_module, x.size(0))\n",
        "            logits = model(x_raw_n, x_fel_n, theta_vec, source_id, client_id)\n",
        "            loss = criterion(logits, y)\n",
        "            if global_model is not None and CFG[\"fedprox_mu\"] > 0:\n",
        "                loss = loss + 0.5 * CFG[\"fedprox_mu\"] * fedprox_term(model, global_model)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            if grad_clip and grad_clip > 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            if grad_clip and grad_clip > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        losses.append(float(loss.item()))\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += int((preds == y).sum().item())\n",
        "        total += int(y.size(0))\n",
        "\n",
        "    return float(np.mean(losses)), float(correct / max(1, total)), float(time.time() - t0)\n",
        "\n",
        "def fedavg_update(global_model, local_models, weights, trainable_names):\n",
        "    gsd = global_model.state_dict()\n",
        "    new_sd = {}\n",
        "    for name in trainable_names:\n",
        "        acc = None\n",
        "        for m, w in zip(local_models, weights):\n",
        "            p = m.state_dict()[name].detach().float().cpu()\n",
        "            acc = (w * p) if acc is None else (acc + w * p)\n",
        "        new_sd[name] = acc\n",
        "    for name, t in new_sd.items():\n",
        "        gsd[name].copy_(t.to(gsd[name].device).type_as(gsd[name]))\n",
        "    global_model.load_state_dict(gsd)\n",
        "\n",
        "@torch.no_grad()\n",
        "def pick_best_theta_from_pool(model, pool, val_loader, max_candidates=10):\n",
        "    if not pool:\n",
        "        return None, None\n",
        "    cand = pool[:max_candidates]\n",
        "    best, best_acc = None, -1\n",
        "    for th in cand:\n",
        "        pre = theta_to_module(th).to(DEVICE)\n",
        "        met, _, _ = evaluate_full(model, val_loader, pre)\n",
        "        if np.isfinite(met[\"acc\"]) and met[\"acc\"] > best_acc:\n",
        "            best_acc = met[\"acc\"]\n",
        "            best = th\n",
        "    return best, best_acc\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 9: INITIALIZE GLOBAL MODEL\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 9: INITIALIZING GLOBAL MODEL\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "global_model = PVTv2B2_MultiScale(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    pretrained=True,\n",
        "    head_dropout=CFG[\"head_dropout\"],\n",
        "    cond_dim=CFG[\"cond_dim\"],\n",
        "    num_clients=CFG[\"clients_total\"],\n",
        "    num_sources=len(DATASET_NAMES),\n",
        ").to(DEVICE)\n",
        "\n",
        "set_trainable_for_round(global_model, rnd=1)\n",
        "\n",
        "total_params, tuned_params = count_params(global_model)\n",
        "print(f\"Backbone: {BACKBONE_NAME} | Total params: {total_params:,} | Trainable: {tuned_params:,} ({(tuned_params/total_params)*100:.2f}%)\")\n",
        "\n",
        "backbone_frozen = global_model.backbone.eval()\n",
        "for p in backbone_frozen.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
        "for ds_name in DATASET_NAMES:\n",
        "    counts += train_frames[ds_name][\"y\"].value_counts().sort_index().reindex(range(NUM_CLASSES), fill_value=0).values\n",
        "w = (counts.sum() / np.clip(counts, 1, None)).astype(np.float32)\n",
        "w = w / max(1e-6, w.mean())\n",
        "class_w = torch.tensor(w, device=DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_w, label_smoothing=CFG[\"label_smoothing\"])\n",
        "scaler = torch.amp.GradScaler(\"cuda\") if DEVICE.type == \"cuda\" else None\n",
        "\n",
        "hp_rows = [{\"hp_name\": k, \"hp_value\": str(v)} for k, v in CFG.items()]\n",
        "hp_rows += [\n",
        "    {\"hp_name\": \"GA_theta_ranges\",\n",
        "     \"hp_value\": \"gamma∈[0.7,1.4], alpha∈[0.15,0.55], beta∈[3,9], tau∈[1.8,3.2], k∈{3,5,7}, sh∈[0,0.25], dn∈[0,0.2]\"},\n",
        "    {\"hp_name\": \"theta_fullforms\", \"hp_value\": str(THETA_FULLFORMS)},\n",
        "    {\"hp_name\": \"backbone_name\", \"hp_value\": BACKBONE_NAME},\n",
        "    {\"hp_name\": \"norm\", \"hp_value\": \"ImageNet mean/std\"},\n",
        "]\n",
        "hp_df = pd.DataFrame(hp_rows)\n",
        "print_table(hp_df, \"Hyperparameters / Search Space\")\n",
        "add_table_to_csv(hp_df, \"hyperparameters\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 10: FEDERATED TRAINING\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 10: FEDERATED TRAINING (NO CENTRAL VAL/TEST)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "elite_pools = {ds: [] for ds in DATASET_NAMES}\n",
        "history_global = []\n",
        "history_local = []\n",
        "\n",
        "best_global_acc = -1.0\n",
        "best_model_state = None\n",
        "best_thetas = {ds: None for ds in DATASET_NAMES}\n",
        "best_round_saved = None\n",
        "\n",
        "t_global_start = time.time()\n",
        "\n",
        "for rnd in range(1, CFG[\"rounds\"] + 1):\n",
        "    round_t0 = time.time()\n",
        "    local_models = []\n",
        "    local_weights = []\n",
        "    local_rows = []\n",
        "\n",
        "    print(f\"\\n{'=' * 92}\\nROUND {rnd}/{CFG['rounds']}\\n{'=' * 92}\")\n",
        "\n",
        "    for k in range(CFG[\"clients_total\"]):\n",
        "        tr_loader, tune_loader, val_loader = client_loaders[k]\n",
        "        ds_name = gid_to_ds[k]\n",
        "        elite_pool = elite_pools[ds_name]\n",
        "\n",
        "        ga_t0 = time.time()\n",
        "        if CFG[\"use_preprocessing\"] and CFG[\"use_ga\"]:\n",
        "            best_theta, top_thetas, best_fit = run_ga_for_client(\n",
        "                backbone_frozen, tune_loader, elite_pool, use_separability=True\n",
        "            )\n",
        "            elite_pool.extend(top_thetas)\n",
        "            elite_pool[:] = elite_pool[: CFG[\"elite_pool_max\"]]\n",
        "            pre_k = theta_to_module(best_theta).to(DEVICE) if best_theta is not None else IDENTITY_PRE\n",
        "        else:\n",
        "            best_theta, best_fit = None, 0.0\n",
        "            pre_k = IDENTITY_PRE\n",
        "        ga_time = float(time.time() - ga_t0)\n",
        "\n",
        "        elite_pools[ds_name] = elite_pool\n",
        "\n",
        "        local_model = PVTv2B2_MultiScale(\n",
        "            num_classes=NUM_CLASSES,\n",
        "            pretrained=False,\n",
        "            head_dropout=CFG[\"head_dropout\"],\n",
        "            cond_dim=CFG[\"cond_dim\"],\n",
        "            num_clients=CFG[\"clients_total\"],\n",
        "            num_sources=len(DATASET_NAMES),\n",
        "        ).to(DEVICE)\n",
        "        local_model.load_state_dict(global_model.state_dict(), strict=True)\n",
        "\n",
        "        set_trainable_for_round(local_model, rnd=rnd)\n",
        "        opt = make_optimizer(local_model)\n",
        "\n",
        "        total_steps = max(1, len(tr_loader) * CFG[\"local_epochs\"])\n",
        "        warmup_steps = max(1, len(tr_loader) * CFG[\"warmup_epochs\"])\n",
        "        scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
        "\n",
        "        tr_losses, tr_accs, tr_time = [], [], 0.0\n",
        "        for _ in range(CFG[\"local_epochs\"]):\n",
        "            loss_ep, acc_ep, t_ep = train_one_epoch(\n",
        "                local_model,\n",
        "                tr_loader,\n",
        "                opt,\n",
        "                pre_k,\n",
        "                criterion,\n",
        "                global_model=global_model,\n",
        "                scheduler=scheduler,\n",
        "                scaler=scaler,\n",
        "                grad_clip=CFG[\"grad_clip\"],\n",
        "            )\n",
        "            tr_losses.append(loss_ep)\n",
        "            tr_accs.append(acc_ep)\n",
        "            tr_time += t_ep\n",
        "\n",
        "        met_loc, _, _ = evaluate_full(local_model, val_loader, pre_k, return_gates=True)\n",
        "\n",
        "        local_models.append(local_model)\n",
        "        local_weights.append(len(tr_loader.dataset))\n",
        "\n",
        "        if best_theta is not None:\n",
        "            g, a, b, t, kk, sh, dn = best_theta\n",
        "        else:\n",
        "            g = a = b = t = kk = sh = dn = None\n",
        "\n",
        "        row = {\n",
        "            \"round\": rnd,\n",
        "            \"client\": f\"client_{k}\",\n",
        "            \"dataset\": ds_name,\n",
        "            \"ga_best_fit_score\": float(best_fit),\n",
        "            \"ga_time_s\": ga_time,\n",
        "            \"theta_str\": theta_str(best_theta),\n",
        "            \"gamma_power\": g,\n",
        "            \"alpha_contrast_weight\": a,\n",
        "            \"beta_contrast_sharpness\": b,\n",
        "            \"tau_clip\": t,\n",
        "            \"k_blur_kernel_size\": kk,\n",
        "            \"sh_sharpen_strength\": sh,\n",
        "            \"dn_denoise_strength\": dn,\n",
        "            \"train_loss\": float(np.mean(tr_losses)),\n",
        "            \"train_acc\": float(np.mean(tr_accs)),\n",
        "            \"train_time_s\": float(tr_time),\n",
        "            **{f\"val_{k2}\": v2 for k2, v2 in met_loc.items()},\n",
        "        }\n",
        "        local_rows.append(row)\n",
        "\n",
        "        auc_val = row.get(\"val_auc_roc_macro_ovr\", row.get(\"val_auc_roc\", np.nan))\n",
        "        print(\n",
        "            f\"Client {k} ({ds_name}) | train_acc={row['train_acc']:.4f} | \"\n",
        "            f\"val_acc={row['val_acc']:.4f} | val_f1={row['val_f1_macro']:.4f} | \"\n",
        "            f\"val_auc={auc_val:.4f} | val_logloss={row['val_log_loss']:.4f} | \"\n",
        "            f\"GA_fit={row['ga_best_fit_score']:.3f} | ga_time={row['ga_time_s']:.1f}s | theta={row['theta_str']}\"\n",
        "        )\n",
        "\n",
        "    wsum = sum(local_weights)\n",
        "    weights = [w / wsum for w in local_weights]\n",
        "    trainable_names = [n for n, p in local_models[0].named_parameters() if p.requires_grad]\n",
        "    fedavg_update(global_model, local_models, weights, trainable_names)\n",
        "\n",
        "    local_val_rows = pd.DataFrame(local_rows)\n",
        "    local_val_rows[\"val_size\"] = [len(client_loaders[i][2].dataset) for i in range(CFG[\"clients_total\"])]\n",
        "    total_val = local_val_rows[\"val_size\"].sum()\n",
        "\n",
        "    def weighted_avg(key):\n",
        "        if total_val == 0:\n",
        "            return np.nan\n",
        "        return float(np.average(local_val_rows[key], weights=local_val_rows[\"val_size\"]))\n",
        "\n",
        "    global_metrics = {\n",
        "        \"acc\": weighted_avg(\"val_acc\"),\n",
        "        \"f1_macro\": weighted_avg(\"val_f1_macro\"),\n",
        "        \"precision_macro\": weighted_avg(\"val_precision_macro\"),\n",
        "        \"recall_macro\": weighted_avg(\"val_recall_macro\"),\n",
        "        \"log_loss\": weighted_avg(\"val_log_loss\"),\n",
        "        \"loss_ce\": weighted_avg(\"val_loss_ce\"),\n",
        "        \"eval_time_s\": weighted_avg(\"val_eval_time_s\"),\n",
        "    }\n",
        "\n",
        "    if CFG[\"use_preprocessing\"]:\n",
        "        for ds_name in DATASET_NAMES:\n",
        "            if elite_pools[ds_name]:\n",
        "                rep_gid = DATASET_NAMES.index(ds_name) * n_per_ds\n",
        "                best_thetas[ds_name], _ = pick_best_theta_from_pool(\n",
        "                    global_model, elite_pools[ds_name], client_loaders[rep_gid][2]\n",
        "                )\n",
        "\n",
        "    history_local.extend(local_rows)\n",
        "    history_global.append({\n",
        "        \"round\": rnd,\n",
        "        \"round_time_s\": float(time.time() - round_t0),\n",
        "        \"global_thetas\": str({ds: theta_str(best_thetas[ds]) for ds in DATASET_NAMES}),\n",
        "        **{f\"global_{k2}\": v2 for k2, v2 in global_metrics.items()},\n",
        "    })\n",
        "\n",
        "    if np.isfinite(global_metrics[\"acc\"]) and global_metrics[\"acc\"] > best_global_acc:\n",
        "        best_global_acc = float(global_metrics[\"acc\"])\n",
        "        best_model_state = {k: v.detach().cpu().clone() for k, v in global_model.state_dict().items()}\n",
        "        best_round_saved = rnd\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 92)\n",
        "    print(\n",
        "        f\"GLOBAL VAL (Round {rnd}) | acc={global_metrics['acc']:.4f} | f1={global_metrics['f1_macro']:.4f} | \"\n",
        "        f\"logloss={global_metrics['log_loss']:.4f} | loss_ce={global_metrics['loss_ce']:.4f} | \"\n",
        "        f\"round_time={history_global[-1]['round_time_s']:.1f}s | \"\n",
        "        f\"thetas={str({ds: theta_str(best_thetas[ds]) for ds in DATASET_NAMES})}\"\n",
        "    )\n",
        "    print(f\"BEST SO FAR (by ACC) | best_val_acc={best_global_acc:.4f} at round={best_round_saved}\")\n",
        "    print(\"-\" * 92)\n",
        "\n",
        "if best_model_state is not None:\n",
        "    global_model.load_state_dict({k: v.to(DEVICE) for k, v in best_model_state.items()})\n",
        "\n",
        "t_total = float(time.time() - t_global_start)\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(f\"TRAINING COMPLETE ✅ | total_time={t_total:.1f}s | best_val_acc={best_global_acc:.4f} | best_round={best_round_saved}\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "glob_df = pd.DataFrame(history_global)\n",
        "loc_df  = pd.DataFrame(history_local)\n",
        "print_table(glob_df, \"GLOBAL per-round metrics\")\n",
        "print_table(loc_df.head(30), \"LOCAL per-client per-round metrics (head)\")\n",
        "add_table_to_csv(glob_df, \"global_round_metrics_full\")\n",
        "add_table_to_csv(loc_df, \"client_round_metrics_full\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 11: FINAL EVALUATION (FEDERATED VAL + TEST)\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 11: FINAL EVALUATION (FEDERATED VAL + TEST)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "pre_by_ds = {\n",
        "    ds_name: (theta_to_module(best_thetas[ds_name]).to(DEVICE)\n",
        "              if (CFG[\"use_preprocessing\"] and best_thetas[ds_name] is not None)\n",
        "              else IDENTITY_PRE)\n",
        "    for ds_name in DATASET_NAMES\n",
        "}\n",
        "\n",
        "def weighted_aggregate(mets):\n",
        "    if not mets:\n",
        "        return {}\n",
        "    total = sum(w for _, _, w in mets)\n",
        "    if total == 0:\n",
        "        return {}\n",
        "    keys = mets[0][1].keys()\n",
        "    out = {}\n",
        "    for k in keys:\n",
        "        vals = [m[1].get(k, np.nan) for m in mets]\n",
        "        weights = [m[2] for m in mets]\n",
        "        out[k] = float(np.average(vals, weights=weights))\n",
        "    return out\n",
        "\n",
        "val_metrics_clients = []\n",
        "for k in range(CFG[\"clients_total\"]):\n",
        "    _, _, val_loader = client_loaders[k]\n",
        "    ds_name = gid_to_ds[k]\n",
        "    met, _, _ = evaluate_full(global_model, val_loader, pre_by_ds[ds_name])\n",
        "    val_metrics_clients.append((k, met, len(val_loader.dataset)))\n",
        "val_best = weighted_aggregate(val_metrics_clients)\n",
        "\n",
        "def eval_test_per_dataset(ds_name):\n",
        "    mets = []\n",
        "    for (ds, local_id, gid, t_loader) in client_test_loaders:\n",
        "        if ds != ds_name:\n",
        "            continue\n",
        "        met, y_true, p_pred = evaluate_full(global_model, t_loader, pre_by_ds[ds], return_gates=True)\n",
        "        mets.append((met, len(t_loader.dataset), y_true, p_pred, gid))\n",
        "    if not mets:\n",
        "        return {}, []\n",
        "    agg = weighted_aggregate([(i, m[0], m[1]) for i, m in enumerate(mets)])\n",
        "    return agg, mets\n",
        "\n",
        "test_by_ds = {}\n",
        "test_detail_by_ds = {}\n",
        "for ds_name in DATASET_NAMES:\n",
        "    tm, td = eval_test_per_dataset(ds_name)\n",
        "    test_by_ds[ds_name] = tm\n",
        "    test_detail_by_ds[ds_name] = td\n",
        "\n",
        "ds1_mets = test_detail_by_ds[\"ds1\"]\n",
        "\n",
        "global_test = weighted_aggregate([\n",
        "    (i, test_by_ds[ds_name], len(test_frames[ds_name])) for i, ds_name in enumerate(DATASET_NAMES)\n",
        "])\n",
        "\n",
        "def compact_metrics(m):\n",
        "    keep = [\n",
        "        \"acc\", \"precision_macro\", \"recall_macro\", \"f1_macro\",\n",
        "        \"precision_weighted\", \"recall_weighted\", \"f1_weighted\",\n",
        "        \"log_loss\"\n",
        "    ]\n",
        "    if \"auc_roc_macro_ovr\" in m:\n",
        "        keep.append(\"auc_roc_macro_ovr\")\n",
        "    if \"auc_roc\" in m:\n",
        "        keep.append(\"auc_roc\")\n",
        "    keep += [\"loss_ce\", \"eval_time_s\"]\n",
        "    return {k: float(m[k]) for k in keep if k in m}\n",
        "\n",
        "paper_rows = [\n",
        "    {\"setting\": \"Enhanced FELCM (Best θ per dataset)\", \"split\": \"VAL\", \"dataset\": \"all datasets weighted\", **compact_metrics(val_best)}\n",
        "]\n",
        "for ds_name in DATASET_NAMES:\n",
        "    paper_rows.append({\n",
        "        \"setting\": f\"Enhanced FELCM (Best θ {ds_name})\",\n",
        "        \"split\": \"TEST\",\n",
        "        \"dataset\": ds_name,\n",
        "        **compact_metrics(test_by_ds[ds_name])\n",
        "    })\n",
        "paper_rows.append({\"setting\": \"Enhanced FELCM (Best θ)\", \"split\": \"TEST\", \"dataset\": \"global weighted\", **compact_metrics(global_test)})\n",
        "\n",
        "paper_df = pd.DataFrame(paper_rows)\n",
        "print_table(paper_df, \"VAL+TEST tables (federated, per-dataset + global)\")\n",
        "add_table_to_csv(paper_df, \"paper_ready_metrics\")\n",
        "\n",
        "def pick_auc(m):\n",
        "    if \"auc_roc_macro_ovr\" in m:\n",
        "        return float(m[\"auc_roc_macro_ovr\"])\n",
        "    if \"auc_roc\" in m:\n",
        "        return float(m[\"auc_roc\"])\n",
        "    return np.nan\n",
        "\n",
        "explicit_rows = []\n",
        "explicit_rows.append({\n",
        "    \"dataset\": \"all_datasets_val_weighted\",\n",
        "    \"acc\": float(val_best.get(\"acc\", np.nan)),\n",
        "    \"pre\": float(val_best.get(\"precision_macro\", np.nan)),\n",
        "    \"rec\": float(val_best.get(\"recall_macro\", np.nan)),\n",
        "    \"f1\": float(val_best.get(\"f1_macro\", np.nan)),\n",
        "    \"logloss\": float(val_best.get(\"log_loss\", np.nan)),\n",
        "    \"auc_roc\": pick_auc(val_best),\n",
        "})\n",
        "for ds_name in DATASET_NAMES:\n",
        "    met = test_by_ds.get(ds_name, {})\n",
        "    explicit_rows.append({\n",
        "        \"dataset\": f\"{ds_name}_test\",\n",
        "        \"acc\": float(met.get(\"acc\", np.nan)),\n",
        "        \"pre\": float(met.get(\"precision_macro\", np.nan)),\n",
        "        \"rec\": float(met.get(\"recall_macro\", np.nan)),\n",
        "        \"f1\": float(met.get(\"f1_macro\", np.nan)),\n",
        "        \"logloss\": float(met.get(\"log_loss\", np.nan)),\n",
        "        \"auc_roc\": pick_auc(met),\n",
        "    })\n",
        "explicit_rows.append({\n",
        "    \"dataset\": \"global_test_weighted\",\n",
        "    \"acc\": float(global_test.get(\"acc\", np.nan)),\n",
        "    \"pre\": float(global_test.get(\"precision_macro\", np.nan)),\n",
        "    \"rec\": float(global_test.get(\"recall_macro\", np.nan)),\n",
        "    \"f1\": float(global_test.get(\"f1_macro\", np.nan)),\n",
        "    \"logloss\": float(global_test.get(\"log_loss\", np.nan)),\n",
        "    \"auc_roc\": pick_auc(global_test),\n",
        "})\n",
        "\n",
        "explicit_metrics_df = pd.DataFrame(explicit_rows)\n",
        "print_table(explicit_metrics_df, \"Requested explicit metrics (acc, pre, rec, f1, logloss, auc_roc)\")\n",
        "add_table_to_csv(explicit_metrics_df, \"requested_explicit_metrics\")\n",
        "\n",
        "print(\"\\nPaper selection summary:\")\n",
        "print(f\"- Best round (by federated VAL accuracy): round={best_round_saved} | best_val_acc={best_global_acc:.4f}\")\n",
        "for ds_name in DATASET_NAMES:\n",
        "    print(f\"- Best θ {ds_name}: {theta_str(best_thetas[ds_name])}\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 12: PREPROCESSING VALIDATION (DS1 VAL SAMPLE)\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 12: PREPROCESSING VALIDATION (DS1 VAL SAMPLE)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "@torch.no_grad()\n",
        "def entropy_per_image(x01):\n",
        "    gray = x01.mean(dim=1)\n",
        "    B = gray.shape[0]\n",
        "    ent = []\n",
        "    for i in range(B):\n",
        "        g = (gray[i].detach().cpu().numpy() * 255).astype(np.uint8)\n",
        "        hist = np.bincount(g.flatten(), minlength=256).astype(np.float32)\n",
        "        p = hist / np.clip(hist.sum(), 1, None)\n",
        "        p = p[p > 0]\n",
        "        ent.append(float(-(p * np.log2(p)).sum()))\n",
        "    return np.array(ent)\n",
        "\n",
        "@torch.no_grad()\n",
        "def edge_energy(x01, lap_kernel):\n",
        "    lap_kernel = lap_kernel.to(device=x01.device, dtype=x01.dtype)\n",
        "    gray = x01.mean(dim=1, keepdim=True)\n",
        "    lap = F.conv2d(F.pad(gray, (1, 1, 1, 1), mode=\"reflect\"), lap_kernel).abs()\n",
        "    return lap.mean(dim=(1, 2, 3)).detach().cpu().numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def contrast_proxy(x01):\n",
        "    gray = x01.mean(dim=1)\n",
        "    return gray.std(dim=(1, 2)).detach().cpu().numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_preproc_validation(frame, preproc, sample_n=500):\n",
        "    n = min(sample_n, len(frame))\n",
        "    if n <= 0:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, None\n",
        "\n",
        "    idx = np.random.choice(len(frame), size=n, replace=False)\n",
        "    ds = MRIDataset(frame, indices=idx.tolist(), tfms=EVAL_TFMS, source_id=0, client_id=0)\n",
        "\n",
        "    xs = []\n",
        "    for i in range(len(ds)):\n",
        "        x, _, *_ = ds[i]\n",
        "        xs.append(x)\n",
        "    x = torch.stack(xs).to(DEVICE)\n",
        "\n",
        "    x_after = preproc(x).clamp(0, 1)\n",
        "    lap_kernel = preproc.lap if hasattr(preproc, \"lap\") else EnhancedFELCM().to(DEVICE).lap\n",
        "\n",
        "    ee_before = edge_energy(x, lap_kernel)\n",
        "    ee_after  = edge_energy(x_after, lap_kernel)\n",
        "    ent_before = entropy_per_image(x)\n",
        "    ent_after  = entropy_per_image(x_after)\n",
        "    con_before = contrast_proxy(x)\n",
        "    con_after  = contrast_proxy(x_after)\n",
        "\n",
        "    dfm = pd.DataFrame({\n",
        "        \"edge_energy_before\": ee_before,\n",
        "        \"edge_energy_after\": ee_after,\n",
        "        \"entropy_before\": ent_before,\n",
        "        \"entropy_after\": ent_after,\n",
        "        \"contrast_before\": con_before,\n",
        "        \"contrast_after\": con_after,\n",
        "        \"edge_gain_ratio\": (ee_after / np.clip(ee_before, 1e-9, None)),\n",
        "        \"entropy_delta\": (ent_after - ent_before),\n",
        "        \"contrast_delta\": (con_after - con_before),\n",
        "    })\n",
        "    summary = dfm.agg([\"mean\", \"std\", \"min\", \"max\"]).T.reset_index().rename(columns={\"index\": \"metric\"})\n",
        "    return dfm, summary, x, x_after\n",
        "\n",
        "preproc_summary_df = pd.DataFrame()\n",
        "if CFG[\"use_preprocessing\"]:\n",
        "    preproc_df, preproc_summary_df, _, _ = run_preproc_validation(val_frames[\"ds1\"], pre_by_ds[\"ds1\"], CFG[\"preproc_val_sample_n\"])\n",
        "    print_table(preproc_summary_df, \"Preprocessing validation summary (DS1 VAL sample)\")\n",
        "    add_table_to_csv(preproc_summary_df, \"preprocessing_validation_summary_ds1\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 13: CURVES + CONFUSION + CALIBRATION TABLES (NO PLOTS)\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 13: COMPUTE CURVES + CONFUSION + CALIBRATION TABLES (NO PLOTS)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "def multiclass_calibration_curve(y_true, p_pred, n_bins=12):\n",
        "    conf = np.max(p_pred, axis=1)\n",
        "    pred = np.argmax(p_pred, axis=1)\n",
        "    acc = (pred == y_true).astype(np.float32)\n",
        "\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    bin_ids = np.digitize(conf, bins) - 1\n",
        "    bin_ids = np.clip(bin_ids, 0, n_bins - 1)\n",
        "\n",
        "    bin_conf, bin_acc, bin_count = [], [], []\n",
        "    for b in range(n_bins):\n",
        "        m = bin_ids == b\n",
        "        if m.sum() == 0:\n",
        "            bin_conf.append(np.nan)\n",
        "            bin_acc.append(np.nan)\n",
        "            bin_count.append(0)\n",
        "        else:\n",
        "            bin_conf.append(conf[m].mean())\n",
        "            bin_acc.append(acc[m].mean())\n",
        "            bin_count.append(int(m.sum()))\n",
        "    return np.array(bin_conf), np.array(bin_acc), np.array(bin_count)\n",
        "\n",
        "if len(ds1_mets) > 0:\n",
        "    met0, n0, y_true_test, p_test_best, gid0 = ds1_mets[0]\n",
        "    if len(y_true_test) > 0:\n",
        "        roc_rows, pr_rows = [], []\n",
        "        for c in range(NUM_CLASSES):\n",
        "            yc = (y_true_test == c).astype(int)\n",
        "            if yc.sum() == 0 or yc.sum() == len(yc):\n",
        "                continue\n",
        "            fpr, tpr, thr_roc = roc_curve(yc, p_test_best[:, c])\n",
        "            for i in range(len(fpr)):\n",
        "                roc_rows.append({\"class\": labels[c], \"fpr\": float(fpr[i]), \"tpr\": float(tpr[i]), \"threshold\": float(thr_roc[i])})\n",
        "\n",
        "            prec, rec, thr_pr = precision_recall_curve(yc, p_test_best[:, c])\n",
        "            for i in range(len(prec)):\n",
        "                thr_val = float(thr_pr[i]) if i < len(thr_pr) else np.nan\n",
        "                pr_rows.append({\"class\": labels[c], \"precision\": float(prec[i]), \"recall\": float(rec[i]), \"threshold\": thr_val})\n",
        "\n",
        "        roc_df = pd.DataFrame(roc_rows)\n",
        "        pr_df = pd.DataFrame(pr_rows)\n",
        "        print_table(roc_df.head(20), \"ROC curve points (DS1 TEST, first 20 rows)\")\n",
        "        print_table(pr_df.head(20), \"PR curve points (DS1 TEST, first 20 rows)\")\n",
        "        add_table_to_csv(roc_df, \"roc_curve_points_ds1\")\n",
        "        add_table_to_csv(pr_df, \"pr_curve_points_ds1\")\n",
        "\n",
        "        y_hat_test = np.argmax(p_test_best, axis=1)\n",
        "        cm_counts = confusion_matrix(y_true_test, y_hat_test, labels=list(range(NUM_CLASSES)))\n",
        "        cm_norm = cm_counts / np.clip(cm_counts.sum(axis=1, keepdims=True), 1, None)\n",
        "\n",
        "        cm_counts_df = pd.DataFrame(cm_counts, index=labels, columns=labels).reset_index().rename(columns={\"index\": \"true\"})\n",
        "        cm_norm_df   = pd.DataFrame(cm_norm,   index=labels, columns=labels).reset_index().rename(columns={\"index\": \"true\"})\n",
        "        print_table(cm_counts_df, \"Confusion matrix counts (DS1 TEST)\")\n",
        "        print_table(cm_norm_df, \"Confusion matrix row-normalized (DS1 TEST)\")\n",
        "        add_table_to_csv(cm_counts_df, \"confusion_counts_ds1\")\n",
        "        add_table_to_csv(cm_norm_df, \"confusion_norm_ds1\")\n",
        "\n",
        "        bin_conf, bin_acc, bin_n = multiclass_calibration_curve(y_true_test, p_test_best, n_bins=12)\n",
        "        cal_df = pd.DataFrame({\"bin_confidence\": bin_conf, \"bin_accuracy\": bin_acc, \"bin_count\": bin_n})\n",
        "        print_table(cal_df, \"Calibration bins table (DS1)\")\n",
        "        add_table_to_csv(cal_df, \"calibration_bins_ds1\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 14: THETA EVOLUTION TABLE\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 14: THETA EVOLUTION TABLE\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "theta_cols = [\n",
        "    \"gamma_power\",\n",
        "    \"alpha_contrast_weight\",\n",
        "    \"beta_contrast_sharpness\",\n",
        "    \"tau_clip\",\n",
        "    \"k_blur_kernel_size\",\n",
        "    \"sh_sharpen_strength\",\n",
        "    \"dn_denoise_strength\",\n",
        "]\n",
        "for c in theta_cols:\n",
        "    if c in loc_df.columns:\n",
        "        loc_df[c] = pd.to_numeric(loc_df[c], errors=\"coerce\")\n",
        "\n",
        "theta_evo = loc_df.groupby(\"round\")[theta_cols].mean(numeric_only=True).reset_index()\n",
        "print_table(theta_evo, \"Mean best-θ parameters over rounds (clients averaged)\")\n",
        "add_table_to_csv(theta_evo, \"theta_evolution_mean\")\n",
        "\n",
        "# ======================================================================================\n",
        "# STEP 15: SAVE CHECKPOINT + ONE CSV\n",
        "# ======================================================================================\n",
        "print(\"\\n\" + \"=\" * 92)\n",
        "print(\"STEP 15: SAVING ONLY TWO FILES (CHECKPOINT + ONE CSV)\")\n",
        "print(\"=\" * 92)\n",
        "\n",
        "checkpoint = {\n",
        "    \"state_dict\": {k: v.detach().cpu() for k, v in global_model.state_dict().items()},\n",
        "    \"config\": CFG,\n",
        "    \"seed\": SEED,\n",
        "    \"device_used\": str(DEVICE),\n",
        "    \"dataset_roots\": DATASET_BASES,\n",
        "    \"labels\": labels,\n",
        "    \"label2id\": label2id,\n",
        "    \"id2label\": id2label,\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"backbone_name\": BACKBONE_NAME,\n",
        "    \"best_round_saved\": best_round_saved,\n",
        "    \"best_val_acc\": best_global_acc,\n",
        "    \"best_thetas\": best_thetas,\n",
        "    \"best_theta_str_by_dataset\": {ds: theta_str(best_thetas[ds]) for ds in DATASET_NAMES},\n",
        "    \"theta_fullforms\": THETA_FULLFORMS,\n",
        "    \"client_splits\": client_splits,\n",
        "    \"client_test_splits\": client_test_splits,\n",
        "    \"history_global\": glob_df.to_dict(orient=\"list\"),\n",
        "    \"history_local\": loc_df.to_dict(orient=\"list\"),\n",
        "    \"final_val_federated\": val_best,\n",
        "    \"final_test_by_dataset\": test_by_ds,\n",
        "    \"final_test_global_weighted\": global_test,\n",
        "    \"preprocessing_validation_summary_ds1\": preproc_summary_df.to_dict(orient=\"list\") if len(preproc_summary_df) else {},\n",
        "    \"total_training_time_s\": t_total,\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, MODEL_PATH)\n",
        "print(f\"✅ Saved checkpoint: {MODEL_PATH}\")\n",
        "\n",
        "all_df = pd.DataFrame(ALL_ROWS)\n",
        "all_df.to_csv(CSV_PATH, index=False)\n",
        "print(f\"✅ Saved CSV (ALL outputs): {CSV_PATH}\")\n",
        "\n",
        "print(\"\\nDONE ✅ (KAGGLE, TRUE FL SIMULATION, 12 clients (3x4 datasets), rounds=12, Preprocessing+GA, Augmentation, Fusion, PVTv2-B2, no plots)\")\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-17T07:22:58.470895Z",
          "iopub.execute_input": "2026-02-17T07:22:58.471245Z",
          "iopub.status.idle": "2026-02-17T10:00:01.490694Z",
          "shell.execute_reply.started": "2026-02-17T07:22:58.471218Z",
          "shell.execute_reply": "2026-02-17T10:00:01.489938Z"
        },
        "id": "03EWntlUgyjo",
        "outputId": "d3688271-b720-40a6-b1ea-e671368c067c",
        "colab": {
          "referenced_widgets": [
            "0745f7c117474e1681b16c53e98a8382"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "============================================================================================\nKAGGLE: TRUE FL + GA-FELCM + PVTv2-B2 (FUSION) — 12 Clients (3x4 datasets) | AUG=ON\n============================================================================================\nDEVICE: cuda | torch=2.8.0+cu126\n============================================================================================\n\n============================================================================================\nSTEP 0: DATASETS (DS1/2/3 via kagglehub) + DS4 from Kaggle input path\n============================================================================================\n✅ DS1: /kaggle/input/datasets/alamshihab075/brain-tumor-mri-dataset-for-deep-learning\n✅ DS2: /kaggle/input/datasets/zehrakucuker/brain-tumor-mri-images-classification-dataset\n✅ DS3: /kaggle/input/datasets/chubskuy/brain-tumor-image\n✅ DS4: /kaggle/input/datasets/mdzubayerahmadshibly/ds4mine\n\n============================================================================================\nSTEP 1: DISCOVER + MERGE DATASET IMAGES BY CLASS\n============================================================================================\nds1: total images = 9257 | glioma:3293, meningioma:3593, notumor:811, pituitary:1560\nds2: total images = 11615 | glioma:3768, meningioma:3806, notumor:0, pituitary:4041\nds3: total images = 7023 | glioma:1621, meningioma:1645, notumor:2000, pituitary:1757\nds4: total images = 12064 | glioma:3773, meningioma:2729, notumor:2432, pituitary:3130\n\n============================================================================================\nSTEP 2: TRAIN/VAL/TEST SPLIT (PER DATASET)\n============================================================================================\nDS1 TRAIN=6479 | VAL=1389 | TEST=1389\nDS2 TRAIN=8130 | VAL=1742 | TEST=1743\nDS3 TRAIN=4916 | VAL=1053 | TEST=1054\nDS4 TRAIN=8444 | VAL=1810 | TEST=1810\n\n============================================================================================\nSTEP 2.5: SANITY / LEAKAGE CHECKS (PER DATASET)\n============================================================================================\n\n--------------------------------------------------------------------------------------------\nLeakage / Sanity Summary — ds1\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "   path_overlap_train_val  path_overlap_train_test  path_overlap_val_test  \\\n0                       0                        0                      0   \n\n   unique_paths_train  unique_paths_val  unique_paths_test  \\\n0                6479              1389               1389   \n\n   filename_overlap_train_val  filename_overlap_train_test  \\\n0                           0                            0   \n\n   filename_overlap_val_test  subset_hash_train_val  subset_hash_train_test  \\\n0                          0                      5                       4   \n\n   subset_hash_val_test  subset_hash_n_train  subset_hash_n_val  \\\n0                     5                  300                300   \n\n   subset_hash_n_test  \n0                 297  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path_overlap_train_val</th>\n      <th>path_overlap_train_test</th>\n      <th>path_overlap_val_test</th>\n      <th>unique_paths_train</th>\n      <th>unique_paths_val</th>\n      <th>unique_paths_test</th>\n      <th>filename_overlap_train_val</th>\n      <th>filename_overlap_train_test</th>\n      <th>filename_overlap_val_test</th>\n      <th>subset_hash_train_val</th>\n      <th>subset_hash_train_test</th>\n      <th>subset_hash_val_test</th>\n      <th>subset_hash_n_train</th>\n      <th>subset_hash_n_val</th>\n      <th>subset_hash_n_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6479</td>\n      <td>1389</td>\n      <td>1389</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>4</td>\n      <td>5</td>\n      <td>300</td>\n      <td>300</td>\n      <td>297</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nLeakage / Sanity Summary — ds2\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "   path_overlap_train_val  path_overlap_train_test  path_overlap_val_test  \\\n0                       0                        0                      0   \n\n   unique_paths_train  unique_paths_val  unique_paths_test  \\\n0                8130              1742               1743   \n\n   filename_overlap_train_val  filename_overlap_train_test  \\\n0                         589                          621   \n\n   filename_overlap_val_test  subset_hash_train_val  subset_hash_train_test  \\\n0                        203                      2                       2   \n\n   subset_hash_val_test  subset_hash_n_train  subset_hash_n_val  \\\n0                     1                  299                299   \n\n   subset_hash_n_test  \n0                 298  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path_overlap_train_val</th>\n      <th>path_overlap_train_test</th>\n      <th>path_overlap_val_test</th>\n      <th>unique_paths_train</th>\n      <th>unique_paths_val</th>\n      <th>unique_paths_test</th>\n      <th>filename_overlap_train_val</th>\n      <th>filename_overlap_train_test</th>\n      <th>filename_overlap_val_test</th>\n      <th>subset_hash_train_val</th>\n      <th>subset_hash_train_test</th>\n      <th>subset_hash_val_test</th>\n      <th>subset_hash_n_train</th>\n      <th>subset_hash_n_val</th>\n      <th>subset_hash_n_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8130</td>\n      <td>1742</td>\n      <td>1743</td>\n      <td>589</td>\n      <td>621</td>\n      <td>203</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>299</td>\n      <td>299</td>\n      <td>298</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nLeakage / Sanity Summary — ds3\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "   path_overlap_train_val  path_overlap_train_test  path_overlap_val_test  \\\n0                       0                        0                      0   \n\n   unique_paths_train  unique_paths_val  unique_paths_test  \\\n0                4916              1053               1054   \n\n   filename_overlap_train_val  filename_overlap_train_test  \\\n0                           0                            0   \n\n   filename_overlap_val_test  subset_hash_train_val  subset_hash_train_test  \\\n0                          0                      3                       3   \n\n   subset_hash_val_test  subset_hash_n_train  subset_hash_n_val  \\\n0                     2                  298                300   \n\n   subset_hash_n_test  \n0                 296  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path_overlap_train_val</th>\n      <th>path_overlap_train_test</th>\n      <th>path_overlap_val_test</th>\n      <th>unique_paths_train</th>\n      <th>unique_paths_val</th>\n      <th>unique_paths_test</th>\n      <th>filename_overlap_train_val</th>\n      <th>filename_overlap_train_test</th>\n      <th>filename_overlap_val_test</th>\n      <th>subset_hash_train_val</th>\n      <th>subset_hash_train_test</th>\n      <th>subset_hash_val_test</th>\n      <th>subset_hash_n_train</th>\n      <th>subset_hash_n_val</th>\n      <th>subset_hash_n_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4916</td>\n      <td>1053</td>\n      <td>1054</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>298</td>\n      <td>300</td>\n      <td>296</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nLeakage / Sanity Summary — ds4\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "   path_overlap_train_val  path_overlap_train_test  path_overlap_val_test  \\\n0                       0                        0                      0   \n\n   unique_paths_train  unique_paths_val  unique_paths_test  \\\n0                8444              1810               1810   \n\n   filename_overlap_train_val  filename_overlap_train_test  \\\n0                         204                          214   \n\n   filename_overlap_val_test  subset_hash_train_val  subset_hash_train_test  \\\n0                         71                      2                       1   \n\n   subset_hash_val_test  subset_hash_n_train  subset_hash_n_val  \\\n0                     0                  299                300   \n\n   subset_hash_n_test  \n0                 300  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path_overlap_train_val</th>\n      <th>path_overlap_train_test</th>\n      <th>path_overlap_val_test</th>\n      <th>unique_paths_train</th>\n      <th>unique_paths_val</th>\n      <th>unique_paths_test</th>\n      <th>filename_overlap_train_val</th>\n      <th>filename_overlap_train_test</th>\n      <th>filename_overlap_val_test</th>\n      <th>subset_hash_train_val</th>\n      <th>subset_hash_train_test</th>\n      <th>subset_hash_val_test</th>\n      <th>subset_hash_n_train</th>\n      <th>subset_hash_n_val</th>\n      <th>subset_hash_n_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8444</td>\n      <td>1810</td>\n      <td>1810</td>\n      <td>204</td>\n      <td>214</td>\n      <td>71</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>299</td>\n      <td>300</td>\n      <td>300</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n============================================================================================\nSTEP 3: NON-IID CLIENT PARTITIONING (3 clients per dataset => 12 total)\n============================================================================================\nDS1 Client 0 (gid 0): train=1129 tune=176 val=155\nDS1 Client 1 (gid 1): train=2446 tune=380 val=334\nDS1 Client 2 (gid 2): train=1438 tune=224 val=197\nDS2 Client 0 (gid 3): train=548 tune=86 val=75\nDS2 Client 1 (gid 4): train=2178 tune=338 val=297\nDS2 Client 2 (gid 5): train=3568 tune=553 val=487\nDS3 Client 0 (gid 6): train=781 tune=122 val=107\nDS3 Client 1 (gid 7): train=1317 tune=205 val=180\nDS3 Client 2 (gid 8): train=1706 tune=265 val=233\nDS4 Client 0 (gid 9): train=2875 tune=446 val=393\nDS4 Client 1 (gid 10): train=1672 tune=260 val=228\nDS4 Client 2 (gid 11): train=1989 tune=309 val=272\n\n--------------------------------------------------------------------------------------------\nClient class distribution (Non-IID, per dataset)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "       client dataset  total_train  total_tune  total_val  glioma  meningioma  \\\n0    client_0     ds1         1129         176        155      37           4   \n1    client_1     ds1         2446         380        334    1738         526   \n2    client_2     ds1         1438         224        197      10        1415   \n3    client_3     ds2          548          86         75     158           5   \n4    client_4     ds2         2178         338        297     112         266   \n5    client_5     ds2         3568         553        487    1772        1792   \n6    client_6     ds3          781         122        107       3          81   \n7    client_7     ds3         1317         205        180      59         593   \n8    client_8     ds3         1706         265        233     816         216   \n9    client_9     ds4         2875         446        393     263        1391   \n10  client_10     ds4         1672         260        228    1635           3   \n11  client_11     ds4         1989         309        272     146          84   \n\n    notumor  pituitary  \n0       421        667  \n1        14        168  \n2         3         10  \n3         0        385  \n4         0       1800  \n5         0          4  \n6       623         74  \n7       209        456  \n8       251        423  \n9      1199         22  \n10        4         30  \n11      114       1645  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>client</th>\n      <th>dataset</th>\n      <th>total_train</th>\n      <th>total_tune</th>\n      <th>total_val</th>\n      <th>glioma</th>\n      <th>meningioma</th>\n      <th>notumor</th>\n      <th>pituitary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>client_0</td>\n      <td>ds1</td>\n      <td>1129</td>\n      <td>176</td>\n      <td>155</td>\n      <td>37</td>\n      <td>4</td>\n      <td>421</td>\n      <td>667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>client_1</td>\n      <td>ds1</td>\n      <td>2446</td>\n      <td>380</td>\n      <td>334</td>\n      <td>1738</td>\n      <td>526</td>\n      <td>14</td>\n      <td>168</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>client_2</td>\n      <td>ds1</td>\n      <td>1438</td>\n      <td>224</td>\n      <td>197</td>\n      <td>10</td>\n      <td>1415</td>\n      <td>3</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>client_3</td>\n      <td>ds2</td>\n      <td>548</td>\n      <td>86</td>\n      <td>75</td>\n      <td>158</td>\n      <td>5</td>\n      <td>0</td>\n      <td>385</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>client_4</td>\n      <td>ds2</td>\n      <td>2178</td>\n      <td>338</td>\n      <td>297</td>\n      <td>112</td>\n      <td>266</td>\n      <td>0</td>\n      <td>1800</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>client_5</td>\n      <td>ds2</td>\n      <td>3568</td>\n      <td>553</td>\n      <td>487</td>\n      <td>1772</td>\n      <td>1792</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>client_6</td>\n      <td>ds3</td>\n      <td>781</td>\n      <td>122</td>\n      <td>107</td>\n      <td>3</td>\n      <td>81</td>\n      <td>623</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>client_7</td>\n      <td>ds3</td>\n      <td>1317</td>\n      <td>205</td>\n      <td>180</td>\n      <td>59</td>\n      <td>593</td>\n      <td>209</td>\n      <td>456</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>client_8</td>\n      <td>ds3</td>\n      <td>1706</td>\n      <td>265</td>\n      <td>233</td>\n      <td>816</td>\n      <td>216</td>\n      <td>251</td>\n      <td>423</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>client_9</td>\n      <td>ds4</td>\n      <td>2875</td>\n      <td>446</td>\n      <td>393</td>\n      <td>263</td>\n      <td>1391</td>\n      <td>1199</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>client_10</td>\n      <td>ds4</td>\n      <td>1672</td>\n      <td>260</td>\n      <td>228</td>\n      <td>1635</td>\n      <td>3</td>\n      <td>4</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>client_11</td>\n      <td>ds4</td>\n      <td>1989</td>\n      <td>309</td>\n      <td>272</td>\n      <td>146</td>\n      <td>84</td>\n      <td>114</td>\n      <td>1645</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n============================================================================================\nSTEP 4: DATA LOADERS (AUG ON) + IMAGENET NORM\n============================================================================================\nAugmentation: ON ✅\nPreprocessing: ON ✅\n\n============================================================================================\nSTEP 5: GA-TUNED ENHANCED FELCM PREPROCESSOR\n============================================================================================\n\n============================================================================================\nSTEP 6: MODEL (PVTv2-B2 + MULTI-SCALE FUSION)\n============================================================================================\n\n============================================================================================\nSTEP 7: GA FITNESS (ROBUST)\n============================================================================================\n\n============================================================================================\nSTEP 8: TRAIN / EVAL UTILITIES (FULL METRICS)\n============================================================================================\n\n============================================================================================\nSTEP 9: INITIALIZING GLOBAL MODEL\n============================================================================================\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/101M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0745f7c117474e1681b16c53e98a8382"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Backbone: pvt_v2_b2 | Total params: 25,990,026 | Trainable: 1,140,170 (4.39%)\n\n--------------------------------------------------------------------------------------------\nHyperparameters / Search Space\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                        hp_name  \\\n0           clients_per_dataset   \n1                 clients_total   \n2                        rounds   \n3                  local_epochs   \n4                            lr   \n5                  weight_decay   \n6                 warmup_epochs   \n7               label_smoothing   \n8                     grad_clip   \n9                    fedprox_mu   \n10                     img_size   \n11                   batch_size   \n12                  num_workers   \n13              global_val_frac   \n14                    test_frac   \n15              client_val_frac   \n16             client_tune_frac   \n17     min_per_class_per_client   \n18              dirichlet_alpha   \n19            use_preprocessing   \n20                       use_ga   \n21                       ga_pop   \n22                      ga_gens   \n23                    ga_elites   \n24               elite_pool_max   \n25             use_augmentation   \n26                     cond_dim   \n27                 head_dropout   \n28         unfreeze_after_round   \n29             unfreeze_lr_mult   \n30           unfreeze_tail_frac   \n31  quick_hash_subset_per_split   \n32         preproc_val_sample_n   \n33               before_after_n   \n34                     ds1_base   \n35                     ds2_base   \n36                     ds3_base   \n37                     ds4_base   \n38              GA_theta_ranges   \n39              theta_fullforms   \n40                backbone_name   \n41                         norm   \n\n                                             hp_value  \n0                                                   3  \n1                                                  12  \n2                                                  12  \n3                                                   2  \n4                                               0.001  \n5                                              0.0005  \n6                                                   1  \n7                                                0.08  \n8                                                 1.0  \n9                                                0.01  \n10                                                224  \n11                                                 20  \n12                                                  2  \n13                                               0.15  \n14                                               0.15  \n15                                               0.12  \n16                                               0.12  \n17                                                  5  \n18                                               0.35  \n19                                               True  \n20                                               True  \n21                                                 10  \n22                                                  5  \n23                                                  3  \n24                                                 18  \n25                                               True  \n26                                                128  \n27                                                0.3  \n28                                                  3  \n29                                                0.1  \n30                                               0.17  \n31                                                300  \n32                                                500  \n33                                                 12  \n34  /kaggle/input/datasets/alamshihab075/brain-tum...  \n35  /kaggle/input/datasets/zehrakucuker/brain-tumo...  \n36  /kaggle/input/datasets/chubskuy/brain-tumor-image  \n37  /kaggle/input/datasets/mdzubayerahmadshibly/ds...  \n38  gamma∈[0.7,1.4], alpha∈[0.15,0.55], beta∈[3,9]...  \n39  {'gamma': 'Power transform exponent (γ)', 'alp...  \n40                                          pvt_v2_b2  \n41                                  ImageNet mean/std  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hp_name</th>\n      <th>hp_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>clients_per_dataset</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>clients_total</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>rounds</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>local_epochs</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>lr</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>weight_decay</td>\n      <td>0.0005</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>warmup_epochs</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>label_smoothing</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>grad_clip</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fedprox_mu</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>img_size</td>\n      <td>224</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>batch_size</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>num_workers</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>global_val_frac</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>test_frac</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>client_val_frac</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>client_tune_frac</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>min_per_class_per_client</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>dirichlet_alpha</td>\n      <td>0.35</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>use_preprocessing</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>use_ga</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>ga_pop</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>ga_gens</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>ga_elites</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>elite_pool_max</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>use_augmentation</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>cond_dim</td>\n      <td>128</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>head_dropout</td>\n      <td>0.3</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>unfreeze_after_round</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>unfreeze_lr_mult</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>unfreeze_tail_frac</td>\n      <td>0.17</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>quick_hash_subset_per_split</td>\n      <td>300</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>preproc_val_sample_n</td>\n      <td>500</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>before_after_n</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>ds1_base</td>\n      <td>/kaggle/input/datasets/alamshihab075/brain-tum...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>ds2_base</td>\n      <td>/kaggle/input/datasets/zehrakucuker/brain-tumo...</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>ds3_base</td>\n      <td>/kaggle/input/datasets/chubskuy/brain-tumor-image</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>ds4_base</td>\n      <td>/kaggle/input/datasets/mdzubayerahmadshibly/ds...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>GA_theta_ranges</td>\n      <td>gamma∈[0.7,1.4], alpha∈[0.15,0.55], beta∈[3,9]...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>theta_fullforms</td>\n      <td>{'gamma': 'Power transform exponent (γ)', 'alp...</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>backbone_name</td>\n      <td>pvt_v2_b2</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>norm</td>\n      <td>ImageNet mean/std</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n============================================================================================\nSTEP 10: FEDERATED TRAINING (NO CENTRAL VAL/TEST)\n============================================================================================\n\n============================================================================================\nROUND 1/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.7879 | val_acc=0.9742 | val_f1=0.7372 | val_auc=0.9945 | val_logloss=0.1241 | GA_fit=9.638 | ga_time=12.1s | theta=(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.03)\nClient 1 (ds1) | train_acc=0.8013 | val_acc=0.9431 | val_f1=0.9340 | val_auc=0.9889 | val_logloss=0.2335 | GA_fit=10.714 | ga_time=5.8s | theta=(γ=1.32, α=0.20, β=6.0, τ=3.1, k=5, sh=0.11, dn=0.28)\nClient 2 (ds1) | train_acc=0.8383 | val_acc=0.9746 | val_f1=0.3307 | val_auc=0.8940 | val_logloss=0.1943 | GA_fit=0.198 | ga_time=5.9s | theta=(γ=0.78, α=0.46, β=3.5, τ=2.8, k=7, sh=0.28, dn=0.04)\nClient 3 (ds2) | train_acc=0.6870 | val_acc=0.9067 | val_f1=0.5905 | val_auc=nan | val_logloss=0.3020 | GA_fit=6.868 | ga_time=6.0s | theta=(γ=0.64, α=0.52, β=5.4, τ=2.7, k=3, sh=0.29, dn=0.10)\nClient 4 (ds2) | train_acc=0.7631 | val_acc=0.9428 | val_f1=0.8897 | val_auc=nan | val_logloss=0.2321 | GA_fit=0.209 | ga_time=6.0s | theta=(γ=0.67, α=0.56, β=5.9, τ=2.5, k=3, sh=0.33, dn=0.02)\nClient 5 (ds2) | train_acc=0.8494 | val_acc=0.9528 | val_f1=0.9684 | val_auc=nan | val_logloss=0.1921 | GA_fit=4.164 | ga_time=6.0s | theta=(γ=0.61, α=0.52, β=5.8, τ=2.8, k=7, sh=0.28, dn=0.13)\nClient 6 (ds3) | train_acc=0.6850 | val_acc=0.9159 | val_f1=0.6256 | val_auc=0.9806 | val_logloss=0.2475 | GA_fit=13.143 | ga_time=5.9s | theta=(γ=1.13, α=0.20, β=4.5, τ=3.3, k=3, sh=0.12, dn=0.15)\nClient 7 (ds3) | train_acc=0.6515 | val_acc=0.8111 | val_f1=0.7592 | val_auc=0.9629 | val_logloss=0.4785 | GA_fit=9.825 | ga_time=6.0s | theta=(γ=1.16, α=0.36, β=4.5, τ=2.4, k=3, sh=0.12, dn=0.15)\nClient 8 (ds3) | train_acc=0.6937 | val_acc=0.9056 | val_f1=0.8599 | val_auc=0.9745 | val_logloss=0.3159 | GA_fit=12.264 | ga_time=6.0s | theta=(γ=1.18, α=0.53, β=5.2, τ=3.0, k=5, sh=0.23, dn=0.04)\nClient 9 (ds4) | train_acc=0.7468 | val_acc=0.9008 | val_f1=0.7182 | val_auc=0.9721 | val_logloss=0.3420 | GA_fit=6.640 | ga_time=5.9s | theta=(γ=0.92, α=0.52, β=3.9, τ=2.2, k=3, sh=0.27, dn=0.01)\nClient 10 (ds4) | train_acc=0.8409 | val_acc=0.9868 | val_f1=0.5978 | val_auc=nan | val_logloss=0.1231 | GA_fit=0.210 | ga_time=5.9s | theta=(γ=0.95, α=0.55, β=2.5, τ=2.0, k=3, sh=0.33, dn=0.00)\nClient 11 (ds4) | train_acc=0.6594 | val_acc=0.8824 | val_f1=0.7185 | val_auc=0.9349 | val_logloss=0.3614 | GA_fit=0.232 | ga_time=5.9s | theta=(γ=0.90, α=0.62, β=2.0, τ=2.2, k=3, sh=0.35, dn=0.00)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 1) | acc=0.9277 | f1=0.7740 | logloss=0.2595 | loss_ce=0.2571 | round_time=806.9s | thetas={'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.03)', 'ds2': '(γ=0.67, α=0.56, β=5.9, τ=2.5, k=3, sh=0.33, dn=0.02)', 'ds3': '(γ=1.13, α=0.20, β=4.5, τ=3.3, k=3, sh=0.12, dn=0.15)', 'ds4': '(γ=0.90, α=0.48, β=3.9, τ=2.3, k=3, sh=0.26, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9277 at round=1\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 2/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9686 | val_acc=1.0000 | val_f1=1.0000 | val_auc=1.0000 | val_logloss=0.0659 | GA_fit=8.835 | ga_time=5.8s | theta=(γ=1.50, α=0.47, β=3.0, τ=3.3, k=5, sh=0.25, dn=0.29)\nClient 1 (ds1) | train_acc=0.9332 | val_acc=0.9611 | val_f1=0.9616 | val_auc=0.9966 | val_logloss=0.1825 | GA_fit=4.497 | ga_time=5.8s | theta=(γ=1.23, α=0.08, β=9.1, τ=2.4, k=5, sh=0.01, dn=0.07)\nClient 2 (ds1) | train_acc=0.9729 | val_acc=0.9746 | val_f1=0.5801 | val_auc=0.9953 | val_logloss=0.1479 | GA_fit=0.199 | ga_time=5.8s | theta=(γ=0.82, α=0.37, β=2.0, τ=1.7, k=5, sh=0.27, dn=0.01)\nClient 3 (ds2) | train_acc=0.9617 | val_acc=0.9600 | val_f1=0.6414 | val_auc=nan | val_logloss=0.2060 | GA_fit=10.626 | ga_time=5.8s | theta=(γ=0.81, α=0.49, β=4.4, τ=2.7, k=7, sh=0.35, dn=0.10)\nClient 4 (ds2) | train_acc=0.9268 | val_acc=0.9798 | val_f1=0.9617 | val_auc=nan | val_logloss=0.1371 | GA_fit=0.220 | ga_time=5.9s | theta=(γ=0.64, α=0.66, β=3.2, τ=2.4, k=3, sh=0.33, dn=0.10)\nClient 5 (ds2) | train_acc=0.9532 | val_acc=0.9651 | val_f1=0.9767 | val_auc=nan | val_logloss=0.1719 | GA_fit=2.942 | ga_time=5.8s | theta=(γ=1.02, α=0.15, β=5.9, τ=3.4, k=3, sh=0.06, dn=0.20)\nClient 6 (ds3) | train_acc=0.9392 | val_acc=0.9533 | val_f1=0.8439 | val_auc=0.9896 | val_logloss=0.1847 | GA_fit=16.173 | ga_time=5.8s | theta=(γ=1.11, α=0.08, β=4.6, τ=3.7, k=3, sh=0.28, dn=0.13)\nClient 7 (ds3) | train_acc=0.9142 | val_acc=0.9167 | val_f1=0.8891 | val_auc=0.9944 | val_logloss=0.2616 | GA_fit=11.054 | ga_time=5.8s | theta=(γ=0.96, α=0.22, β=5.0, τ=3.8, k=3, sh=0.09, dn=0.20)\nClient 8 (ds3) | train_acc=0.8892 | val_acc=0.9528 | val_f1=0.9384 | val_auc=0.9963 | val_logloss=0.1850 | GA_fit=12.811 | ga_time=5.8s | theta=(γ=1.14, α=0.17, β=7.2, τ=3.3, k=3, sh=0.06, dn=0.20)\nClient 9 (ds4) | train_acc=0.9153 | val_acc=0.9440 | val_f1=0.8136 | val_auc=0.9901 | val_logloss=0.2286 | GA_fit=8.524 | ga_time=5.8s | theta=(γ=1.09, α=0.70, β=2.0, τ=2.5, k=3, sh=0.32, dn=0.01)\nClient 10 (ds4) | train_acc=0.9746 | val_acc=0.9825 | val_f1=0.4477 | val_auc=nan | val_logloss=0.1383 | GA_fit=0.219 | ga_time=5.7s | theta=(γ=0.90, α=0.67, β=2.0, τ=2.0, k=3, sh=0.35, dn=0.00)\nClient 11 (ds4) | train_acc=0.8924 | val_acc=0.9449 | val_f1=0.8386 | val_auc=0.9580 | val_logloss=0.2252 | GA_fit=0.233 | ga_time=5.7s | theta=(γ=0.94, α=0.70, β=2.0, τ=2.1, k=3, sh=0.32, dn=0.00)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 2) | acc=0.9608 | f1=0.8515 | logloss=0.1801 | loss_ce=0.1785 | round_time=723.8s | thetas={'ds1': '(γ=1.32, α=0.20, β=6.0, τ=3.1, k=5, sh=0.11, dn=0.28)', 'ds2': '(γ=0.68, α=0.44, β=4.5, τ=2.5, k=7, sh=0.27, dn=0.12)', 'ds3': '(γ=1.11, α=0.08, β=4.6, τ=3.7, k=3, sh=0.28, dn=0.13)', 'ds4': '(γ=0.90, α=0.48, β=3.9, τ=2.3, k=3, sh=0.26, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9608 at round=2\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 3/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9841 | val_acc=0.9742 | val_f1=0.7375 | val_auc=0.9994 | val_logloss=0.1200 | GA_fit=9.898 | ga_time=5.7s | theta=(γ=1.24, α=0.56, β=2.0, τ=2.2, k=3, sh=0.25, dn=0.00)\nClient 1 (ds1) | train_acc=0.9591 | val_acc=0.9880 | val_f1=0.9840 | val_auc=0.9979 | val_logloss=0.1379 | GA_fit=4.123 | ga_time=5.8s | theta=(γ=1.39, α=0.10, β=6.4, τ=3.5, k=3, sh=0.00, dn=0.30)\nClient 2 (ds1) | train_acc=0.9861 | val_acc=0.9898 | val_f1=0.7487 | val_auc=0.9898 | val_logloss=0.0936 | GA_fit=0.207 | ga_time=5.8s | theta=(γ=0.76, α=0.55, β=2.4, τ=2.5, k=5, sh=0.26, dn=0.10)\nClient 3 (ds2) | train_acc=0.9745 | val_acc=0.9733 | val_f1=0.6524 | val_auc=nan | val_logloss=0.1859 | GA_fit=4.694 | ga_time=5.8s | theta=(γ=0.70, α=0.66, β=4.4, τ=3.2, k=5, sh=0.30, dn=0.09)\nClient 4 (ds2) | train_acc=0.9444 | val_acc=0.9798 | val_f1=0.9682 | val_auc=nan | val_logloss=0.1292 | GA_fit=5.448 | ga_time=5.8s | theta=(γ=0.60, α=0.52, β=5.7, τ=3.2, k=3, sh=0.27, dn=0.02)\nClient 5 (ds2) | train_acc=0.9561 | val_acc=0.9795 | val_f1=0.9863 | val_auc=nan | val_logloss=0.1414 | GA_fit=2.582 | ga_time=5.9s | theta=(γ=0.61, α=0.45, β=4.4, τ=2.0, k=7, sh=0.16, dn=0.00)\nClient 6 (ds3) | train_acc=0.9680 | val_acc=0.9907 | val_f1=0.9877 | val_auc=0.9994 | val_logloss=0.0844 | GA_fit=12.460 | ga_time=5.8s | theta=(γ=1.39, α=0.36, β=4.5, τ=3.3, k=3, sh=0.11, dn=0.05)\nClient 7 (ds3) | train_acc=0.9374 | val_acc=0.9722 | val_f1=0.9527 | val_auc=0.9988 | val_logloss=0.1402 | GA_fit=9.926 | ga_time=5.8s | theta=(γ=1.14, α=0.22, β=5.3, τ=2.6, k=3, sh=0.29, dn=0.12)\nClient 8 (ds3) | train_acc=0.9232 | val_acc=0.9485 | val_f1=0.9314 | val_auc=0.9936 | val_logloss=0.2136 | GA_fit=10.615 | ga_time=5.8s | theta=(γ=0.99, α=0.16, β=7.1, τ=3.3, k=3, sh=0.20, dn=0.15)\nClient 9 (ds4) | train_acc=0.9403 | val_acc=0.9415 | val_f1=0.8269 | val_auc=0.9899 | val_logloss=0.2258 | GA_fit=9.902 | ga_time=5.8s | theta=(γ=0.72, α=0.21, β=5.7, τ=2.5, k=7, sh=0.00, dn=0.15)\nClient 10 (ds4) | train_acc=0.9833 | val_acc=0.9868 | val_f1=0.4705 | val_auc=nan | val_logloss=0.1368 | GA_fit=0.214 | ga_time=5.7s | theta=(γ=0.88, α=0.68, β=2.4, τ=1.9, k=3, sh=0.34, dn=0.00)\nClient 11 (ds4) | train_acc=0.9437 | val_acc=0.9559 | val_f1=0.8632 | val_auc=0.9847 | val_logloss=0.1975 | GA_fit=13.342 | ga_time=5.8s | theta=(γ=1.39, α=0.46, β=4.9, τ=3.3, k=3, sh=0.18, dn=0.00)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 3) | acc=0.9716 | f1=0.8683 | logloss=0.1562 | loss_ce=0.1547 | round_time=781.1s | thetas={'ds1': '(γ=1.25, α=0.57, β=2.3, τ=2.2, k=5, sh=0.02, dn=0.00)', 'ds2': '(γ=0.64, α=0.52, β=5.4, τ=2.7, k=3, sh=0.29, dn=0.10)', 'ds3': '(γ=1.13, α=0.20, β=4.5, τ=3.3, k=3, sh=0.12, dn=0.15)', 'ds4': '(γ=0.90, α=0.48, β=3.9, τ=2.3, k=3, sh=0.26, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9716 at round=3\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 4/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9867 | val_acc=0.9871 | val_f1=0.9673 | val_auc=0.9996 | val_logloss=0.0785 | GA_fit=7.780 | ga_time=5.8s | theta=(γ=1.29, α=0.44, β=2.1, τ=3.4, k=3, sh=0.24, dn=0.12)\nClient 1 (ds1) | train_acc=0.9663 | val_acc=0.9790 | val_f1=0.9803 | val_auc=0.9989 | val_logloss=0.1428 | GA_fit=6.419 | ga_time=5.8s | theta=(γ=1.17, α=0.08, β=6.4, τ=2.7, k=7, sh=0.11, dn=0.00)\nClient 2 (ds1) | train_acc=0.9910 | val_acc=0.9949 | val_f1=0.9160 | val_auc=1.0000 | val_logloss=0.1077 | GA_fit=0.212 | ga_time=5.7s | theta=(γ=0.93, α=0.67, β=2.0, τ=1.5, k=5, sh=0.24, dn=0.12)\nClient 3 (ds2) | train_acc=0.9754 | val_acc=0.9733 | val_f1=0.6526 | val_auc=nan | val_logloss=0.1542 | GA_fit=7.612 | ga_time=5.8s | theta=(γ=0.83, α=0.63, β=5.2, τ=2.8, k=3, sh=0.30, dn=0.20)\nClient 4 (ds2) | train_acc=0.9674 | val_acc=0.9832 | val_f1=0.9565 | val_auc=nan | val_logloss=0.1224 | GA_fit=7.669 | ga_time=5.8s | theta=(γ=0.94, α=0.37, β=5.1, τ=3.0, k=3, sh=0.05, dn=0.09)\nClient 5 (ds2) | train_acc=0.9714 | val_acc=0.9713 | val_f1=0.9808 | val_auc=nan | val_logloss=0.1614 | GA_fit=2.288 | ga_time=5.8s | theta=(γ=0.75, α=0.16, β=6.9, τ=2.9, k=3, sh=0.16, dn=0.10)\nClient 6 (ds3) | train_acc=0.9641 | val_acc=0.9533 | val_f1=0.8439 | val_auc=0.9983 | val_logloss=0.1394 | GA_fit=17.951 | ga_time=5.7s | theta=(γ=0.96, α=0.15, β=7.4, τ=3.4, k=3, sh=0.09, dn=0.03)\nClient 7 (ds3) | train_acc=0.9689 | val_acc=0.9611 | val_f1=0.9342 | val_auc=0.9991 | val_logloss=0.1386 | GA_fit=9.973 | ga_time=5.8s | theta=(γ=1.16, α=0.55, β=7.2, τ=2.4, k=3, sh=0.12, dn=0.15)\nClient 8 (ds3) | train_acc=0.9505 | val_acc=0.9657 | val_f1=0.9536 | val_auc=0.9966 | val_logloss=0.1667 | GA_fit=8.967 | ga_time=5.8s | theta=(γ=1.01, α=0.47, β=7.2, τ=3.3, k=3, sh=0.18, dn=0.00)\nClient 9 (ds4) | train_acc=0.9628 | val_acc=0.9644 | val_f1=0.9298 | val_auc=0.9921 | val_logloss=0.1789 | GA_fit=6.397 | ga_time=5.8s | theta=(γ=0.88, α=0.58, β=2.0, τ=3.1, k=5, sh=0.34, dn=0.01)\nClient 10 (ds4) | train_acc=0.9862 | val_acc=0.9781 | val_f1=0.4477 | val_auc=nan | val_logloss=0.1474 | GA_fit=0.223 | ga_time=5.7s | theta=(γ=0.92, α=0.70, β=2.0, τ=2.1, k=3, sh=0.35, dn=0.00)\nClient 11 (ds4) | train_acc=0.9507 | val_acc=0.9669 | val_f1=0.8919 | val_auc=0.9689 | val_logloss=0.1804 | GA_fit=5.451 | ga_time=5.8s | theta=(γ=0.87, α=0.70, β=3.4, τ=1.9, k=5, sh=0.35, dn=0.00)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 4) | acc=0.9733 | f1=0.8990 | logloss=0.1485 | loss_ce=0.1469 | round_time=764.0s | thetas={'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.03)', 'ds2': '(γ=0.67, α=0.56, β=5.9, τ=2.5, k=3, sh=0.33, dn=0.02)', 'ds3': '(γ=1.18, α=0.53, β=5.2, τ=3.0, k=5, sh=0.23, dn=0.04)', 'ds4': '(γ=0.90, α=0.48, β=3.9, τ=2.3, k=3, sh=0.26, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9733 at round=4\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 5/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9898 | val_acc=0.9871 | val_f1=0.9930 | val_auc=1.0000 | val_logloss=0.0901 | GA_fit=19.326 | ga_time=5.8s | theta=(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.00)\nClient 1 (ds1) | train_acc=0.9769 | val_acc=0.9850 | val_f1=0.9850 | val_auc=0.9996 | val_logloss=0.1233 | GA_fit=3.458 | ga_time=5.8s | theta=(γ=0.91, α=0.29, β=4.7, τ=3.5, k=5, sh=0.07, dn=0.23)\nClient 2 (ds1) | train_acc=0.9955 | val_acc=0.9949 | val_f1=0.9160 | val_auc=1.0000 | val_logloss=0.1094 | GA_fit=0.208 | ga_time=5.8s | theta=(γ=0.80, α=0.59, β=2.6, τ=2.2, k=7, sh=0.26, dn=0.07)\nClient 3 (ds2) | train_acc=0.9854 | val_acc=0.9333 | val_f1=0.6166 | val_auc=nan | val_logloss=0.3128 | GA_fit=6.756 | ga_time=5.8s | theta=(γ=0.94, α=0.14, β=9.3, τ=3.4, k=7, sh=0.00, dn=0.11)\nClient 4 (ds2) | train_acc=0.9720 | val_acc=0.9899 | val_f1=0.9780 | val_auc=nan | val_logloss=0.1067 | GA_fit=6.379 | ga_time=5.8s | theta=(γ=0.80, α=0.45, β=4.5, τ=2.9, k=3, sh=0.30, dn=0.13)\nClient 5 (ds2) | train_acc=0.9744 | val_acc=0.9754 | val_f1=0.9835 | val_auc=nan | val_logloss=0.1433 | GA_fit=5.202 | ga_time=5.8s | theta=(γ=0.97, α=0.08, β=5.8, τ=3.3, k=3, sh=0.08, dn=0.20)\nClient 6 (ds3) | train_acc=0.9795 | val_acc=0.9626 | val_f1=0.8779 | val_auc=0.9995 | val_logloss=0.1203 | GA_fit=10.465 | ga_time=5.7s | theta=(γ=1.13, α=0.12, β=5.2, τ=3.4, k=3, sh=0.15, dn=0.14)\nClient 7 (ds3) | train_acc=0.9666 | val_acc=0.9889 | val_f1=0.9929 | val_auc=0.9993 | val_logloss=0.1083 | GA_fit=13.041 | ga_time=5.8s | theta=(γ=0.96, α=0.09, β=5.0, τ=3.4, k=7, sh=0.11, dn=0.16)\nClient 8 (ds3) | train_acc=0.9672 | val_acc=0.9700 | val_f1=0.9586 | val_auc=0.9936 | val_logloss=0.1567 | GA_fit=14.891 | ga_time=5.8s | theta=(γ=1.10, α=0.18, β=6.9, τ=3.2, k=3, sh=0.02, dn=0.22)\nClient 9 (ds4) | train_acc=0.9637 | val_acc=0.9644 | val_f1=0.8604 | val_auc=0.9930 | val_logloss=0.1715 | GA_fit=9.932 | ga_time=5.8s | theta=(γ=1.15, α=0.59, β=2.8, τ=2.7, k=3, sh=0.35, dn=0.07)\nClient 10 (ds4) | train_acc=0.9937 | val_acc=0.9868 | val_f1=0.6650 | val_auc=nan | val_logloss=0.1307 | GA_fit=0.219 | ga_time=5.7s | theta=(γ=0.91, α=0.70, β=2.0, τ=1.7, k=5, sh=0.31, dn=0.01)\nClient 11 (ds4) | train_acc=0.9703 | val_acc=0.9632 | val_f1=0.8855 | val_auc=0.9863 | val_logloss=0.1977 | GA_fit=0.240 | ga_time=5.7s | theta=(γ=0.90, α=0.70, β=2.0, τ=2.1, k=3, sh=0.35, dn=0.00)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 5) | acc=0.9770 | f1=0.9147 | logloss=0.1425 | loss_ce=0.1411 | round_time=762.7s | thetas={'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.03)', 'ds2': '(γ=0.81, α=0.49, β=4.4, τ=2.7, k=7, sh=0.35, dn=0.10)', 'ds3': '(γ=1.11, α=0.08, β=4.6, τ=3.7, k=3, sh=0.28, dn=0.13)', 'ds4': '(γ=0.90, α=0.48, β=3.9, τ=2.3, k=3, sh=0.26, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9770 at round=5\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 6/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9898 | val_acc=0.9742 | val_f1=0.9860 | val_auc=0.9948 | val_logloss=0.1194 | GA_fit=10.244 | ga_time=5.8s | theta=(γ=0.99, α=0.20, β=6.0, τ=3.1, k=5, sh=0.11, dn=0.28)\nClient 1 (ds1) | train_acc=0.9787 | val_acc=0.9820 | val_f1=0.9827 | val_auc=0.9995 | val_logloss=0.1327 | GA_fit=10.108 | ga_time=5.8s | theta=(γ=1.39, α=0.15, β=6.9, τ=3.3, k=5, sh=0.14, dn=0.14)\nClient 2 (ds1) | train_acc=0.9906 | val_acc=0.9848 | val_f1=0.7897 | val_auc=0.9962 | val_logloss=0.1283 | GA_fit=0.214 | ga_time=5.7s | theta=(γ=0.93, α=0.68, β=2.0, τ=1.8, k=3, sh=0.20, dn=0.13)\nClient 3 (ds2) | train_acc=0.9845 | val_acc=0.9733 | val_f1=0.6524 | val_auc=nan | val_logloss=0.1176 | GA_fit=10.896 | ga_time=5.8s | theta=(γ=0.99, α=0.15, β=5.4, τ=2.9, k=7, sh=0.06, dn=0.07)\nClient 4 (ds2) | train_acc=0.9784 | val_acc=0.9899 | val_f1=0.9843 | val_auc=nan | val_logloss=0.1138 | GA_fit=10.553 | ga_time=5.8s | theta=(γ=0.60, α=0.50, β=7.1, τ=2.5, k=3, sh=0.24, dn=0.05)\nClient 5 (ds2) | train_acc=0.9797 | val_acc=0.9836 | val_f1=0.9890 | val_auc=nan | val_logloss=0.1231 | GA_fit=3.678 | ga_time=5.8s | theta=(γ=1.26, α=0.23, β=5.8, τ=3.2, k=3, sh=0.11, dn=0.18)\nClient 6 (ds3) | train_acc=0.9859 | val_acc=0.9813 | val_f1=0.9018 | val_auc=0.9999 | val_logloss=0.0563 | GA_fit=14.772 | ga_time=5.8s | theta=(γ=1.25, α=0.47, β=5.2, τ=2.2, k=3, sh=0.13, dn=0.18)\nClient 7 (ds3) | train_acc=0.9753 | val_acc=0.9778 | val_f1=0.9858 | val_auc=0.9993 | val_logloss=0.1340 | GA_fit=11.043 | ga_time=5.8s | theta=(γ=1.10, α=0.16, β=4.0, τ=3.8, k=3, sh=0.00, dn=0.13)\nClient 8 (ds3) | train_acc=0.9669 | val_acc=0.9742 | val_f1=0.9684 | val_auc=0.9991 | val_logloss=0.1370 | GA_fit=18.623 | ga_time=5.8s | theta=(γ=1.11, α=0.10, β=6.4, τ=3.8, k=3, sh=0.11, dn=0.09)\nClient 9 (ds4) | train_acc=0.9744 | val_acc=0.9746 | val_f1=0.9402 | val_auc=0.9989 | val_logloss=0.1271 | GA_fit=12.183 | ga_time=5.8s | theta=(γ=0.92, α=0.52, β=2.0, τ=2.4, k=7, sh=0.28, dn=0.07)\nClient 10 (ds4) | train_acc=0.9955 | val_acc=0.9868 | val_f1=0.4705 | val_auc=nan | val_logloss=0.1144 | GA_fit=0.222 | ga_time=5.7s | theta=(γ=0.80, α=0.70, β=2.0, τ=1.8, k=3, sh=0.30, dn=0.02)\nClient 11 (ds4) | train_acc=0.9764 | val_acc=0.9669 | val_f1=0.9083 | val_auc=0.9886 | val_logloss=0.1778 | GA_fit=21.193 | ga_time=5.8s | theta=(γ=0.91, α=0.33, β=5.7, τ=3.3, k=3, sh=0.00, dn=0.14)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 6) | acc=0.9797 | f1=0.9070 | logloss=0.1275 | loss_ce=0.1266 | round_time=762.6s | thetas={'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.03)', 'ds2': '(γ=0.81, α=0.49, β=4.5, τ=2.7, k=7, sh=0.29, dn=0.10)', 'ds3': '(γ=1.17, α=0.37, β=7.0, τ=2.6, k=5, sh=0.00, dn=0.13)', 'ds4': '(γ=0.92, α=0.52, β=3.9, τ=2.2, k=3, sh=0.27, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9797 at round=6\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 7/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9960 | val_acc=0.9935 | val_f1=0.9751 | val_auc=1.0000 | val_logloss=0.0634 | GA_fit=13.334 | ga_time=5.7s | theta=(γ=1.18, α=0.46, β=8.9, τ=2.4, k=3, sh=0.00, dn=0.10)\nClient 1 (ds1) | train_acc=0.9814 | val_acc=0.9880 | val_f1=0.9873 | val_auc=0.9991 | val_logloss=0.1241 | GA_fit=10.759 | ga_time=5.8s | theta=(γ=1.18, α=0.08, β=6.7, τ=3.2, k=7, sh=0.12, dn=0.30)\nClient 2 (ds1) | train_acc=0.9965 | val_acc=1.0000 | val_f1=1.0000 | val_auc=1.0000 | val_logloss=0.0931 | GA_fit=0.201 | ga_time=5.7s | theta=(γ=0.95, α=0.49, β=2.0, τ=2.6, k=5, sh=0.26, dn=0.04)\nClient 3 (ds2) | train_acc=0.9872 | val_acc=1.0000 | val_f1=1.0000 | val_auc=nan | val_logloss=0.0930 | GA_fit=7.569 | ga_time=5.8s | theta=(γ=0.81, α=0.60, β=4.7, τ=3.1, k=3, sh=0.31, dn=0.02)\nClient 4 (ds2) | train_acc=0.9860 | val_acc=0.9832 | val_f1=0.9637 | val_auc=nan | val_logloss=0.1208 | GA_fit=8.399 | ga_time=5.8s | theta=(γ=0.96, α=0.52, β=4.2, τ=2.9, k=3, sh=0.35, dn=0.09)\nClient 5 (ds2) | train_acc=0.9832 | val_acc=0.9836 | val_f1=0.9890 | val_auc=nan | val_logloss=0.1404 | GA_fit=3.646 | ga_time=5.8s | theta=(γ=1.07, α=0.08, β=8.0, τ=2.8, k=3, sh=0.08, dn=0.06)\nClient 6 (ds3) | train_acc=0.9770 | val_acc=0.9813 | val_f1=0.9028 | val_auc=0.9998 | val_logloss=0.0851 | GA_fit=18.191 | ga_time=5.7s | theta=(γ=1.05, α=0.22, β=7.5, τ=2.6, k=3, sh=0.00, dn=0.16)\nClient 7 (ds3) | train_acc=0.9768 | val_acc=0.9833 | val_f1=0.9850 | val_auc=0.9997 | val_logloss=0.1094 | GA_fit=13.000 | ga_time=5.8s | theta=(γ=1.08, α=0.12, β=4.1, τ=2.4, k=7, sh=0.17, dn=0.18)\nClient 8 (ds3) | train_acc=0.9754 | val_acc=0.9871 | val_f1=0.9804 | val_auc=0.9941 | val_logloss=0.1271 | GA_fit=14.131 | ga_time=5.8s | theta=(γ=0.95, α=0.13, β=3.9, τ=3.4, k=3, sh=0.12, dn=0.14)\nClient 9 (ds4) | train_acc=0.9737 | val_acc=0.9695 | val_f1=0.8650 | val_auc=0.9991 | val_logloss=0.1302 | GA_fit=15.049 | ga_time=5.7s | theta=(γ=0.87, α=0.27, β=5.9, τ=2.1, k=3, sh=0.02, dn=0.00)\nClient 10 (ds4) | train_acc=0.9931 | val_acc=0.9912 | val_f1=0.6655 | val_auc=nan | val_logloss=0.1185 | GA_fit=0.222 | ga_time=5.7s | theta=(γ=0.83, α=0.69, β=2.0, τ=1.9, k=3, sh=0.35, dn=0.04)\nClient 11 (ds4) | train_acc=0.9731 | val_acc=0.9669 | val_f1=0.9052 | val_auc=0.9710 | val_logloss=0.1792 | GA_fit=0.229 | ga_time=5.7s | theta=(γ=1.00, α=0.67, β=2.0, τ=1.9, k=3, sh=0.31, dn=0.04)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 7) | acc=0.9834 | f1=0.9334 | logloss=0.1238 | loss_ce=0.1237 | round_time=761.7s | thetas={'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.03)', 'ds2': '(γ=0.68, α=0.44, β=4.5, τ=2.5, k=7, sh=0.27, dn=0.12)', 'ds3': '(γ=1.13, α=0.20, β=4.5, τ=3.3, k=3, sh=0.12, dn=0.15)', 'ds4': '(γ=0.90, α=0.48, β=3.9, τ=2.3, k=3, sh=0.26, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9834 at round=7\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 8/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9925 | val_acc=0.9935 | val_f1=0.9751 | val_auc=1.0000 | val_logloss=0.0678 | GA_fit=11.985 | ga_time=5.8s | theta=(γ=1.08, α=0.08, β=6.6, τ=3.0, k=5, sh=0.04, dn=0.13)\nClient 1 (ds1) | train_acc=0.9839 | val_acc=0.9850 | val_f1=0.9850 | val_auc=0.9983 | val_logloss=0.1339 | GA_fit=3.531 | ga_time=5.8s | theta=(γ=1.11, α=0.08, β=2.3, τ=3.3, k=5, sh=0.00, dn=0.14)\nClient 2 (ds1) | train_acc=0.9951 | val_acc=0.9898 | val_f1=0.8737 | val_auc=1.0000 | val_logloss=0.1034 | GA_fit=0.209 | ga_time=5.7s | theta=(γ=0.77, α=0.59, β=3.7, τ=1.8, k=3, sh=0.30, dn=0.02)\nClient 3 (ds2) | train_acc=0.9954 | val_acc=0.9867 | val_f1=0.6636 | val_auc=nan | val_logloss=0.1267 | GA_fit=8.732 | ga_time=5.8s | theta=(γ=0.67, α=0.60, β=4.1, τ=2.8, k=3, sh=0.25, dn=0.04)\nClient 4 (ds2) | train_acc=0.9869 | val_acc=0.9899 | val_f1=0.9843 | val_auc=nan | val_logloss=0.1115 | GA_fit=12.220 | ga_time=5.8s | theta=(γ=1.37, α=0.21, β=5.4, τ=2.4, k=3, sh=0.02, dn=0.08)\nClient 5 (ds2) | train_acc=0.9839 | val_acc=0.9877 | val_f1=0.7443 | val_auc=nan | val_logloss=0.1164 | GA_fit=2.648 | ga_time=5.8s | theta=(γ=0.92, α=0.40, β=9.0, τ=3.1, k=7, sh=0.14, dn=0.12)\nClient 6 (ds3) | train_acc=0.9878 | val_acc=0.9720 | val_f1=0.8791 | val_auc=1.0000 | val_logloss=0.0751 | GA_fit=0.185 | ga_time=5.7s | theta=(γ=1.04, α=0.37, β=6.4, τ=2.3, k=5, sh=0.18, dn=0.00)\nClient 7 (ds3) | train_acc=0.9844 | val_acc=0.9833 | val_f1=0.9850 | val_auc=0.9976 | val_logloss=0.1258 | GA_fit=13.096 | ga_time=5.8s | theta=(γ=1.10, α=0.22, β=4.4, τ=3.2, k=3, sh=0.04, dn=0.15)\nClient 8 (ds3) | train_acc=0.9786 | val_acc=0.9700 | val_f1=0.9586 | val_auc=0.9872 | val_logloss=0.1595 | GA_fit=16.261 | ga_time=5.8s | theta=(γ=1.20, α=0.27, β=7.1, τ=3.4, k=3, sh=0.00, dn=0.13)\nClient 9 (ds4) | train_acc=0.9805 | val_acc=0.9746 | val_f1=0.9453 | val_auc=0.9953 | val_logloss=0.1445 | GA_fit=11.253 | ga_time=5.8s | theta=(γ=1.21, α=0.30, β=6.0, τ=3.0, k=3, sh=0.12, dn=0.15)\nClient 10 (ds4) | train_acc=0.9937 | val_acc=0.9912 | val_f1=0.6652 | val_auc=nan | val_logloss=0.1177 | GA_fit=0.221 | ga_time=5.7s | theta=(γ=0.82, α=0.69, β=2.0, τ=1.5, k=5, sh=0.34, dn=0.00)\nClient 11 (ds4) | train_acc=0.9791 | val_acc=0.9632 | val_f1=0.8839 | val_auc=0.9704 | val_logloss=0.1840 | GA_fit=6.637 | ga_time=5.7s | theta=(γ=1.09, α=0.09, β=5.7, τ=3.3, k=3, sh=0.09, dn=0.14)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 8) | acc=0.9821 | f1=0.8841 | logloss=0.1272 | loss_ce=0.1261 | round_time=761.0s | thetas={'ds1': '(γ=1.19, α=0.23, β=6.3, τ=3.1, k=7, sh=0.03, dn=0.30)', 'ds2': '(γ=0.68, α=0.44, β=4.5, τ=2.5, k=7, sh=0.27, dn=0.12)', 'ds3': '(γ=1.18, α=0.53, β=5.2, τ=3.0, k=5, sh=0.23, dn=0.04)', 'ds4': '(γ=0.92, α=0.52, β=3.9, τ=2.2, k=3, sh=0.27, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9834 at round=7\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 9/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9982 | val_acc=0.9935 | val_f1=0.9965 | val_auc=1.0000 | val_logloss=0.0581 | GA_fit=11.367 | ga_time=5.8s | theta=(γ=1.32, α=0.20, β=6.3, τ=2.8, k=3, sh=0.00, dn=0.30)\nClient 1 (ds1) | train_acc=0.9847 | val_acc=0.9880 | val_f1=0.9908 | val_auc=1.0000 | val_logloss=0.1219 | GA_fit=11.966 | ga_time=5.8s | theta=(γ=1.39, α=0.08, β=6.8, τ=3.8, k=5, sh=0.17, dn=0.13)\nClient 2 (ds1) | train_acc=0.9917 | val_acc=1.0000 | val_f1=1.0000 | val_auc=1.0000 | val_logloss=0.0885 | GA_fit=0.189 | ga_time=5.7s | theta=(γ=1.02, α=0.62, β=2.0, τ=2.2, k=5, sh=0.02, dn=0.04)\nClient 3 (ds2) | train_acc=0.9982 | val_acc=0.9467 | val_f1=0.6289 | val_auc=nan | val_logloss=0.2014 | GA_fit=9.729 | ga_time=5.8s | theta=(γ=0.72, α=0.31, β=8.0, τ=2.6, k=5, sh=0.04, dn=0.24)\nClient 4 (ds2) | train_acc=0.9897 | val_acc=0.9832 | val_f1=0.9731 | val_auc=nan | val_logloss=0.1178 | GA_fit=10.853 | ga_time=5.8s | theta=(γ=0.90, α=0.29, β=7.0, τ=3.4, k=3, sh=0.06, dn=0.18)\nClient 5 (ds2) | train_acc=0.9868 | val_acc=0.9774 | val_f1=0.9849 | val_auc=nan | val_logloss=0.1546 | GA_fit=5.186 | ga_time=5.8s | theta=(γ=0.75, α=0.21, β=3.6, τ=2.6, k=7, sh=0.35, dn=0.15)\nClient 6 (ds3) | train_acc=0.9846 | val_acc=1.0000 | val_f1=1.0000 | val_auc=1.0000 | val_logloss=0.0420 | GA_fit=14.925 | ga_time=5.8s | theta=(γ=1.05, α=0.15, β=4.8, τ=3.3, k=3, sh=0.06, dn=0.14)\nClient 7 (ds3) | train_acc=0.9882 | val_acc=0.9889 | val_f1=0.9929 | val_auc=0.9998 | val_logloss=0.0960 | GA_fit=18.272 | ga_time=5.8s | theta=(γ=1.13, α=0.44, β=3.3, τ=3.1, k=3, sh=0.16, dn=0.12)\nClient 8 (ds3) | train_acc=0.9830 | val_acc=0.9785 | val_f1=0.9693 | val_auc=0.9961 | val_logloss=0.1256 | GA_fit=14.183 | ga_time=5.8s | theta=(γ=0.92, α=0.13, β=5.7, τ=3.7, k=7, sh=0.17, dn=0.02)\nClient 9 (ds4) | train_acc=0.9837 | val_acc=0.9746 | val_f1=0.9339 | val_auc=0.9981 | val_logloss=0.1407 | GA_fit=12.182 | ga_time=5.7s | theta=(γ=1.21, α=0.26, β=2.5, τ=2.8, k=5, sh=0.14, dn=0.05)\nClient 10 (ds4) | train_acc=0.9934 | val_acc=0.9868 | val_f1=0.4983 | val_auc=nan | val_logloss=0.1258 | GA_fit=0.216 | ga_time=5.7s | theta=(γ=1.01, α=0.58, β=2.0, τ=1.6, k=3, sh=0.35, dn=0.00)\nClient 11 (ds4) | train_acc=0.9801 | val_acc=0.9559 | val_f1=0.8551 | val_auc=0.9559 | val_logloss=0.1944 | GA_fit=14.626 | ga_time=5.8s | theta=(γ=1.23, α=0.12, β=5.4, τ=3.0, k=3, sh=0.19, dn=0.15)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 9) | acc=0.9807 | f1=0.9205 | logloss=0.1286 | loss_ce=0.1274 | round_time=761.2s | thetas={'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.03)', 'ds2': '(γ=0.68, α=0.44, β=4.5, τ=2.5, k=7, sh=0.27, dn=0.12)', 'ds3': '(γ=1.13, α=0.20, β=4.5, τ=3.3, k=3, sh=0.12, dn=0.15)', 'ds4': '(γ=0.92, α=0.52, β=3.9, τ=2.2, k=3, sh=0.27, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9834 at round=7\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 10/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9956 | val_acc=0.9935 | val_f1=0.9965 | val_auc=1.0000 | val_logloss=0.0801 | GA_fit=10.158 | ga_time=5.7s | theta=(γ=1.30, α=0.08, β=6.0, τ=3.2, k=5, sh=0.00, dn=0.29)\nClient 1 (ds1) | train_acc=0.9859 | val_acc=0.9910 | val_f1=0.9838 | val_auc=0.9999 | val_logloss=0.1031 | GA_fit=11.341 | ga_time=5.8s | theta=(γ=1.28, α=0.08, β=6.6, τ=3.6, k=3, sh=0.04, dn=0.04)\nClient 2 (ds1) | train_acc=0.9969 | val_acc=1.0000 | val_f1=1.0000 | val_auc=1.0000 | val_logloss=0.0854 | GA_fit=0.194 | ga_time=5.7s | theta=(γ=1.16, α=0.70, β=2.0, τ=2.6, k=5, sh=0.13, dn=0.02)\nClient 3 (ds2) | train_acc=0.9954 | val_acc=1.0000 | val_f1=1.0000 | val_auc=nan | val_logloss=0.0825 | GA_fit=11.199 | ga_time=5.8s | theta=(γ=0.89, α=0.41, β=7.7, τ=2.3, k=5, sh=0.00, dn=0.19)\nClient 4 (ds2) | train_acc=0.9908 | val_acc=0.9865 | val_f1=0.9788 | val_auc=nan | val_logloss=0.1130 | GA_fit=0.234 | ga_time=5.7s | theta=(γ=0.60, α=0.69, β=2.9, τ=2.1, k=5, sh=0.35, dn=0.04)\nClient 5 (ds2) | train_acc=0.9893 | val_acc=0.9897 | val_f1=0.9931 | val_auc=nan | val_logloss=0.1098 | GA_fit=3.600 | ga_time=5.8s | theta=(γ=0.99, α=0.14, β=2.4, τ=3.3, k=3, sh=0.04, dn=0.00)\nClient 6 (ds3) | train_acc=0.9910 | val_acc=0.9907 | val_f1=0.9762 | val_auc=1.0000 | val_logloss=0.0663 | GA_fit=20.891 | ga_time=5.8s | theta=(γ=1.25, α=0.34, β=7.4, τ=2.7, k=7, sh=0.00, dn=0.09)\nClient 7 (ds3) | train_acc=0.9806 | val_acc=0.9667 | val_f1=0.9618 | val_auc=0.9988 | val_logloss=0.1457 | GA_fit=14.600 | ga_time=5.8s | theta=(γ=1.14, α=0.13, β=6.5, τ=3.0, k=3, sh=0.07, dn=0.18)\nClient 8 (ds3) | train_acc=0.9807 | val_acc=0.9785 | val_f1=0.9734 | val_auc=0.9941 | val_logloss=0.1464 | GA_fit=14.760 | ga_time=5.8s | theta=(γ=1.12, α=0.28, β=6.1, τ=2.5, k=5, sh=0.00, dn=0.19)\nClient 9 (ds4) | train_acc=0.9831 | val_acc=0.9746 | val_f1=0.8941 | val_auc=0.9992 | val_logloss=0.1178 | GA_fit=6.984 | ga_time=5.8s | theta=(γ=1.29, α=0.57, β=2.0, τ=2.0, k=3, sh=0.17, dn=0.00)\nClient 10 (ds4) | train_acc=0.9937 | val_acc=0.9912 | val_f1=0.6652 | val_auc=nan | val_logloss=0.1178 | GA_fit=0.209 | ga_time=5.7s | theta=(γ=0.90, α=0.62, β=2.3, τ=2.3, k=3, sh=0.35, dn=0.00)\nClient 11 (ds4) | train_acc=0.9842 | val_acc=0.9596 | val_f1=0.8770 | val_auc=0.9792 | val_logloss=0.1887 | GA_fit=12.068 | ga_time=5.8s | theta=(γ=0.89, α=0.28, β=3.5, τ=2.4, k=3, sh=0.23, dn=0.12)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 10) | acc=0.9838 | f1=0.9383 | logloss=0.1179 | loss_ce=0.1171 | round_time=760.5s | thetas={'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, dn=0.03)', 'ds2': '(γ=0.81, α=0.49, β=4.5, τ=2.7, k=7, sh=0.29, dn=0.10)', 'ds3': '(γ=1.18, α=0.53, β=5.2, τ=3.0, k=5, sh=0.23, dn=0.04)', 'ds4': '(γ=0.92, α=0.52, β=3.9, τ=2.2, k=3, sh=0.27, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9838 at round=10\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 11/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9960 | val_acc=0.9935 | val_f1=0.9965 | val_auc=1.0000 | val_logloss=0.0742 | GA_fit=15.627 | ga_time=5.8s | theta=(γ=0.96, α=0.20, β=5.8, τ=3.2, k=3, sh=0.00, dn=0.16)\nClient 1 (ds1) | train_acc=0.9916 | val_acc=0.9910 | val_f1=0.9896 | val_auc=0.9999 | val_logloss=0.1134 | GA_fit=19.685 | ga_time=5.8s | theta=(γ=1.32, α=0.09, β=4.0, τ=3.4, k=3, sh=0.21, dn=0.03)\nClient 2 (ds1) | train_acc=0.9986 | val_acc=0.9898 | val_f1=0.8737 | val_auc=1.0000 | val_logloss=0.1223 | GA_fit=0.202 | ga_time=5.7s | theta=(γ=0.69, α=0.52, β=2.3, τ=1.8, k=3, sh=0.19, dn=0.00)\nClient 3 (ds2) | train_acc=0.9964 | val_acc=0.9733 | val_f1=0.6524 | val_auc=nan | val_logloss=0.1634 | GA_fit=15.352 | ga_time=5.8s | theta=(γ=0.68, α=0.46, β=2.8, τ=2.6, k=3, sh=0.35, dn=0.11)\nClient 4 (ds2) | train_acc=0.9927 | val_acc=0.9899 | val_f1=0.9839 | val_auc=nan | val_logloss=0.1128 | GA_fit=9.989 | ga_time=5.8s | theta=(γ=0.88, α=0.24, β=10.0, τ=2.1, k=3, sh=0.15, dn=0.06)\nClient 5 (ds2) | train_acc=0.9875 | val_acc=0.9877 | val_f1=0.9918 | val_auc=nan | val_logloss=0.1229 | GA_fit=3.104 | ga_time=5.8s | theta=(γ=0.98, α=0.54, β=5.4, τ=2.8, k=3, sh=0.35, dn=0.14)\nClient 6 (ds3) | train_acc=0.9917 | val_acc=1.0000 | val_f1=1.0000 | val_auc=1.0000 | val_logloss=0.0425 | GA_fit=18.589 | ga_time=5.7s | theta=(γ=1.10, α=0.08, β=6.6, τ=2.9, k=5, sh=0.00, dn=0.14)\nClient 7 (ds3) | train_acc=0.9909 | val_acc=0.9889 | val_f1=0.9797 | val_auc=0.9999 | val_logloss=0.1044 | GA_fit=13.051 | ga_time=5.8s | theta=(γ=0.98, α=0.30, β=7.4, τ=3.4, k=7, sh=0.00, dn=0.23)\nClient 8 (ds3) | train_acc=0.9865 | val_acc=0.9828 | val_f1=0.9753 | val_auc=0.9995 | val_logloss=0.1187 | GA_fit=23.348 | ga_time=5.8s | theta=(γ=1.28, α=0.08, β=4.9, τ=3.6, k=5, sh=0.16, dn=0.20)\nClient 9 (ds4) | train_acc=0.9807 | val_acc=0.9898 | val_f1=0.9916 | val_auc=0.9975 | val_logloss=0.1167 | GA_fit=6.164 | ga_time=5.8s | theta=(γ=0.94, α=0.17, β=2.0, τ=3.3, k=7, sh=0.01, dn=0.09)\nClient 10 (ds4) | train_acc=0.9946 | val_acc=0.9912 | val_f1=0.6655 | val_auc=nan | val_logloss=0.1083 | GA_fit=0.217 | ga_time=5.7s | theta=(γ=0.95, α=0.68, β=2.0, τ=1.6, k=5, sh=0.35, dn=0.01)\nClient 11 (ds4) | train_acc=0.9894 | val_acc=0.9743 | val_f1=0.9092 | val_auc=0.9645 | val_logloss=0.1630 | GA_fit=7.761 | ga_time=5.8s | theta=(γ=1.33, α=0.31, β=3.4, τ=2.4, k=3, sh=0.02, dn=0.20)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 11) | acc=0.9878 | f1=0.9400 | logloss=0.1166 | loss_ce=0.1160 | round_time=761.5s | thetas={'ds1': '(γ=1.19, α=0.23, β=6.3, τ=3.1, k=7, sh=0.03, dn=0.30)', 'ds2': '(γ=0.68, α=0.44, β=4.5, τ=2.5, k=7, sh=0.27, dn=0.12)', 'ds3': '(γ=1.17, α=0.37, β=7.0, τ=2.6, k=5, sh=0.00, dn=0.13)', 'ds4': '(γ=1.09, α=0.70, β=2.0, τ=2.5, k=3, sh=0.32, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9878 at round=11\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nROUND 12/12\n============================================================================================\nClient 0 (ds1) | train_acc=0.9969 | val_acc=0.9935 | val_f1=0.9965 | val_auc=1.0000 | val_logloss=0.0753 | GA_fit=15.272 | ga_time=5.8s | theta=(γ=1.18, α=0.08, β=6.6, τ=2.7, k=5, sh=0.05, dn=0.08)\nClient 1 (ds1) | train_acc=0.9902 | val_acc=0.9970 | val_f1=0.9942 | val_auc=0.9983 | val_logloss=0.1040 | GA_fit=10.410 | ga_time=5.8s | theta=(γ=0.94, α=0.08, β=7.5, τ=2.2, k=3, sh=0.03, dn=0.06)\nClient 2 (ds1) | train_acc=0.9955 | val_acc=0.9898 | val_f1=0.8737 | val_auc=0.9983 | val_logloss=0.1148 | GA_fit=0.202 | ga_time=5.7s | theta=(γ=1.29, α=0.70, β=2.1, τ=2.1, k=7, sh=0.29, dn=0.15)\nClient 3 (ds2) | train_acc=0.9954 | val_acc=0.9733 | val_f1=0.8167 | val_auc=nan | val_logloss=0.1153 | GA_fit=17.132 | ga_time=5.8s | theta=(γ=1.00, α=0.13, β=7.3, τ=2.8, k=3, sh=0.02, dn=0.12)\nClient 4 (ds2) | train_acc=0.9924 | val_acc=0.9899 | val_f1=0.9835 | val_auc=nan | val_logloss=0.1120 | GA_fit=15.319 | ga_time=5.8s | theta=(γ=0.75, α=0.08, β=4.7, τ=1.8, k=7, sh=0.05, dn=0.20)\nClient 5 (ds2) | train_acc=0.9902 | val_acc=0.9877 | val_f1=0.9918 | val_auc=nan | val_logloss=0.1222 | GA_fit=4.766 | ga_time=5.8s | theta=(γ=0.75, α=0.19, β=5.6, τ=3.2, k=3, sh=0.00, dn=0.21)\nClient 6 (ds3) | train_acc=0.9885 | val_acc=1.0000 | val_f1=1.0000 | val_auc=1.0000 | val_logloss=0.0360 | GA_fit=11.480 | ga_time=5.7s | theta=(γ=1.23, α=0.08, β=4.9, τ=2.9, k=3, sh=0.07, dn=0.12)\nClient 7 (ds3) | train_acc=0.9863 | val_acc=0.9722 | val_f1=0.9632 | val_auc=0.9995 | val_logloss=0.1376 | GA_fit=7.634 | ga_time=5.7s | theta=(γ=1.06, α=0.17, β=7.6, τ=2.6, k=5, sh=0.00, dn=0.13)\nClient 8 (ds3) | train_acc=0.9865 | val_acc=0.9914 | val_f1=0.9859 | val_auc=0.9999 | val_logloss=0.1028 | GA_fit=10.508 | ga_time=5.8s | theta=(γ=0.86, α=0.08, β=6.2, τ=2.8, k=5, sh=0.03, dn=0.08)\nClient 9 (ds4) | train_acc=0.9856 | val_acc=0.9949 | val_f1=0.9594 | val_auc=0.9999 | val_logloss=0.0822 | GA_fit=6.602 | ga_time=5.7s | theta=(γ=1.35, α=0.54, β=7.8, τ=2.1, k=3, sh=0.00, dn=0.10)\nClient 10 (ds4) | train_acc=0.9937 | val_acc=0.9912 | val_f1=0.6655 | val_auc=nan | val_logloss=0.1253 | GA_fit=0.217 | ga_time=5.7s | theta=(γ=0.84, α=0.70, β=2.0, τ=1.9, k=5, sh=0.35, dn=0.00)\nClient 11 (ds4) | train_acc=0.9879 | val_acc=0.9706 | val_f1=0.9090 | val_auc=0.9724 | val_logloss=0.1524 | GA_fit=14.243 | ga_time=5.8s | theta=(γ=1.18, α=0.11, β=3.5, τ=3.3, k=7, sh=0.00, dn=0.17)\n\n--------------------------------------------------------------------------------------------\nGLOBAL VAL (Round 12) | acc=0.9885 | f1=0.9402 | logloss=0.1100 | loss_ce=0.1095 | round_time=760.8s | thetas={'ds1': '(γ=1.36, α=0.08, β=6.8, τ=3.1, k=5, sh=0.05, dn=0.20)', 'ds2': '(γ=0.68, α=0.44, β=4.5, τ=2.5, k=7, sh=0.27, dn=0.12)', 'ds3': '(γ=1.18, α=0.53, β=5.2, τ=3.0, k=5, sh=0.23, dn=0.04)', 'ds4': '(γ=0.97, α=0.49, β=2.0, τ=2.1, k=3, sh=0.34, dn=0.01)'}\nBEST SO FAR (by ACC) | best_val_acc=0.9885 at round=12\n--------------------------------------------------------------------------------------------\n\n============================================================================================\nTRAINING COMPLETE ✅ | total_time=9168.3s | best_val_acc=0.9885 | best_round=12\n============================================================================================\n\n--------------------------------------------------------------------------------------------\nGLOBAL per-round metrics\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    round  round_time_s                                      global_thetas  \\\n0       1    806.945952  {'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...   \n1       2    723.766821  {'ds1': '(γ=1.32, α=0.20, β=6.0, τ=3.1, k=5, s...   \n2       3    781.099070  {'ds1': '(γ=1.25, α=0.57, β=2.3, τ=2.2, k=5, s...   \n3       4    763.970252  {'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...   \n4       5    762.685288  {'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...   \n5       6    762.572541  {'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...   \n6       7    761.740906  {'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...   \n7       8    760.979891  {'ds1': '(γ=1.19, α=0.23, β=6.3, τ=3.1, k=7, s...   \n8       9    761.152471  {'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...   \n9      10    760.474721  {'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...   \n10     11    761.528522  {'ds1': '(γ=1.19, α=0.23, β=6.3, τ=3.1, k=7, s...   \n11     12    760.763054  {'ds1': '(γ=1.36, α=0.08, β=6.8, τ=3.1, k=5, s...   \n\n    global_acc  global_f1_macro  global_precision_macro  global_recall_macro  \\\n0     0.927654         0.773983                0.765457             0.803862   \n1     0.960784         0.851495                0.843712             0.879925   \n2     0.971602         0.868322                0.862058             0.878170   \n3     0.973293         0.898991                0.892093             0.915847   \n4     0.977011         0.914691                0.905577             0.933317   \n5     0.979716         0.907016                0.893641             0.932473   \n6     0.983435         0.933408                0.923432             0.949613   \n7     0.982082         0.884094                0.876849             0.900796   \n8     0.980730         0.920547                0.920137             0.923269   \n9     0.983773         0.938251                0.933468             0.949480   \n10    0.987830         0.940005                0.936350             0.953528   \n11    0.988506         0.940197                0.931508             0.962229   \n\n    global_log_loss  global_loss_ce  global_eval_time_s  \n0          0.259516        0.257144            5.753748  \n1          0.180107        0.178522            3.551848  \n2          0.156187        0.154740            3.549849  \n3          0.148511        0.146950            3.559128  \n4          0.142490        0.141086            3.557618  \n5          0.127490        0.126646            3.559277  \n6          0.123804        0.123712            3.563437  \n7          0.127243        0.126081            3.547035  \n8          0.128611        0.127449            3.556252  \n9          0.117907        0.117080            3.555869  \n10         0.116614        0.116021            3.558563  \n11         0.109977        0.109502            3.565159  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>round</th>\n      <th>round_time_s</th>\n      <th>global_thetas</th>\n      <th>global_acc</th>\n      <th>global_f1_macro</th>\n      <th>global_precision_macro</th>\n      <th>global_recall_macro</th>\n      <th>global_log_loss</th>\n      <th>global_loss_ce</th>\n      <th>global_eval_time_s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>806.945952</td>\n      <td>{'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...</td>\n      <td>0.927654</td>\n      <td>0.773983</td>\n      <td>0.765457</td>\n      <td>0.803862</td>\n      <td>0.259516</td>\n      <td>0.257144</td>\n      <td>5.753748</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>723.766821</td>\n      <td>{'ds1': '(γ=1.32, α=0.20, β=6.0, τ=3.1, k=5, s...</td>\n      <td>0.960784</td>\n      <td>0.851495</td>\n      <td>0.843712</td>\n      <td>0.879925</td>\n      <td>0.180107</td>\n      <td>0.178522</td>\n      <td>3.551848</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>781.099070</td>\n      <td>{'ds1': '(γ=1.25, α=0.57, β=2.3, τ=2.2, k=5, s...</td>\n      <td>0.971602</td>\n      <td>0.868322</td>\n      <td>0.862058</td>\n      <td>0.878170</td>\n      <td>0.156187</td>\n      <td>0.154740</td>\n      <td>3.549849</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>763.970252</td>\n      <td>{'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...</td>\n      <td>0.973293</td>\n      <td>0.898991</td>\n      <td>0.892093</td>\n      <td>0.915847</td>\n      <td>0.148511</td>\n      <td>0.146950</td>\n      <td>3.559128</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>762.685288</td>\n      <td>{'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...</td>\n      <td>0.977011</td>\n      <td>0.914691</td>\n      <td>0.905577</td>\n      <td>0.933317</td>\n      <td>0.142490</td>\n      <td>0.141086</td>\n      <td>3.557618</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>762.572541</td>\n      <td>{'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...</td>\n      <td>0.979716</td>\n      <td>0.907016</td>\n      <td>0.893641</td>\n      <td>0.932473</td>\n      <td>0.127490</td>\n      <td>0.126646</td>\n      <td>3.559277</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>761.740906</td>\n      <td>{'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...</td>\n      <td>0.983435</td>\n      <td>0.933408</td>\n      <td>0.923432</td>\n      <td>0.949613</td>\n      <td>0.123804</td>\n      <td>0.123712</td>\n      <td>3.563437</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>760.979891</td>\n      <td>{'ds1': '(γ=1.19, α=0.23, β=6.3, τ=3.1, k=7, s...</td>\n      <td>0.982082</td>\n      <td>0.884094</td>\n      <td>0.876849</td>\n      <td>0.900796</td>\n      <td>0.127243</td>\n      <td>0.126081</td>\n      <td>3.547035</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>761.152471</td>\n      <td>{'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...</td>\n      <td>0.980730</td>\n      <td>0.920547</td>\n      <td>0.920137</td>\n      <td>0.923269</td>\n      <td>0.128611</td>\n      <td>0.127449</td>\n      <td>3.556252</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>760.474721</td>\n      <td>{'ds1': '(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, s...</td>\n      <td>0.983773</td>\n      <td>0.938251</td>\n      <td>0.933468</td>\n      <td>0.949480</td>\n      <td>0.117907</td>\n      <td>0.117080</td>\n      <td>3.555869</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>761.528522</td>\n      <td>{'ds1': '(γ=1.19, α=0.23, β=6.3, τ=3.1, k=7, s...</td>\n      <td>0.987830</td>\n      <td>0.940005</td>\n      <td>0.936350</td>\n      <td>0.953528</td>\n      <td>0.116614</td>\n      <td>0.116021</td>\n      <td>3.558563</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>760.763054</td>\n      <td>{'ds1': '(γ=1.36, α=0.08, β=6.8, τ=3.1, k=5, s...</td>\n      <td>0.988506</td>\n      <td>0.940197</td>\n      <td>0.931508</td>\n      <td>0.962229</td>\n      <td>0.109977</td>\n      <td>0.109502</td>\n      <td>3.565159</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nLOCAL per-client per-round metrics (head)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    round     client dataset  ga_best_fit_score  ga_time_s  \\\n0       1   client_0     ds1           9.638477  12.094798   \n1       1   client_1     ds1          10.714323   5.845026   \n2       1   client_2     ds1           0.198072   5.948863   \n3       1   client_3     ds2           6.867913   6.014931   \n4       1   client_4     ds2           0.209249   5.965567   \n5       1   client_5     ds2           4.163899   5.987546   \n6       1   client_6     ds3          13.143195   5.949632   \n7       1   client_7     ds3           9.825009   5.999177   \n8       1   client_8     ds3          12.263569   6.002362   \n9       1   client_9     ds4           6.640467   5.926919   \n10      1  client_10     ds4           0.209736   5.914981   \n11      1  client_11     ds4           0.232460   5.896827   \n12      2   client_0     ds1           8.835107   5.825562   \n13      2   client_1     ds1           4.496946   5.842251   \n14      2   client_2     ds1           0.199398   5.783090   \n15      2   client_3     ds2          10.625721   5.784555   \n16      2   client_4     ds2           0.220300   5.853811   \n17      2   client_5     ds2           2.942063   5.846388   \n18      2   client_6     ds3          16.172888   5.757773   \n19      2   client_7     ds3          11.054463   5.793454   \n20      2   client_8     ds3          12.811290   5.792402   \n21      2   client_9     ds4           8.523519   5.804382   \n22      2  client_10     ds4           0.218942   5.732693   \n23      2  client_11     ds4           0.233247   5.746492   \n24      3   client_0     ds1           9.897944   5.738739   \n25      3   client_1     ds1           4.122640   5.846867   \n26      3   client_2     ds1           0.207307   5.779426   \n27      3   client_3     ds2           4.694156   5.788155   \n28      3   client_4     ds2           5.447990   5.804596   \n29      3   client_5     ds2           2.581774   5.871058   \n\n                                            theta_str  gamma_power  \\\n0   (γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, d...     1.244318   \n1   (γ=1.32, α=0.20, β=6.0, τ=3.1, k=5, sh=0.11, d...     1.320571   \n2   (γ=0.78, α=0.46, β=3.5, τ=2.8, k=7, sh=0.28, d...     0.777290   \n3   (γ=0.64, α=0.52, β=5.4, τ=2.7, k=3, sh=0.29, d...     0.641079   \n4   (γ=0.67, α=0.56, β=5.9, τ=2.5, k=3, sh=0.33, d...     0.667650   \n5   (γ=0.61, α=0.52, β=5.8, τ=2.8, k=7, sh=0.28, d...     0.614877   \n6   (γ=1.13, α=0.20, β=4.5, τ=3.3, k=3, sh=0.12, d...     1.133613   \n7   (γ=1.16, α=0.36, β=4.5, τ=2.4, k=3, sh=0.12, d...     1.163480   \n8   (γ=1.18, α=0.53, β=5.2, τ=3.0, k=5, sh=0.23, d...     1.178856   \n9   (γ=0.92, α=0.52, β=3.9, τ=2.2, k=3, sh=0.27, d...     0.921893   \n10  (γ=0.95, α=0.55, β=2.5, τ=2.0, k=3, sh=0.33, d...     0.947269   \n11  (γ=0.90, α=0.62, β=2.0, τ=2.2, k=3, sh=0.35, d...     0.898665   \n12  (γ=1.50, α=0.47, β=3.0, τ=3.3, k=5, sh=0.25, d...     1.500000   \n13  (γ=1.23, α=0.08, β=9.1, τ=2.4, k=5, sh=0.01, d...     1.231308   \n14  (γ=0.82, α=0.37, β=2.0, τ=1.7, k=5, sh=0.27, d...     0.816557   \n15  (γ=0.81, α=0.49, β=4.4, τ=2.7, k=7, sh=0.35, d...     0.813214   \n16  (γ=0.64, α=0.66, β=3.2, τ=2.4, k=3, sh=0.33, d...     0.643158   \n17  (γ=1.02, α=0.15, β=5.9, τ=3.4, k=3, sh=0.06, d...     1.015920   \n18  (γ=1.11, α=0.08, β=4.6, τ=3.7, k=3, sh=0.28, d...     1.111488   \n19  (γ=0.96, α=0.22, β=5.0, τ=3.8, k=3, sh=0.09, d...     0.956287   \n20  (γ=1.14, α=0.17, β=7.2, τ=3.3, k=3, sh=0.06, d...     1.140023   \n21  (γ=1.09, α=0.70, β=2.0, τ=2.5, k=3, sh=0.32, d...     1.088305   \n22  (γ=0.90, α=0.67, β=2.0, τ=2.0, k=3, sh=0.35, d...     0.899258   \n23  (γ=0.94, α=0.70, β=2.0, τ=2.1, k=3, sh=0.32, d...     0.941782   \n24  (γ=1.24, α=0.56, β=2.0, τ=2.2, k=3, sh=0.25, d...     1.244488   \n25  (γ=1.39, α=0.10, β=6.4, τ=3.5, k=3, sh=0.00, d...     1.390871   \n26  (γ=0.76, α=0.55, β=2.4, τ=2.5, k=5, sh=0.26, d...     0.756566   \n27  (γ=0.70, α=0.66, β=4.4, τ=3.2, k=5, sh=0.30, d...     0.702756   \n28  (γ=0.60, α=0.52, β=5.7, τ=3.2, k=3, sh=0.27, d...     0.600000   \n29  (γ=0.61, α=0.45, β=4.4, τ=2.0, k=7, sh=0.16, d...     0.608097   \n\n    alpha_contrast_weight  beta_contrast_sharpness  tau_clip  ...  \\\n0                0.473141                 3.055522  2.742403  ...   \n1                0.203079                 6.022620  3.131826  ...   \n2                0.457708                 3.509380  2.842538  ...   \n3                0.524832                 5.440878  2.665325  ...   \n4                0.562850                 5.855335  2.518214  ...   \n5                0.522026                 5.792319  2.784551  ...   \n6                0.199126                 4.511599  3.323043  ...   \n7                0.361975                 4.511599  2.410383  ...   \n8                0.530371                 5.195688  3.038469  ...   \n9                0.519938                 3.864403  2.184408  ...   \n10               0.551120                 2.481549  1.990395  ...   \n11               0.624441                 2.000000  2.176174  ...   \n12               0.470438                 3.008206  3.260905  ...   \n13               0.080000                 9.061774  2.389782  ...   \n14               0.372134                 2.000000  1.676045  ...   \n15               0.486914                 4.445728  2.714397  ...   \n16               0.659146                 3.212204  2.388446  ...   \n17               0.154840                 5.913049  3.446064  ...   \n18               0.081618                 4.638423  3.678511  ...   \n19               0.216165                 4.960932  3.800000  ...   \n20               0.169270                 7.157392  3.346643  ...   \n21               0.700000                 2.000000  2.473522  ...   \n22               0.667218                 2.000000  1.989042  ...   \n23               0.700000                 2.000000  2.109315  ...   \n24               0.556118                 2.000000  2.189196  ...   \n25               0.099405                 6.425513  3.468924  ...   \n26               0.547553                 2.440866  2.451606  ...   \n27               0.660924                 4.368978  3.162710  ...   \n28               0.516396                 5.703050  3.233999  ...   \n29               0.448936                 4.424067  2.012359  ...   \n\n    val_g2_mean  val_g2_entropy_mean  val_g2_mean_c0  val_g2_entropy_c0  \\\n0      0.499455             0.943852        0.499455           0.943852   \n1      0.497293             0.948486        0.497293           0.948486   \n2      0.492240             0.945733        0.492240           0.945732   \n3      0.495601             0.945834        0.495601           0.945834   \n4      0.488902             0.938683        0.488902           0.938683   \n5      0.489800             0.942876        0.489800           0.942876   \n6      0.485627             0.943933        0.485627           0.943933   \n7      0.490888             0.949444        0.490888           0.949444   \n8      0.486209             0.943719        0.486209           0.943719   \n9      0.476817             0.936757        0.476817           0.936757   \n10     0.491254             0.944593        0.491254           0.944593   \n11     0.477735             0.941027        0.477735           0.941027   \n12     0.494623             0.943845        0.494623           0.943845   \n13     0.487925             0.948592        0.487925           0.948592   \n14     0.487805             0.943652        0.487805           0.943652   \n15     0.489180             0.946478        0.489180           0.946478   \n16     0.485590             0.942289        0.485590           0.942289   \n17     0.497811             0.945306        0.497811           0.945306   \n18     0.490537             0.945146        0.490537           0.945146   \n19     0.492034             0.950966        0.492034           0.950966   \n20     0.488875             0.947206        0.488875           0.947206   \n21     0.478561             0.938044        0.478561           0.938044   \n22     0.488213             0.943466        0.488213           0.943466   \n23     0.476577             0.940738        0.476577           0.940738   \n24     0.495392             0.944818        0.495392           0.944817   \n25     0.490546             0.949060        0.490546           0.949060   \n26     0.489020             0.944704        0.489020           0.944704   \n27     0.486550             0.946270        0.486550           0.946270   \n28     0.490073             0.944602        0.490073           0.944602   \n29     0.489620             0.940646        0.489620           0.940646   \n\n    val_g2_mean_c1  val_g2_entropy_c1  val_g2_mean_c2  val_g2_entropy_c2  \\\n0         0.499455           0.943852        0.499455           0.943852   \n1         0.497293           0.948486        0.497293           0.948487   \n2         0.492240           0.945733        0.492240           0.945732   \n3         0.495601           0.945834             NaN                NaN   \n4         0.488902           0.938683             NaN                NaN   \n5         0.489800           0.942876             NaN                NaN   \n6         0.485627           0.943933        0.485627           0.943933   \n7         0.490888           0.949444        0.490888           0.949444   \n8         0.486209           0.943719        0.486209           0.943719   \n9         0.476817           0.936757        0.476817           0.936757   \n10        0.491254           0.944593             NaN                NaN   \n11        0.477735           0.941027        0.477735           0.941027   \n12        0.494623           0.943845        0.494623           0.943845   \n13        0.487925           0.948592        0.487925           0.948592   \n14        0.487805           0.943652        0.487805           0.943652   \n15        0.489180           0.946478             NaN                NaN   \n16        0.485590           0.942289             NaN                NaN   \n17        0.497811           0.945306             NaN                NaN   \n18        0.490537           0.945146        0.490537           0.945146   \n19        0.492034           0.950966        0.492034           0.950966   \n20        0.488875           0.947206        0.488875           0.947206   \n21        0.478561           0.938044        0.478561           0.938044   \n22        0.488213           0.943466             NaN                NaN   \n23        0.476577           0.940738        0.476577           0.940738   \n24        0.495392           0.944817        0.495392           0.944817   \n25        0.490546           0.949060        0.490546           0.949060   \n26        0.489020           0.944704        0.489020           0.944704   \n27        0.486550           0.946270             NaN                NaN   \n28        0.490073           0.944602             NaN                NaN   \n29        0.489620           0.940646             NaN                NaN   \n\n    val_g2_mean_c3  val_g2_entropy_c3  \n0         0.499455           0.943852  \n1         0.497293           0.948486  \n2         0.492240           0.945732  \n3         0.495601           0.945834  \n4         0.488902           0.938683  \n5         0.489800           0.942876  \n6         0.485627           0.943933  \n7         0.490888           0.949444  \n8         0.486209           0.943719  \n9         0.476817           0.936757  \n10        0.491254           0.944593  \n11        0.477735           0.941027  \n12        0.494623           0.943845  \n13        0.487925           0.948592  \n14        0.487805           0.943652  \n15        0.489180           0.946478  \n16        0.485590           0.942289  \n17        0.497811           0.945306  \n18        0.490537           0.945146  \n19        0.492034           0.950966  \n20        0.488875           0.947206  \n21        0.478561           0.938044  \n22        0.488213           0.943466  \n23        0.476577           0.940738  \n24        0.495392           0.944817  \n25        0.490546           0.949060  \n26        0.489020           0.944704  \n27        0.486550           0.946270  \n28        0.490073           0.944602  \n29        0.489620           0.940646  \n\n[30 rows x 61 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>round</th>\n      <th>client</th>\n      <th>dataset</th>\n      <th>ga_best_fit_score</th>\n      <th>ga_time_s</th>\n      <th>theta_str</th>\n      <th>gamma_power</th>\n      <th>alpha_contrast_weight</th>\n      <th>beta_contrast_sharpness</th>\n      <th>tau_clip</th>\n      <th>...</th>\n      <th>val_g2_mean</th>\n      <th>val_g2_entropy_mean</th>\n      <th>val_g2_mean_c0</th>\n      <th>val_g2_entropy_c0</th>\n      <th>val_g2_mean_c1</th>\n      <th>val_g2_entropy_c1</th>\n      <th>val_g2_mean_c2</th>\n      <th>val_g2_entropy_c2</th>\n      <th>val_g2_mean_c3</th>\n      <th>val_g2_entropy_c3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>client_0</td>\n      <td>ds1</td>\n      <td>9.638477</td>\n      <td>12.094798</td>\n      <td>(γ=1.24, α=0.47, β=3.1, τ=2.7, k=7, sh=0.00, d...</td>\n      <td>1.244318</td>\n      <td>0.473141</td>\n      <td>3.055522</td>\n      <td>2.742403</td>\n      <td>...</td>\n      <td>0.499455</td>\n      <td>0.943852</td>\n      <td>0.499455</td>\n      <td>0.943852</td>\n      <td>0.499455</td>\n      <td>0.943852</td>\n      <td>0.499455</td>\n      <td>0.943852</td>\n      <td>0.499455</td>\n      <td>0.943852</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>client_1</td>\n      <td>ds1</td>\n      <td>10.714323</td>\n      <td>5.845026</td>\n      <td>(γ=1.32, α=0.20, β=6.0, τ=3.1, k=5, sh=0.11, d...</td>\n      <td>1.320571</td>\n      <td>0.203079</td>\n      <td>6.022620</td>\n      <td>3.131826</td>\n      <td>...</td>\n      <td>0.497293</td>\n      <td>0.948486</td>\n      <td>0.497293</td>\n      <td>0.948486</td>\n      <td>0.497293</td>\n      <td>0.948486</td>\n      <td>0.497293</td>\n      <td>0.948487</td>\n      <td>0.497293</td>\n      <td>0.948486</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>client_2</td>\n      <td>ds1</td>\n      <td>0.198072</td>\n      <td>5.948863</td>\n      <td>(γ=0.78, α=0.46, β=3.5, τ=2.8, k=7, sh=0.28, d...</td>\n      <td>0.777290</td>\n      <td>0.457708</td>\n      <td>3.509380</td>\n      <td>2.842538</td>\n      <td>...</td>\n      <td>0.492240</td>\n      <td>0.945733</td>\n      <td>0.492240</td>\n      <td>0.945732</td>\n      <td>0.492240</td>\n      <td>0.945733</td>\n      <td>0.492240</td>\n      <td>0.945732</td>\n      <td>0.492240</td>\n      <td>0.945732</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>client_3</td>\n      <td>ds2</td>\n      <td>6.867913</td>\n      <td>6.014931</td>\n      <td>(γ=0.64, α=0.52, β=5.4, τ=2.7, k=3, sh=0.29, d...</td>\n      <td>0.641079</td>\n      <td>0.524832</td>\n      <td>5.440878</td>\n      <td>2.665325</td>\n      <td>...</td>\n      <td>0.495601</td>\n      <td>0.945834</td>\n      <td>0.495601</td>\n      <td>0.945834</td>\n      <td>0.495601</td>\n      <td>0.945834</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.495601</td>\n      <td>0.945834</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>client_4</td>\n      <td>ds2</td>\n      <td>0.209249</td>\n      <td>5.965567</td>\n      <td>(γ=0.67, α=0.56, β=5.9, τ=2.5, k=3, sh=0.33, d...</td>\n      <td>0.667650</td>\n      <td>0.562850</td>\n      <td>5.855335</td>\n      <td>2.518214</td>\n      <td>...</td>\n      <td>0.488902</td>\n      <td>0.938683</td>\n      <td>0.488902</td>\n      <td>0.938683</td>\n      <td>0.488902</td>\n      <td>0.938683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.488902</td>\n      <td>0.938683</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>client_5</td>\n      <td>ds2</td>\n      <td>4.163899</td>\n      <td>5.987546</td>\n      <td>(γ=0.61, α=0.52, β=5.8, τ=2.8, k=7, sh=0.28, d...</td>\n      <td>0.614877</td>\n      <td>0.522026</td>\n      <td>5.792319</td>\n      <td>2.784551</td>\n      <td>...</td>\n      <td>0.489800</td>\n      <td>0.942876</td>\n      <td>0.489800</td>\n      <td>0.942876</td>\n      <td>0.489800</td>\n      <td>0.942876</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.489800</td>\n      <td>0.942876</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>client_6</td>\n      <td>ds3</td>\n      <td>13.143195</td>\n      <td>5.949632</td>\n      <td>(γ=1.13, α=0.20, β=4.5, τ=3.3, k=3, sh=0.12, d...</td>\n      <td>1.133613</td>\n      <td>0.199126</td>\n      <td>4.511599</td>\n      <td>3.323043</td>\n      <td>...</td>\n      <td>0.485627</td>\n      <td>0.943933</td>\n      <td>0.485627</td>\n      <td>0.943933</td>\n      <td>0.485627</td>\n      <td>0.943933</td>\n      <td>0.485627</td>\n      <td>0.943933</td>\n      <td>0.485627</td>\n      <td>0.943933</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>client_7</td>\n      <td>ds3</td>\n      <td>9.825009</td>\n      <td>5.999177</td>\n      <td>(γ=1.16, α=0.36, β=4.5, τ=2.4, k=3, sh=0.12, d...</td>\n      <td>1.163480</td>\n      <td>0.361975</td>\n      <td>4.511599</td>\n      <td>2.410383</td>\n      <td>...</td>\n      <td>0.490888</td>\n      <td>0.949444</td>\n      <td>0.490888</td>\n      <td>0.949444</td>\n      <td>0.490888</td>\n      <td>0.949444</td>\n      <td>0.490888</td>\n      <td>0.949444</td>\n      <td>0.490888</td>\n      <td>0.949444</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>client_8</td>\n      <td>ds3</td>\n      <td>12.263569</td>\n      <td>6.002362</td>\n      <td>(γ=1.18, α=0.53, β=5.2, τ=3.0, k=5, sh=0.23, d...</td>\n      <td>1.178856</td>\n      <td>0.530371</td>\n      <td>5.195688</td>\n      <td>3.038469</td>\n      <td>...</td>\n      <td>0.486209</td>\n      <td>0.943719</td>\n      <td>0.486209</td>\n      <td>0.943719</td>\n      <td>0.486209</td>\n      <td>0.943719</td>\n      <td>0.486209</td>\n      <td>0.943719</td>\n      <td>0.486209</td>\n      <td>0.943719</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>client_9</td>\n      <td>ds4</td>\n      <td>6.640467</td>\n      <td>5.926919</td>\n      <td>(γ=0.92, α=0.52, β=3.9, τ=2.2, k=3, sh=0.27, d...</td>\n      <td>0.921893</td>\n      <td>0.519938</td>\n      <td>3.864403</td>\n      <td>2.184408</td>\n      <td>...</td>\n      <td>0.476817</td>\n      <td>0.936757</td>\n      <td>0.476817</td>\n      <td>0.936757</td>\n      <td>0.476817</td>\n      <td>0.936757</td>\n      <td>0.476817</td>\n      <td>0.936757</td>\n      <td>0.476817</td>\n      <td>0.936757</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>client_10</td>\n      <td>ds4</td>\n      <td>0.209736</td>\n      <td>5.914981</td>\n      <td>(γ=0.95, α=0.55, β=2.5, τ=2.0, k=3, sh=0.33, d...</td>\n      <td>0.947269</td>\n      <td>0.551120</td>\n      <td>2.481549</td>\n      <td>1.990395</td>\n      <td>...</td>\n      <td>0.491254</td>\n      <td>0.944593</td>\n      <td>0.491254</td>\n      <td>0.944593</td>\n      <td>0.491254</td>\n      <td>0.944593</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.491254</td>\n      <td>0.944593</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>client_11</td>\n      <td>ds4</td>\n      <td>0.232460</td>\n      <td>5.896827</td>\n      <td>(γ=0.90, α=0.62, β=2.0, τ=2.2, k=3, sh=0.35, d...</td>\n      <td>0.898665</td>\n      <td>0.624441</td>\n      <td>2.000000</td>\n      <td>2.176174</td>\n      <td>...</td>\n      <td>0.477735</td>\n      <td>0.941027</td>\n      <td>0.477735</td>\n      <td>0.941027</td>\n      <td>0.477735</td>\n      <td>0.941027</td>\n      <td>0.477735</td>\n      <td>0.941027</td>\n      <td>0.477735</td>\n      <td>0.941027</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2</td>\n      <td>client_0</td>\n      <td>ds1</td>\n      <td>8.835107</td>\n      <td>5.825562</td>\n      <td>(γ=1.50, α=0.47, β=3.0, τ=3.3, k=5, sh=0.25, d...</td>\n      <td>1.500000</td>\n      <td>0.470438</td>\n      <td>3.008206</td>\n      <td>3.260905</td>\n      <td>...</td>\n      <td>0.494623</td>\n      <td>0.943845</td>\n      <td>0.494623</td>\n      <td>0.943845</td>\n      <td>0.494623</td>\n      <td>0.943845</td>\n      <td>0.494623</td>\n      <td>0.943845</td>\n      <td>0.494623</td>\n      <td>0.943845</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>client_1</td>\n      <td>ds1</td>\n      <td>4.496946</td>\n      <td>5.842251</td>\n      <td>(γ=1.23, α=0.08, β=9.1, τ=2.4, k=5, sh=0.01, d...</td>\n      <td>1.231308</td>\n      <td>0.080000</td>\n      <td>9.061774</td>\n      <td>2.389782</td>\n      <td>...</td>\n      <td>0.487925</td>\n      <td>0.948592</td>\n      <td>0.487925</td>\n      <td>0.948592</td>\n      <td>0.487925</td>\n      <td>0.948592</td>\n      <td>0.487925</td>\n      <td>0.948592</td>\n      <td>0.487925</td>\n      <td>0.948592</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2</td>\n      <td>client_2</td>\n      <td>ds1</td>\n      <td>0.199398</td>\n      <td>5.783090</td>\n      <td>(γ=0.82, α=0.37, β=2.0, τ=1.7, k=5, sh=0.27, d...</td>\n      <td>0.816557</td>\n      <td>0.372134</td>\n      <td>2.000000</td>\n      <td>1.676045</td>\n      <td>...</td>\n      <td>0.487805</td>\n      <td>0.943652</td>\n      <td>0.487805</td>\n      <td>0.943652</td>\n      <td>0.487805</td>\n      <td>0.943652</td>\n      <td>0.487805</td>\n      <td>0.943652</td>\n      <td>0.487805</td>\n      <td>0.943652</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n      <td>client_3</td>\n      <td>ds2</td>\n      <td>10.625721</td>\n      <td>5.784555</td>\n      <td>(γ=0.81, α=0.49, β=4.4, τ=2.7, k=7, sh=0.35, d...</td>\n      <td>0.813214</td>\n      <td>0.486914</td>\n      <td>4.445728</td>\n      <td>2.714397</td>\n      <td>...</td>\n      <td>0.489180</td>\n      <td>0.946478</td>\n      <td>0.489180</td>\n      <td>0.946478</td>\n      <td>0.489180</td>\n      <td>0.946478</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.489180</td>\n      <td>0.946478</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2</td>\n      <td>client_4</td>\n      <td>ds2</td>\n      <td>0.220300</td>\n      <td>5.853811</td>\n      <td>(γ=0.64, α=0.66, β=3.2, τ=2.4, k=3, sh=0.33, d...</td>\n      <td>0.643158</td>\n      <td>0.659146</td>\n      <td>3.212204</td>\n      <td>2.388446</td>\n      <td>...</td>\n      <td>0.485590</td>\n      <td>0.942289</td>\n      <td>0.485590</td>\n      <td>0.942289</td>\n      <td>0.485590</td>\n      <td>0.942289</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.485590</td>\n      <td>0.942289</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2</td>\n      <td>client_5</td>\n      <td>ds2</td>\n      <td>2.942063</td>\n      <td>5.846388</td>\n      <td>(γ=1.02, α=0.15, β=5.9, τ=3.4, k=3, sh=0.06, d...</td>\n      <td>1.015920</td>\n      <td>0.154840</td>\n      <td>5.913049</td>\n      <td>3.446064</td>\n      <td>...</td>\n      <td>0.497811</td>\n      <td>0.945306</td>\n      <td>0.497811</td>\n      <td>0.945306</td>\n      <td>0.497811</td>\n      <td>0.945306</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.497811</td>\n      <td>0.945306</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>client_6</td>\n      <td>ds3</td>\n      <td>16.172888</td>\n      <td>5.757773</td>\n      <td>(γ=1.11, α=0.08, β=4.6, τ=3.7, k=3, sh=0.28, d...</td>\n      <td>1.111488</td>\n      <td>0.081618</td>\n      <td>4.638423</td>\n      <td>3.678511</td>\n      <td>...</td>\n      <td>0.490537</td>\n      <td>0.945146</td>\n      <td>0.490537</td>\n      <td>0.945146</td>\n      <td>0.490537</td>\n      <td>0.945146</td>\n      <td>0.490537</td>\n      <td>0.945146</td>\n      <td>0.490537</td>\n      <td>0.945146</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2</td>\n      <td>client_7</td>\n      <td>ds3</td>\n      <td>11.054463</td>\n      <td>5.793454</td>\n      <td>(γ=0.96, α=0.22, β=5.0, τ=3.8, k=3, sh=0.09, d...</td>\n      <td>0.956287</td>\n      <td>0.216165</td>\n      <td>4.960932</td>\n      <td>3.800000</td>\n      <td>...</td>\n      <td>0.492034</td>\n      <td>0.950966</td>\n      <td>0.492034</td>\n      <td>0.950966</td>\n      <td>0.492034</td>\n      <td>0.950966</td>\n      <td>0.492034</td>\n      <td>0.950966</td>\n      <td>0.492034</td>\n      <td>0.950966</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2</td>\n      <td>client_8</td>\n      <td>ds3</td>\n      <td>12.811290</td>\n      <td>5.792402</td>\n      <td>(γ=1.14, α=0.17, β=7.2, τ=3.3, k=3, sh=0.06, d...</td>\n      <td>1.140023</td>\n      <td>0.169270</td>\n      <td>7.157392</td>\n      <td>3.346643</td>\n      <td>...</td>\n      <td>0.488875</td>\n      <td>0.947206</td>\n      <td>0.488875</td>\n      <td>0.947206</td>\n      <td>0.488875</td>\n      <td>0.947206</td>\n      <td>0.488875</td>\n      <td>0.947206</td>\n      <td>0.488875</td>\n      <td>0.947206</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2</td>\n      <td>client_9</td>\n      <td>ds4</td>\n      <td>8.523519</td>\n      <td>5.804382</td>\n      <td>(γ=1.09, α=0.70, β=2.0, τ=2.5, k=3, sh=0.32, d...</td>\n      <td>1.088305</td>\n      <td>0.700000</td>\n      <td>2.000000</td>\n      <td>2.473522</td>\n      <td>...</td>\n      <td>0.478561</td>\n      <td>0.938044</td>\n      <td>0.478561</td>\n      <td>0.938044</td>\n      <td>0.478561</td>\n      <td>0.938044</td>\n      <td>0.478561</td>\n      <td>0.938044</td>\n      <td>0.478561</td>\n      <td>0.938044</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2</td>\n      <td>client_10</td>\n      <td>ds4</td>\n      <td>0.218942</td>\n      <td>5.732693</td>\n      <td>(γ=0.90, α=0.67, β=2.0, τ=2.0, k=3, sh=0.35, d...</td>\n      <td>0.899258</td>\n      <td>0.667218</td>\n      <td>2.000000</td>\n      <td>1.989042</td>\n      <td>...</td>\n      <td>0.488213</td>\n      <td>0.943466</td>\n      <td>0.488213</td>\n      <td>0.943466</td>\n      <td>0.488213</td>\n      <td>0.943466</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.488213</td>\n      <td>0.943466</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n      <td>client_11</td>\n      <td>ds4</td>\n      <td>0.233247</td>\n      <td>5.746492</td>\n      <td>(γ=0.94, α=0.70, β=2.0, τ=2.1, k=3, sh=0.32, d...</td>\n      <td>0.941782</td>\n      <td>0.700000</td>\n      <td>2.000000</td>\n      <td>2.109315</td>\n      <td>...</td>\n      <td>0.476577</td>\n      <td>0.940738</td>\n      <td>0.476577</td>\n      <td>0.940738</td>\n      <td>0.476577</td>\n      <td>0.940738</td>\n      <td>0.476577</td>\n      <td>0.940738</td>\n      <td>0.476577</td>\n      <td>0.940738</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>3</td>\n      <td>client_0</td>\n      <td>ds1</td>\n      <td>9.897944</td>\n      <td>5.738739</td>\n      <td>(γ=1.24, α=0.56, β=2.0, τ=2.2, k=3, sh=0.25, d...</td>\n      <td>1.244488</td>\n      <td>0.556118</td>\n      <td>2.000000</td>\n      <td>2.189196</td>\n      <td>...</td>\n      <td>0.495392</td>\n      <td>0.944818</td>\n      <td>0.495392</td>\n      <td>0.944817</td>\n      <td>0.495392</td>\n      <td>0.944817</td>\n      <td>0.495392</td>\n      <td>0.944817</td>\n      <td>0.495392</td>\n      <td>0.944817</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>3</td>\n      <td>client_1</td>\n      <td>ds1</td>\n      <td>4.122640</td>\n      <td>5.846867</td>\n      <td>(γ=1.39, α=0.10, β=6.4, τ=3.5, k=3, sh=0.00, d...</td>\n      <td>1.390871</td>\n      <td>0.099405</td>\n      <td>6.425513</td>\n      <td>3.468924</td>\n      <td>...</td>\n      <td>0.490546</td>\n      <td>0.949060</td>\n      <td>0.490546</td>\n      <td>0.949060</td>\n      <td>0.490546</td>\n      <td>0.949060</td>\n      <td>0.490546</td>\n      <td>0.949060</td>\n      <td>0.490546</td>\n      <td>0.949060</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>3</td>\n      <td>client_2</td>\n      <td>ds1</td>\n      <td>0.207307</td>\n      <td>5.779426</td>\n      <td>(γ=0.76, α=0.55, β=2.4, τ=2.5, k=5, sh=0.26, d...</td>\n      <td>0.756566</td>\n      <td>0.547553</td>\n      <td>2.440866</td>\n      <td>2.451606</td>\n      <td>...</td>\n      <td>0.489020</td>\n      <td>0.944704</td>\n      <td>0.489020</td>\n      <td>0.944704</td>\n      <td>0.489020</td>\n      <td>0.944704</td>\n      <td>0.489020</td>\n      <td>0.944704</td>\n      <td>0.489020</td>\n      <td>0.944704</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>3</td>\n      <td>client_3</td>\n      <td>ds2</td>\n      <td>4.694156</td>\n      <td>5.788155</td>\n      <td>(γ=0.70, α=0.66, β=4.4, τ=3.2, k=5, sh=0.30, d...</td>\n      <td>0.702756</td>\n      <td>0.660924</td>\n      <td>4.368978</td>\n      <td>3.162710</td>\n      <td>...</td>\n      <td>0.486550</td>\n      <td>0.946270</td>\n      <td>0.486550</td>\n      <td>0.946270</td>\n      <td>0.486550</td>\n      <td>0.946270</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.486550</td>\n      <td>0.946270</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>3</td>\n      <td>client_4</td>\n      <td>ds2</td>\n      <td>5.447990</td>\n      <td>5.804596</td>\n      <td>(γ=0.60, α=0.52, β=5.7, τ=3.2, k=3, sh=0.27, d...</td>\n      <td>0.600000</td>\n      <td>0.516396</td>\n      <td>5.703050</td>\n      <td>3.233999</td>\n      <td>...</td>\n      <td>0.490073</td>\n      <td>0.944602</td>\n      <td>0.490073</td>\n      <td>0.944602</td>\n      <td>0.490073</td>\n      <td>0.944602</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.490073</td>\n      <td>0.944602</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3</td>\n      <td>client_5</td>\n      <td>ds2</td>\n      <td>2.581774</td>\n      <td>5.871058</td>\n      <td>(γ=0.61, α=0.45, β=4.4, τ=2.0, k=7, sh=0.16, d...</td>\n      <td>0.608097</td>\n      <td>0.448936</td>\n      <td>4.424067</td>\n      <td>2.012359</td>\n      <td>...</td>\n      <td>0.489620</td>\n      <td>0.940646</td>\n      <td>0.489620</td>\n      <td>0.940646</td>\n      <td>0.489620</td>\n      <td>0.940646</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.489620</td>\n      <td>0.940646</td>\n    </tr>\n  </tbody>\n</table>\n<p>30 rows × 61 columns</p>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n============================================================================================\nSTEP 11: FINAL EVALUATION (FEDERATED VAL + TEST)\n============================================================================================\n\n--------------------------------------------------------------------------------------------\nVAL+TEST tables (federated, per-dataset + global)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                               setting split                dataset       acc  \\\n0  Enhanced FELCM (Best θ per dataset)   VAL  all datasets weighted  0.983097   \n1          Enhanced FELCM (Best θ ds1)  TEST                    ds1  0.978402   \n2          Enhanced FELCM (Best θ ds2)  TEST                    ds2  0.987378   \n3          Enhanced FELCM (Best θ ds3)  TEST                    ds3  0.991461   \n4          Enhanced FELCM (Best θ ds4)  TEST                    ds4  0.961326   \n5              Enhanced FELCM (Best θ)  TEST        global weighted  0.978152   \n\n   precision_macro  recall_macro  f1_macro  precision_weighted  \\\n0         0.844038      0.924235  0.863740            0.988753   \n1         0.981584      0.977547  0.979278            0.979055   \n2         0.905945      0.905039  0.905469            0.988551   \n3         0.991283      0.991519  0.991330            0.991659   \n4         0.962196      0.961883  0.961765            0.961832   \n5         0.955448      0.954197  0.954654            0.978832   \n\n   recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr   loss_ce  \\\n0         0.983097     0.985377  0.131958                NaN  0.131386   \n1         0.978402     0.978391  0.136622           0.998678  0.135022   \n2         0.987378     0.987939  0.121670                NaN  0.120600   \n3         0.991461     0.991491  0.088016           0.999698  0.087542   \n4         0.961326     0.961300  0.179371           0.996109  0.183370   \n5         0.978152     0.978310  0.136636                NaN  0.137078   \n\n   eval_time_s  \n0     3.548910  \n1     6.445197  \n2     7.165533  \n3     5.388990  \n4     8.016314  \n5     6.943200  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>setting</th>\n      <th>split</th>\n      <th>dataset</th>\n      <th>acc</th>\n      <th>precision_macro</th>\n      <th>recall_macro</th>\n      <th>f1_macro</th>\n      <th>precision_weighted</th>\n      <th>recall_weighted</th>\n      <th>f1_weighted</th>\n      <th>log_loss</th>\n      <th>auc_roc_macro_ovr</th>\n      <th>loss_ce</th>\n      <th>eval_time_s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Enhanced FELCM (Best θ per dataset)</td>\n      <td>VAL</td>\n      <td>all datasets weighted</td>\n      <td>0.983097</td>\n      <td>0.844038</td>\n      <td>0.924235</td>\n      <td>0.863740</td>\n      <td>0.988753</td>\n      <td>0.983097</td>\n      <td>0.985377</td>\n      <td>0.131958</td>\n      <td>NaN</td>\n      <td>0.131386</td>\n      <td>3.548910</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Enhanced FELCM (Best θ ds1)</td>\n      <td>TEST</td>\n      <td>ds1</td>\n      <td>0.978402</td>\n      <td>0.981584</td>\n      <td>0.977547</td>\n      <td>0.979278</td>\n      <td>0.979055</td>\n      <td>0.978402</td>\n      <td>0.978391</td>\n      <td>0.136622</td>\n      <td>0.998678</td>\n      <td>0.135022</td>\n      <td>6.445197</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Enhanced FELCM (Best θ ds2)</td>\n      <td>TEST</td>\n      <td>ds2</td>\n      <td>0.987378</td>\n      <td>0.905945</td>\n      <td>0.905039</td>\n      <td>0.905469</td>\n      <td>0.988551</td>\n      <td>0.987378</td>\n      <td>0.987939</td>\n      <td>0.121670</td>\n      <td>NaN</td>\n      <td>0.120600</td>\n      <td>7.165533</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Enhanced FELCM (Best θ ds3)</td>\n      <td>TEST</td>\n      <td>ds3</td>\n      <td>0.991461</td>\n      <td>0.991283</td>\n      <td>0.991519</td>\n      <td>0.991330</td>\n      <td>0.991659</td>\n      <td>0.991461</td>\n      <td>0.991491</td>\n      <td>0.088016</td>\n      <td>0.999698</td>\n      <td>0.087542</td>\n      <td>5.388990</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Enhanced FELCM (Best θ ds4)</td>\n      <td>TEST</td>\n      <td>ds4</td>\n      <td>0.961326</td>\n      <td>0.962196</td>\n      <td>0.961883</td>\n      <td>0.961765</td>\n      <td>0.961832</td>\n      <td>0.961326</td>\n      <td>0.961300</td>\n      <td>0.179371</td>\n      <td>0.996109</td>\n      <td>0.183370</td>\n      <td>8.016314</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Enhanced FELCM (Best θ)</td>\n      <td>TEST</td>\n      <td>global weighted</td>\n      <td>0.978152</td>\n      <td>0.955448</td>\n      <td>0.954197</td>\n      <td>0.954654</td>\n      <td>0.978832</td>\n      <td>0.978152</td>\n      <td>0.978310</td>\n      <td>0.136636</td>\n      <td>NaN</td>\n      <td>0.137078</td>\n      <td>6.943200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nRequested explicit metrics (acc, pre, rec, f1, logloss, auc_roc)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                     dataset       acc       pre       rec        f1  \\\n0  all_datasets_val_weighted  0.983097  0.844038  0.924235  0.863740   \n1                   ds1_test  0.978402  0.981584  0.977547  0.979278   \n2                   ds2_test  0.987378  0.905945  0.905039  0.905469   \n3                   ds3_test  0.991461  0.991283  0.991519  0.991330   \n4                   ds4_test  0.961326  0.962196  0.961883  0.961765   \n5       global_test_weighted  0.978152  0.955448  0.954197  0.954654   \n\n    logloss   auc_roc  \n0  0.131958       NaN  \n1  0.136622  0.998678  \n2  0.121670       NaN  \n3  0.088016  0.999698  \n4  0.179371  0.996109  \n5  0.136636       NaN  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>acc</th>\n      <th>pre</th>\n      <th>rec</th>\n      <th>f1</th>\n      <th>logloss</th>\n      <th>auc_roc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>all_datasets_val_weighted</td>\n      <td>0.983097</td>\n      <td>0.844038</td>\n      <td>0.924235</td>\n      <td>0.863740</td>\n      <td>0.131958</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ds1_test</td>\n      <td>0.978402</td>\n      <td>0.981584</td>\n      <td>0.977547</td>\n      <td>0.979278</td>\n      <td>0.136622</td>\n      <td>0.998678</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ds2_test</td>\n      <td>0.987378</td>\n      <td>0.905945</td>\n      <td>0.905039</td>\n      <td>0.905469</td>\n      <td>0.121670</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ds3_test</td>\n      <td>0.991461</td>\n      <td>0.991283</td>\n      <td>0.991519</td>\n      <td>0.991330</td>\n      <td>0.088016</td>\n      <td>0.999698</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ds4_test</td>\n      <td>0.961326</td>\n      <td>0.962196</td>\n      <td>0.961883</td>\n      <td>0.961765</td>\n      <td>0.179371</td>\n      <td>0.996109</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>global_test_weighted</td>\n      <td>0.978152</td>\n      <td>0.955448</td>\n      <td>0.954197</td>\n      <td>0.954654</td>\n      <td>0.136636</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nPaper selection summary:\n- Best round (by federated VAL accuracy): round=12 | best_val_acc=0.9885\n- Best θ ds1: (γ=1.36, α=0.08, β=6.8, τ=3.1, k=5, sh=0.05, dn=0.20)\n- Best θ ds2: (γ=0.68, α=0.44, β=4.5, τ=2.5, k=7, sh=0.27, dn=0.12)\n- Best θ ds3: (γ=1.18, α=0.53, β=5.2, τ=3.0, k=5, sh=0.23, dn=0.04)\n- Best θ ds4: (γ=0.97, α=0.49, β=2.0, τ=2.1, k=3, sh=0.34, dn=0.01)\n\n============================================================================================\nSTEP 12: PREPROCESSING VALIDATION (DS1 VAL SAMPLE)\n============================================================================================\n\n--------------------------------------------------------------------------------------------\nPreprocessing validation summary (DS1 VAL sample)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "               metric      mean       std       min       max\n0  edge_energy_before  0.033557  0.011519  0.010264  0.110010\n1   edge_energy_after  0.045955  0.010286  0.018803  0.102962\n2      entropy_before  5.620200  0.832229  2.852003  7.564816\n3       entropy_after  5.989759  0.673774  3.300354  7.341709\n4     contrast_before  0.168932  0.034004  0.089726  0.356358\n5      contrast_after  0.191185  0.016224  0.156567  0.316045\n6     edge_gain_ratio  1.437564  0.296954  0.866943  3.060668\n7       entropy_delta  0.369558  0.206458 -0.234723  0.920488\n8      contrast_delta  0.022253  0.028139 -0.055470  0.108779",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>metric</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>edge_energy_before</td>\n      <td>0.033557</td>\n      <td>0.011519</td>\n      <td>0.010264</td>\n      <td>0.110010</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>edge_energy_after</td>\n      <td>0.045955</td>\n      <td>0.010286</td>\n      <td>0.018803</td>\n      <td>0.102962</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>entropy_before</td>\n      <td>5.620200</td>\n      <td>0.832229</td>\n      <td>2.852003</td>\n      <td>7.564816</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>entropy_after</td>\n      <td>5.989759</td>\n      <td>0.673774</td>\n      <td>3.300354</td>\n      <td>7.341709</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>contrast_before</td>\n      <td>0.168932</td>\n      <td>0.034004</td>\n      <td>0.089726</td>\n      <td>0.356358</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>contrast_after</td>\n      <td>0.191185</td>\n      <td>0.016224</td>\n      <td>0.156567</td>\n      <td>0.316045</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>edge_gain_ratio</td>\n      <td>1.437564</td>\n      <td>0.296954</td>\n      <td>0.866943</td>\n      <td>3.060668</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>entropy_delta</td>\n      <td>0.369558</td>\n      <td>0.206458</td>\n      <td>-0.234723</td>\n      <td>0.920488</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>contrast_delta</td>\n      <td>0.022253</td>\n      <td>0.028139</td>\n      <td>-0.055470</td>\n      <td>0.108779</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n============================================================================================\nSTEP 13: COMPUTE CURVES + CONFUSION + CALIBRATION TABLES (NO PLOTS)\n============================================================================================\n\n--------------------------------------------------------------------------------------------\nROC curve points (DS1 TEST, first 20 rows)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "         class       fpr       tpr  threshold\n0       glioma  0.000000  0.000000        inf\n1       glioma  0.000000  0.005435   0.912852\n2       glioma  0.000000  0.016304   0.911762\n3       glioma  0.003584  0.016304   0.911719\n4       glioma  0.003584  0.608696   0.908241\n5       glioma  0.007168  0.608696   0.908240\n6       glioma  0.007168  0.983696   0.903709\n7       glioma  0.010753  0.983696   0.902528\n8       glioma  0.010753  0.989130   0.900419\n9       glioma  0.021505  0.989130   0.743146\n10      glioma  0.021505  0.994565   0.724339\n11      glioma  0.035842  0.994565   0.371017\n12      glioma  0.035842  1.000000   0.323106\n13      glioma  0.652330  1.000000   0.016748\n14      glioma  0.659498  1.000000   0.016715\n15      glioma  0.992832  1.000000   0.007197\n16      glioma  1.000000  1.000000   0.006892\n17  meningioma  0.000000  0.000000        inf\n18  meningioma  0.000000  0.006135   0.919772\n19  meningioma  0.000000  0.938650   0.588275",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>fpr</th>\n      <th>tpr</th>\n      <th>threshold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>glioma</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>inf</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>glioma</td>\n      <td>0.000000</td>\n      <td>0.005435</td>\n      <td>0.912852</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>glioma</td>\n      <td>0.000000</td>\n      <td>0.016304</td>\n      <td>0.911762</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>glioma</td>\n      <td>0.003584</td>\n      <td>0.016304</td>\n      <td>0.911719</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>glioma</td>\n      <td>0.003584</td>\n      <td>0.608696</td>\n      <td>0.908241</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>glioma</td>\n      <td>0.007168</td>\n      <td>0.608696</td>\n      <td>0.908240</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>glioma</td>\n      <td>0.007168</td>\n      <td>0.983696</td>\n      <td>0.903709</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>glioma</td>\n      <td>0.010753</td>\n      <td>0.983696</td>\n      <td>0.902528</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>glioma</td>\n      <td>0.010753</td>\n      <td>0.989130</td>\n      <td>0.900419</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>glioma</td>\n      <td>0.021505</td>\n      <td>0.989130</td>\n      <td>0.743146</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>glioma</td>\n      <td>0.021505</td>\n      <td>0.994565</td>\n      <td>0.724339</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>glioma</td>\n      <td>0.035842</td>\n      <td>0.994565</td>\n      <td>0.371017</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>glioma</td>\n      <td>0.035842</td>\n      <td>1.000000</td>\n      <td>0.323106</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>glioma</td>\n      <td>0.652330</td>\n      <td>1.000000</td>\n      <td>0.016748</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>glioma</td>\n      <td>0.659498</td>\n      <td>1.000000</td>\n      <td>0.016715</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>glioma</td>\n      <td>0.992832</td>\n      <td>1.000000</td>\n      <td>0.007197</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>glioma</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.006892</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>meningioma</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>inf</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>meningioma</td>\n      <td>0.000000</td>\n      <td>0.006135</td>\n      <td>0.919772</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>meningioma</td>\n      <td>0.000000</td>\n      <td>0.938650</td>\n      <td>0.588275</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nPR curve points (DS1 TEST, first 20 rows)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "     class  precision  recall  threshold\n0   glioma   0.397408     1.0   0.006892\n1   glioma   0.399132     1.0   0.007197\n2   glioma   0.400000     1.0   0.007210\n3   glioma   0.400871     1.0   0.007232\n4   glioma   0.401747     1.0   0.007319\n5   glioma   0.402626     1.0   0.007361\n6   glioma   0.403509     1.0   0.007382\n7   glioma   0.404396     1.0   0.007513\n8   glioma   0.405286     1.0   0.007594\n9   glioma   0.406181     1.0   0.007615\n10  glioma   0.407080     1.0   0.007704\n11  glioma   0.407982     1.0   0.007716\n12  glioma   0.408889     1.0   0.007775\n13  glioma   0.409800     1.0   0.007809\n14  glioma   0.410714     1.0   0.007826\n15  glioma   0.411633     1.0   0.007868\n16  glioma   0.412556     1.0   0.007892\n17  glioma   0.413483     1.0   0.007987\n18  glioma   0.414414     1.0   0.007991\n19  glioma   0.415350     1.0   0.008003",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>threshold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>glioma</td>\n      <td>0.397408</td>\n      <td>1.0</td>\n      <td>0.006892</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>glioma</td>\n      <td>0.399132</td>\n      <td>1.0</td>\n      <td>0.007197</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>glioma</td>\n      <td>0.400000</td>\n      <td>1.0</td>\n      <td>0.007210</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>glioma</td>\n      <td>0.400871</td>\n      <td>1.0</td>\n      <td>0.007232</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>glioma</td>\n      <td>0.401747</td>\n      <td>1.0</td>\n      <td>0.007319</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>glioma</td>\n      <td>0.402626</td>\n      <td>1.0</td>\n      <td>0.007361</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>glioma</td>\n      <td>0.403509</td>\n      <td>1.0</td>\n      <td>0.007382</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>glioma</td>\n      <td>0.404396</td>\n      <td>1.0</td>\n      <td>0.007513</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>glioma</td>\n      <td>0.405286</td>\n      <td>1.0</td>\n      <td>0.007594</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>glioma</td>\n      <td>0.406181</td>\n      <td>1.0</td>\n      <td>0.007615</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>glioma</td>\n      <td>0.407080</td>\n      <td>1.0</td>\n      <td>0.007704</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>glioma</td>\n      <td>0.407982</td>\n      <td>1.0</td>\n      <td>0.007716</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>glioma</td>\n      <td>0.408889</td>\n      <td>1.0</td>\n      <td>0.007775</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>glioma</td>\n      <td>0.409800</td>\n      <td>1.0</td>\n      <td>0.007809</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>glioma</td>\n      <td>0.410714</td>\n      <td>1.0</td>\n      <td>0.007826</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>glioma</td>\n      <td>0.411633</td>\n      <td>1.0</td>\n      <td>0.007868</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>glioma</td>\n      <td>0.412556</td>\n      <td>1.0</td>\n      <td>0.007892</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>glioma</td>\n      <td>0.413483</td>\n      <td>1.0</td>\n      <td>0.007987</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>glioma</td>\n      <td>0.414414</td>\n      <td>1.0</td>\n      <td>0.007991</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>glioma</td>\n      <td>0.415350</td>\n      <td>1.0</td>\n      <td>0.008003</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nConfusion matrix counts (DS1 TEST)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "         true  glioma  meningioma  notumor  pituitary\n0      glioma     183           1        0          0\n1  meningioma       6         155        0          2\n2     notumor       2           0       39          0\n3   pituitary       0           0        1         74",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>true</th>\n      <th>glioma</th>\n      <th>meningioma</th>\n      <th>notumor</th>\n      <th>pituitary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>glioma</td>\n      <td>183</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>meningioma</td>\n      <td>6</td>\n      <td>155</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>notumor</td>\n      <td>2</td>\n      <td>0</td>\n      <td>39</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pituitary</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>74</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nConfusion matrix row-normalized (DS1 TEST)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "         true    glioma  meningioma   notumor  pituitary\n0      glioma  0.994565    0.005435  0.000000   0.000000\n1  meningioma  0.036810    0.950920  0.000000   0.012270\n2     notumor  0.048780    0.000000  0.951220   0.000000\n3   pituitary  0.000000    0.000000  0.013333   0.986667",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>true</th>\n      <th>glioma</th>\n      <th>meningioma</th>\n      <th>notumor</th>\n      <th>pituitary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>glioma</td>\n      <td>0.994565</td>\n      <td>0.005435</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>meningioma</td>\n      <td>0.036810</td>\n      <td>0.950920</td>\n      <td>0.000000</td>\n      <td>0.012270</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>notumor</td>\n      <td>0.048780</td>\n      <td>0.000000</td>\n      <td>0.951220</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pituitary</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.013333</td>\n      <td>0.986667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n--------------------------------------------------------------------------------------------\nCalibration bins table (DS1)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    bin_confidence  bin_accuracy  bin_count\n0              NaN           NaN          0\n1              NaN           NaN          0\n2              NaN           NaN          0\n3              NaN           NaN          0\n4              NaN           NaN          0\n5         0.493923      1.000000          2\n6         0.542857      0.000000          3\n7         0.629280      0.333333          3\n8         0.721814      0.800000          5\n9         0.765013      0.666667          3\n10        0.908195      0.984710        327\n11        0.941508      1.000000        120",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_confidence</th>\n      <th>bin_accuracy</th>\n      <th>bin_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.493923</td>\n      <td>1.000000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.542857</td>\n      <td>0.000000</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.629280</td>\n      <td>0.333333</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.721814</td>\n      <td>0.800000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.765013</td>\n      <td>0.666667</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.908195</td>\n      <td>0.984710</td>\n      <td>327</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.941508</td>\n      <td>1.000000</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n============================================================================================\nSTEP 14: THETA EVOLUTION TABLE\n============================================================================================\n\n--------------------------------------------------------------------------------------------\nMean best-θ parameters over rounds (clients averaged)\n--------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    round  gamma_power  alpha_contrast_weight  beta_contrast_sharpness  \\\n0       1     0.959130               0.460884                 4.353408   \n1       2     1.013108               0.396479                 4.199809   \n2       3     0.985409               0.409887                 4.608343   \n3       4     0.977390               0.458337                 4.749270   \n4       5     0.983716               0.367041                 4.490318   \n5       6     1.019666               0.347660                 4.876448   \n6       7     0.993556               0.361964                 4.992942   \n7       8     1.032470               0.325638                 5.218503   \n8       9     1.054699               0.282786                 4.788884   \n9      10     1.067166               0.359066                 4.611436   \n10     11     1.008095               0.304970                 4.708644   \n11     12     1.035737               0.245101                 5.487991   \n\n    tau_clip  k_blur_kernel_size  sh_sharpen_strength  dn_denoise_strength  \n0   2.650644            4.333333             0.226634             0.080778  \n1   2.772723            3.833333             0.223487             0.108809  \n2   2.782583            4.000000             0.196151             0.082068  \n3   2.707201            3.833333             0.211109             0.069298  \n4   2.877774            4.666667             0.166931             0.111062  \n5   2.865048            4.000000             0.139473             0.122805  \n6   2.607640            3.833333             0.174861             0.096345  \n7   2.772185            4.000000             0.125115             0.092170  \n8   2.925929            4.333333             0.142183             0.125500  \n9   2.664418            4.166667             0.114938             0.096090  \n10  2.754572            4.166667             0.149397             0.115369  \n11  2.533212            4.666667             0.074800             0.118845  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>round</th>\n      <th>gamma_power</th>\n      <th>alpha_contrast_weight</th>\n      <th>beta_contrast_sharpness</th>\n      <th>tau_clip</th>\n      <th>k_blur_kernel_size</th>\n      <th>sh_sharpen_strength</th>\n      <th>dn_denoise_strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.959130</td>\n      <td>0.460884</td>\n      <td>4.353408</td>\n      <td>2.650644</td>\n      <td>4.333333</td>\n      <td>0.226634</td>\n      <td>0.080778</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.013108</td>\n      <td>0.396479</td>\n      <td>4.199809</td>\n      <td>2.772723</td>\n      <td>3.833333</td>\n      <td>0.223487</td>\n      <td>0.108809</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.985409</td>\n      <td>0.409887</td>\n      <td>4.608343</td>\n      <td>2.782583</td>\n      <td>4.000000</td>\n      <td>0.196151</td>\n      <td>0.082068</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.977390</td>\n      <td>0.458337</td>\n      <td>4.749270</td>\n      <td>2.707201</td>\n      <td>3.833333</td>\n      <td>0.211109</td>\n      <td>0.069298</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.983716</td>\n      <td>0.367041</td>\n      <td>4.490318</td>\n      <td>2.877774</td>\n      <td>4.666667</td>\n      <td>0.166931</td>\n      <td>0.111062</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1.019666</td>\n      <td>0.347660</td>\n      <td>4.876448</td>\n      <td>2.865048</td>\n      <td>4.000000</td>\n      <td>0.139473</td>\n      <td>0.122805</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0.993556</td>\n      <td>0.361964</td>\n      <td>4.992942</td>\n      <td>2.607640</td>\n      <td>3.833333</td>\n      <td>0.174861</td>\n      <td>0.096345</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1.032470</td>\n      <td>0.325638</td>\n      <td>5.218503</td>\n      <td>2.772185</td>\n      <td>4.000000</td>\n      <td>0.125115</td>\n      <td>0.092170</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1.054699</td>\n      <td>0.282786</td>\n      <td>4.788884</td>\n      <td>2.925929</td>\n      <td>4.333333</td>\n      <td>0.142183</td>\n      <td>0.125500</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1.067166</td>\n      <td>0.359066</td>\n      <td>4.611436</td>\n      <td>2.664418</td>\n      <td>4.166667</td>\n      <td>0.114938</td>\n      <td>0.096090</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1.008095</td>\n      <td>0.304970</td>\n      <td>4.708644</td>\n      <td>2.754572</td>\n      <td>4.166667</td>\n      <td>0.149397</td>\n      <td>0.115369</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>1.035737</td>\n      <td>0.245101</td>\n      <td>5.487991</td>\n      <td>2.533212</td>\n      <td>4.666667</td>\n      <td>0.074800</td>\n      <td>0.118845</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n============================================================================================\nSTEP 15: SAVING ONLY TWO FILES (CHECKPOINT + ONE CSV)\n============================================================================================\n✅ Saved checkpoint: /kaggle/working/outputs/FL_GAFELCM_PVTv2B2_FUSION_checkpoint.pth\n✅ Saved CSV (ALL outputs): /kaggle/working/outputs/ALL_OUTPUTS_AND_METRICS.csv\n\nDONE ✅ (KAGGLE, TRUE FL SIMULATION, 12 clients (3x4 datasets), rounds=12, Preprocessing+GA, Augmentation, Fusion, PVTv2-B2, no plots)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "\n",
        "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "\n",
        "DATASETS = {\n",
        "    \"ds1\": CFG[\"ds1_base\"],  # or put your paths directly\n",
        "    \"ds2\": CFG[\"ds2_base\"],\n",
        "    \"ds3\": CFG[\"ds3_base\"],\n",
        "    \"ds4\": CFG[\"ds4_base\"],\n",
        "}\n",
        "\n",
        "def iter_images(root):\n",
        "    root = Path(root)\n",
        "    for p in root.rglob(\"*\"):\n",
        "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
        "            yield p\n",
        "\n",
        "def image_size_counts(root, max_images=None):\n",
        "    cnt = Counter()\n",
        "    n_ok, n_bad = 0, 0\n",
        "    for i, p in enumerate(iter_images(root)):\n",
        "        if max_images is not None and i >= max_images:\n",
        "            break\n",
        "        try:\n",
        "            with Image.open(p) as im:\n",
        "                w, h = im.size\n",
        "            cnt[f\"{w}x{h}\"] += 1\n",
        "            n_ok += 1\n",
        "        except Exception:\n",
        "            n_bad += 1\n",
        "    return cnt, n_ok, n_bad\n",
        "\n",
        "# Change this if you want faster scan (e.g., max_images=2000)\n",
        "MAX_IMAGES = None\n",
        "\n",
        "for name, path in DATASETS.items():\n",
        "    cnt, n_ok, n_bad = image_size_counts(path, max_images=MAX_IMAGES)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"{name.upper()} | root: {path}\")\n",
        "    print(f\"read_ok={n_ok} | read_failed={n_bad} | unique_sizes={len(cnt)}\")\n",
        "    print(\"Top image sizes:\")\n",
        "    for size, c in cnt.most_common(20):\n",
        "        print(f\"  {size:>10}  ->  {c}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-17T10:41:20.143415Z",
          "iopub.execute_input": "2026-02-17T10:41:20.143693Z",
          "iopub.status.idle": "2026-02-17T10:53:13.339469Z",
          "shell.execute_reply.started": "2026-02-17T10:41:20.143667Z",
          "shell.execute_reply": "2026-02-17T10:53:13.338709Z"
        },
        "id": "Tk9wRBQcgyjs",
        "outputId": "849c3342-2a57-409f-b3d1-3380b7c3b2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n======================================================================\nDS1 | root: /kaggle/input/datasets/alamshihab075/brain-tumor-mri-dataset-for-deep-learning\nread_ok=9257 | read_failed=0 | unique_sizes=325\nTop image sizes:\n     512x512  ->  8214\n     225x225  ->  119\n     630x630  ->  66\n     442x442  ->  42\n     236x236  ->  33\n     256x256  ->  25\n     201x251  ->  23\n     201x250  ->  22\n     214x236  ->  18\n     468x444  ->  15\n     504x540  ->  13\n     359x449  ->  13\n     550x664  ->  12\n     393x400  ->  12\n     200x252  ->  11\n     220x275  ->  11\n     400x442  ->  11\n     350x350  ->  10\n     442x454  ->  10\n     232x217  ->  9\n\n======================================================================\nDS2 | root: /kaggle/input/datasets/zehrakucuker/brain-tumor-mri-images-classification-dataset\nread_ok=15605 | read_failed=0 | unique_sizes=387\nTop image sizes:\n     512x512  ->  11006\n     225x225  ->  630\n     630x630  ->  181\n     236x236  ->  180\n     201x251  ->  109\n     442x442  ->  98\n     228x221  ->  92\n     300x168  ->  87\n     232x217  ->  87\n     150x198  ->  80\n     428x417  ->  78\n     200x252  ->  77\n     256x256  ->  72\n     173x201  ->  66\n     206x244  ->  64\n     227x222  ->  62\n     192x192  ->  58\n     201x250  ->  57\n     218x231  ->  53\n     215x234  ->  51\n\n======================================================================\nDS3 | root: /kaggle/input/datasets/chubskuy/brain-tumor-image\nread_ok=7023 | read_failed=0 | unique_sizes=387\nTop image sizes:\n     512x512  ->  4742\n     225x225  ->  332\n     630x630  ->  90\n     236x236  ->  81\n     201x251  ->  58\n     228x221  ->  51\n     232x217  ->  50\n     300x168  ->  49\n     442x442  ->  46\n     150x198  ->  44\n     200x252  ->  43\n     428x417  ->  42\n     227x222  ->  39\n     173x201  ->  36\n     206x244  ->  35\n     256x256  ->  31\n     192x192  ->  31\n     218x231  ->  29\n     201x250  ->  29\n     215x234  ->  28\n\n======================================================================\nDS4 | root: /kaggle/input/datasets/mdzubayerahmadshibly/ds4mine\nread_ok=12064 | read_failed=0 | unique_sizes=434\nTop image sizes:\n     512x512  ->  6008\n     224x224  ->  3732\n     225x225  ->  280\n     236x236  ->  123\n     630x630  ->  70\n     201x251  ->  56\n     256x256  ->  48\n     442x442  ->  41\n     232x217  ->  41\n     428x417  ->  41\n     227x222  ->  39\n     228x221  ->  39\n     200x252  ->  36\n     150x198  ->  35\n     300x168  ->  33\n     201x250  ->  32\n     173x201  ->  31\n     206x244  ->  27\n     215x234  ->  24\n     192x192  ->  24\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}