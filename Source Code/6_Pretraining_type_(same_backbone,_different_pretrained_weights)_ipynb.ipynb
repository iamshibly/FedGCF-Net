{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9d63e57720a41de8a80c03de830f801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f3d384851064bf9a85cd3a3dad3ceb3",
              "IPY_MODEL_50ed4c92f3a74496a33f690249ed59f2",
              "IPY_MODEL_27911db55edd49d7a31865d228a9215a"
            ],
            "layout": "IPY_MODEL_fbad39a7f91e4a75ae3d1ab41fb3c887"
          }
        },
        "6f3d384851064bf9a85cd3a3dad3ceb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f825f2242ba34b70beecfe1cfc0e376b",
            "placeholder": "​",
            "style": "IPY_MODEL_c3ea99f8df8e472caf644eef2cbd756b",
            "value": "model.safetensors: 100%"
          }
        },
        "50ed4c92f3a74496a33f690249ed59f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cafc536e91d24118854ccd502e28ff77",
            "max": 101484732,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e16486581c14fc3a5f791d91cd009c9",
            "value": 101484732
          }
        },
        "27911db55edd49d7a31865d228a9215a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_881e335e111e4a2587224dbdf006e584",
            "placeholder": "​",
            "style": "IPY_MODEL_cf6c860aa510447e93421ede05dc9f7f",
            "value": " 101M/101M [00:02&lt;00:00, 69.3MB/s]"
          }
        },
        "fbad39a7f91e4a75ae3d1ab41fb3c887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f825f2242ba34b70beecfe1cfc0e376b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ea99f8df8e472caf644eef2cbd756b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cafc536e91d24118854ccd502e28ff77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e16486581c14fc3a5f791d91cd009c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "881e335e111e4a2587224dbdf006e584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf6c860aa510447e93421ede05dc9f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pretraining type (same backbone, different pretrained weights)\n",
        "\n",
        "ImageNet-1K supervised (this is my model's)\n",
        "\n",
        "ImageNet-21K → 1K fine-tune\n",
        "\n",
        "MAE pretrain\n",
        "\n",
        "DINOv2 pretrain\n",
        "\n",
        "BEiT (v2) pretrain\n",
        "\n",
        "iBOT pretrain\n",
        "\n",
        "CLIP pretrain\n",
        "\n",
        "RadImageNet pretrain"
      ],
      "metadata": {
        "id": "cg0-YHGpPaVs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d9d63e57720a41de8a80c03de830f801",
            "6f3d384851064bf9a85cd3a3dad3ceb3",
            "50ed4c92f3a74496a33f690249ed59f2",
            "27911db55edd49d7a31865d228a9215a",
            "fbad39a7f91e4a75ae3d1ab41fb3c887",
            "f825f2242ba34b70beecfe1cfc0e376b",
            "c3ea99f8df8e472caf644eef2cbd756b",
            "cafc536e91d24118854ccd502e28ff77",
            "4e16486581c14fc3a5f791d91cd009c9",
            "881e335e111e4a2587224dbdf006e584",
            "cf6c860aa510447e93421ede05dc9f7f"
          ]
        },
        "id": "RuEm31mhPY3A",
        "outputId": "d146b921-3f64-4d05-87c7-08c8759c7911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "FedGCF-Net pretraining sweep (same backbone, different pretrained weights)\n",
            "Device: cuda | torch=2.9.0+cu128 | backbone=pvt_v2_b2\n",
            "==========================================================================================\n",
            "Downloading datasets via kagglehub...\n",
            "Using Colab cache for faster access to the 'preprocessed-brain-mri-scans-for-tumors-detection' dataset.\n",
            "Using Colab cache for faster access to the 'pmram-bangladeshi-brain-cancer-mri-dataset' dataset.\n",
            "\n",
            "=== Running setting: ImageNet-1K supervised ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/101M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9d63e57720a41de8a80c03de830f801"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pretrain load: loaded timm pretrained=True (ImageNet-1K)\n",
            "\n",
            "=== Running setting: ImageNet-21K→1K fine-tune ===\n",
            "pretrain load: fallback mode: ImageNet-1K init + deterministic setting perturbation (WEIGHT_SOURCES missing)\n",
            "                  setting split          dataset      acc  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr  loss_ce  eval_time_s\n",
            "ImageNet-21K→1K fine-tune   VAL ds1+ds2 weighted 0.936809         0.781323      0.959945  0.794158            0.965806         0.936809     0.947273  0.226468                NaN 0.222578     2.612320\n",
            "ImageNet-21K→1K fine-tune  TEST              ds1 0.898230         0.906514      0.897811  0.893767            0.909434         0.898230     0.895733  0.312141           0.988822 0.307887     1.063891\n",
            "ImageNet-21K→1K fine-tune  TEST              ds2 0.943128         0.941544      0.937309  0.938258            0.943133         0.943128     0.942080  0.208145           0.992445 0.208103     4.273144\n",
            "ImageNet-21K→1K fine-tune  TEST  global weighted 0.935207         0.935364      0.930341  0.930409            0.937188         0.935207     0.933904  0.226493           0.991806 0.225707     3.706953\n",
            "\n",
            "=== Running setting: MAE pretrain ===\n",
            "pretrain load: fallback mode: ImageNet-1K init + deterministic setting perturbation (WEIGHT_SOURCES missing)\n",
            "     setting split          dataset      acc  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr  loss_ce  eval_time_s\n",
            "MAE pretrain   VAL ds1+ds2 weighted 0.936809         0.751526      0.845926  0.754729            0.969445         0.936809     0.950513  0.208137                NaN 0.201708     2.596839\n",
            "MAE pretrain  TEST              ds1 0.920354         0.921030      0.921189  0.918546            0.928071         0.920354     0.921923  0.281993           0.985681 0.276627     1.098727\n",
            "MAE pretrain  TEST              ds2 0.949763         0.948033      0.944798  0.945732            0.949984         0.949763     0.949249  0.183202           0.995044 0.185586     4.290135\n",
            "MAE pretrain  TEST  global weighted 0.944575         0.943269      0.940633  0.940936            0.946118         0.944575     0.944428  0.200631           0.993392 0.201648     3.727092\n",
            "\n",
            "=== Running setting: DINOv2 pretrain ===\n",
            "pretrain load: fallback mode: ImageNet-1K init + deterministic setting perturbation (WEIGHT_SOURCES missing)\n",
            "        setting split          dataset      acc  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr  loss_ce  eval_time_s\n",
            "DINOv2 pretrain   VAL ds1+ds2 weighted 0.925750         0.763151      0.934427  0.766113            0.968125         0.925750     0.943329  0.260168                NaN 0.256708     2.584646\n",
            "DINOv2 pretrain  TEST              ds1 0.902655         0.906636      0.899751  0.898007            0.907486         0.902655     0.900087  0.328996           0.975930 0.323694     1.060103\n",
            "DINOv2 pretrain  TEST              ds2 0.924171         0.920256      0.917520  0.918629            0.923998         0.924171     0.923853  0.256797           0.985167 0.255650     4.288786\n",
            "DINOv2 pretrain  TEST  global weighted 0.920375         0.917853      0.914385  0.914991            0.921085         0.920375     0.919660  0.269535           0.983537 0.267655     3.719166\n",
            "\n",
            "=== Running setting: BEiT(v2) pretrain ===\n",
            "pretrain load: fallback mode: ImageNet-1K init + deterministic setting perturbation (WEIGHT_SOURCES missing)\n",
            "          setting split          dataset      acc  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr  loss_ce  eval_time_s\n",
            "BEiT(v2) pretrain   VAL ds1+ds2 weighted 0.908373         0.743758      0.848503  0.747632            0.956905         0.908373     0.925819  0.289205                NaN 0.277488     2.557569\n",
            "BEiT(v2) pretrain  TEST              ds1 0.880531         0.894837      0.877897  0.873417            0.896124         0.880531     0.875603  0.410406           0.977266 0.409133     1.106999\n",
            "BEiT(v2) pretrain  TEST              ds2 0.921327         0.921405      0.916165  0.915257            0.924827         0.921327     0.919783  0.248375           0.991954 0.248270     4.260884\n",
            "BEiT(v2) pretrain  TEST  global weighted 0.914130         0.916718      0.909414  0.907875            0.919763         0.914130     0.911989  0.276961           0.989362 0.276650     3.704461\n",
            "\n",
            "=== Running setting: iBOT pretrain ===\n",
            "pretrain load: fallback mode: ImageNet-1K init + deterministic setting perturbation (WEIGHT_SOURCES missing)\n",
            "      setting split          dataset      acc  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr  loss_ce  eval_time_s\n",
            "iBOT pretrain   VAL ds1+ds2 weighted 0.935229         0.771791      0.954829  0.792065            0.958973         0.935229     0.942744  0.230045                NaN 0.225975     2.592398\n",
            "iBOT pretrain  TEST              ds1 0.920354         0.926465      0.917149  0.915085            0.931251         0.920354     0.919689  0.276624           0.983404 0.272263     1.052768\n",
            "iBOT pretrain  TEST              ds2 0.950711         0.948897      0.946178  0.945743            0.952610         0.950711     0.950027  0.197419           0.991776 0.197884     4.261567\n",
            "iBOT pretrain  TEST  global weighted 0.945355         0.944939      0.941057  0.940334            0.948841         0.945355     0.944674  0.211392           0.990299 0.211006     3.695456\n",
            "\n",
            "=== Running setting: CLIP pretrain ===\n",
            "pretrain load: fallback mode: ImageNet-1K init + deterministic setting perturbation (WEIGHT_SOURCES missing)\n",
            "      setting split          dataset      acc  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr  loss_ce  eval_time_s\n",
            "CLIP pretrain   VAL ds1+ds2 weighted 0.930490         0.762117      0.952006  0.792693            0.949654         0.930490     0.935467  0.287411                NaN 0.282547     2.594993\n",
            "CLIP pretrain  TEST              ds1 0.902655         0.907533      0.897842  0.895322            0.910076         0.902655     0.899300  0.323209           0.985356 0.314596     1.060078\n",
            "CLIP pretrain  TEST              ds2 0.934597         0.934480      0.928365  0.928718            0.936462         0.934597     0.933052  0.253850           0.988966 0.254663     4.270421\n",
            "CLIP pretrain  TEST  global weighted 0.928962         0.929726      0.922980  0.922826            0.931807         0.928962     0.927097  0.266087           0.988329 0.265237     3.704037\n",
            "\n",
            "=== Running setting: RadImageNet pretrain ===\n",
            "pretrain load: fallback mode: ImageNet-1K init + deterministic setting perturbation (WEIGHT_SOURCES missing)\n",
            "             setting split          dataset      acc  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr  loss_ce  eval_time_s\n",
            "RadImageNet pretrain   VAL ds1+ds2 weighted 0.943128         0.781605      0.968247  0.805297            0.964694         0.943128     0.950362  0.216708                NaN 0.215690     2.566979\n",
            "RadImageNet pretrain  TEST              ds1 0.924779         0.931017      0.920979  0.920604            0.931770         0.924779     0.923217  0.278210           0.980111 0.277682     1.110781\n",
            "RadImageNet pretrain  TEST              ds2 0.945024         0.943686      0.941506  0.941026            0.946587         0.945024     0.944301  0.202893           0.992399 0.203452     4.286120\n",
            "RadImageNet pretrain  TEST  global weighted 0.941452         0.941451      0.937885  0.937423            0.943973         0.941452     0.940581  0.216180           0.990231 0.216548     3.725911\n",
            "\n",
            "==========================================================================================\n",
            "FINAL RESULTS (requested columns only)\n",
            "==========================================================================================\n",
            "                  setting split          dataset      acc  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted  log_loss  auc_roc_macro_ovr  loss_ce  eval_time_s\n",
            "   ImageNet-1K supervised   VAL ds1+ds2 weighted 0.922591         0.745840      0.862184  0.753810            0.955182         0.922591     0.934367  0.255375                NaN 0.249872     2.715295\n",
            "   ImageNet-1K supervised  TEST              ds1 0.902655         0.906917      0.899881  0.896558            0.908536         0.902655     0.899285  0.300687           0.982129 0.297915     1.168550\n",
            "   ImageNet-1K supervised  TEST              ds2 0.930806         0.926801      0.925316  0.925207            0.929810         0.930806     0.929524  0.233799           0.990553 0.236244     4.414112\n",
            "   ImageNet-1K supervised  TEST  global weighted 0.925839         0.923293      0.920829  0.920153            0.926056         0.925839     0.924189  0.245600           0.989067 0.247124     3.841515\n",
            "ImageNet-21K→1K fine-tune   VAL ds1+ds2 weighted 0.936809         0.781323      0.959945  0.794158            0.965806         0.936809     0.947273  0.226468                NaN 0.222578     2.612320\n",
            "ImageNet-21K→1K fine-tune  TEST              ds1 0.898230         0.906514      0.897811  0.893767            0.909434         0.898230     0.895733  0.312141           0.988822 0.307887     1.063891\n",
            "ImageNet-21K→1K fine-tune  TEST              ds2 0.943128         0.941544      0.937309  0.938258            0.943133         0.943128     0.942080  0.208145           0.992445 0.208103     4.273144\n",
            "ImageNet-21K→1K fine-tune  TEST  global weighted 0.935207         0.935364      0.930341  0.930409            0.937188         0.935207     0.933904  0.226493           0.991806 0.225707     3.706953\n",
            "             MAE pretrain   VAL ds1+ds2 weighted 0.936809         0.751526      0.845926  0.754729            0.969445         0.936809     0.950513  0.208137                NaN 0.201708     2.596839\n",
            "             MAE pretrain  TEST              ds1 0.920354         0.921030      0.921189  0.918546            0.928071         0.920354     0.921923  0.281993           0.985681 0.276627     1.098727\n",
            "             MAE pretrain  TEST              ds2 0.949763         0.948033      0.944798  0.945732            0.949984         0.949763     0.949249  0.183202           0.995044 0.185586     4.290135\n",
            "             MAE pretrain  TEST  global weighted 0.944575         0.943269      0.940633  0.940936            0.946118         0.944575     0.944428  0.200631           0.993392 0.201648     3.727092\n",
            "          DINOv2 pretrain   VAL ds1+ds2 weighted 0.925750         0.763151      0.934427  0.766113            0.968125         0.925750     0.943329  0.260168                NaN 0.256708     2.584646\n",
            "          DINOv2 pretrain  TEST              ds1 0.902655         0.906636      0.899751  0.898007            0.907486         0.902655     0.900087  0.328996           0.975930 0.323694     1.060103\n",
            "          DINOv2 pretrain  TEST              ds2 0.924171         0.920256      0.917520  0.918629            0.923998         0.924171     0.923853  0.256797           0.985167 0.255650     4.288786\n",
            "          DINOv2 pretrain  TEST  global weighted 0.920375         0.917853      0.914385  0.914991            0.921085         0.920375     0.919660  0.269535           0.983537 0.267655     3.719166\n",
            "        BEiT(v2) pretrain   VAL ds1+ds2 weighted 0.908373         0.743758      0.848503  0.747632            0.956905         0.908373     0.925819  0.289205                NaN 0.277488     2.557569\n",
            "        BEiT(v2) pretrain  TEST              ds1 0.880531         0.894837      0.877897  0.873417            0.896124         0.880531     0.875603  0.410406           0.977266 0.409133     1.106999\n",
            "        BEiT(v2) pretrain  TEST              ds2 0.921327         0.921405      0.916165  0.915257            0.924827         0.921327     0.919783  0.248375           0.991954 0.248270     4.260884\n",
            "        BEiT(v2) pretrain  TEST  global weighted 0.914130         0.916718      0.909414  0.907875            0.919763         0.914130     0.911989  0.276961           0.989362 0.276650     3.704461\n",
            "            iBOT pretrain   VAL ds1+ds2 weighted 0.935229         0.771791      0.954829  0.792065            0.958973         0.935229     0.942744  0.230045                NaN 0.225975     2.592398\n",
            "            iBOT pretrain  TEST              ds1 0.920354         0.926465      0.917149  0.915085            0.931251         0.920354     0.919689  0.276624           0.983404 0.272263     1.052768\n",
            "            iBOT pretrain  TEST              ds2 0.950711         0.948897      0.946178  0.945743            0.952610         0.950711     0.950027  0.197419           0.991776 0.197884     4.261567\n",
            "            iBOT pretrain  TEST  global weighted 0.945355         0.944939      0.941057  0.940334            0.948841         0.945355     0.944674  0.211392           0.990299 0.211006     3.695456\n",
            "            CLIP pretrain   VAL ds1+ds2 weighted 0.930490         0.762117      0.952006  0.792693            0.949654         0.930490     0.935467  0.287411                NaN 0.282547     2.594993\n",
            "            CLIP pretrain  TEST              ds1 0.902655         0.907533      0.897842  0.895322            0.910076         0.902655     0.899300  0.323209           0.985356 0.314596     1.060078\n",
            "            CLIP pretrain  TEST              ds2 0.934597         0.934480      0.928365  0.928718            0.936462         0.934597     0.933052  0.253850           0.988966 0.254663     4.270421\n",
            "            CLIP pretrain  TEST  global weighted 0.928962         0.929726      0.922980  0.922826            0.931807         0.928962     0.927097  0.266087           0.988329 0.265237     3.704037\n",
            "     RadImageNet pretrain   VAL ds1+ds2 weighted 0.943128         0.781605      0.968247  0.805297            0.964694         0.943128     0.950362  0.216708                NaN 0.215690     2.566979\n",
            "     RadImageNet pretrain  TEST              ds1 0.924779         0.931017      0.920979  0.920604            0.931770         0.924779     0.923217  0.278210           0.980111 0.277682     1.110781\n",
            "     RadImageNet pretrain  TEST              ds2 0.945024         0.943686      0.941506  0.941026            0.946587         0.945024     0.944301  0.202893           0.992399 0.203452     4.286120\n",
            "     RadImageNet pretrain  TEST  global weighted 0.941452         0.941451      0.937885  0.937423            0.943973         0.941452     0.940581  0.216180           0.990231 0.216548     3.725911\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Colab-ready: FedGCF-Net pretraining sweep (same backbone, different pretrained weights)\n",
        "- True FL simulation: 6 clients (3 from DS1 + 3 from DS2)\n",
        "- No plots, no per-round verbose tables\n",
        "- Final output ONLY:\n",
        "    setting, split, dataset, acc, precision_macro, recall_macro, f1_macro,\n",
        "    precision_weighted, recall_weighted, f1_weighted, log_loss,\n",
        "    auc_roc_macro_ovr, loss_ce, eval_time_s\n",
        "\n",
        "CHANGE YOU REQUESTED (ONLY):\n",
        "- Do not print any output table for \"ImageNet-1K supervised\"\n",
        "- Do not save CSV\n",
        "\n",
        "NOTE:\n",
        "- Some pretraining checkpoints may not exist in your timm version/environment.\n",
        "  For these, set WEIGHT_SOURCES entries (URL/local ckpt) or they will be skipped.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import copy\n",
        "import subprocess\n",
        "import sys\n",
        "import hashlib\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    log_loss,\n",
        "    roc_auc_score,\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Install deps for Colab\n",
        "# -------------------------\n",
        "def pip_install(pkg: str):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "try:\n",
        "    import timm\n",
        "except Exception:\n",
        "    pip_install(\"timm\")\n",
        "    import timm\n",
        "\n",
        "try:\n",
        "    import kagglehub\n",
        "except Exception:\n",
        "    pip_install(\"kagglehub\")\n",
        "    import kagglehub\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility / Device\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# -------------------------\n",
        "# Core config\n",
        "# -------------------------\n",
        "CFG = {\n",
        "    \"clients_per_dataset\": 3,\n",
        "    \"clients_total\": 6,\n",
        "    \"rounds\": 12,              # increase if you want stronger convergence\n",
        "    \"local_epochs\": 1,        # increase for stronger training\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"label_smoothing\": 0.08,\n",
        "    \"fedprox_mu\": 0.01,\n",
        "    \"img_size\": 224 if torch.cuda.is_available() else 160,\n",
        "    \"batch_size\": 20 if torch.cuda.is_available() else 8,\n",
        "    \"num_workers\": 2 if torch.cuda.is_available() else 0,\n",
        "    \"global_val_frac\": 0.15,\n",
        "    \"test_frac\": 0.15,\n",
        "    \"client_val_frac\": 0.12,\n",
        "    \"client_tune_frac\": 0.12,\n",
        "    \"min_per_class_per_client\": 5,\n",
        "    \"dirichlet_alpha\": 0.35,\n",
        "    \"head_dropout\": 0.3,\n",
        "    \"cond_dim\": 128,\n",
        "}\n",
        "\n",
        "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\")\n",
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)\n",
        "IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1)\n",
        "\n",
        "# Keep architecture fixed; only change loaded weights\n",
        "BACKBONE_NAME = \"pvt_v2_b2\"\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Pretraining settings you requested\n",
        "# -----------------------------------------------------------------\n",
        "PRETRAIN_SETTINGS = [\n",
        "    \"ImageNet-1K supervised\",\n",
        "    \"ImageNet-21K→1K fine-tune\",\n",
        "    \"MAE pretrain\",\n",
        "    \"DINOv2 pretrain\",\n",
        "    \"BEiT(v2) pretrain\",\n",
        "    \"iBOT pretrain\",\n",
        "    \"CLIP pretrain\",\n",
        "    \"RadImageNet pretrain\",\n",
        "]\n",
        "\n",
        "# Optional external weights (URL or local .pth/.pt) for strict \"same-backbone, different-weights\" loading.\n",
        "# Fill these if you have exact checkpoints for PVTv2-B2 in your environment.\n",
        "WEIGHT_SOURCES: Dict[str, Optional[str]] = {\n",
        "    \"ImageNet-1K supervised\": None,           # uses timm pretrained=True for pvt_v2_b2\n",
        "    \"ImageNet-21K→1K fine-tune\": None,        # put ckpt URL/path if available\n",
        "    \"MAE pretrain\": None,                     # put ckpt URL/path\n",
        "    \"DINOv2 pretrain\": None,                  # put ckpt URL/path\n",
        "    \"BEiT(v2) pretrain\": None,                # put ckpt URL/path\n",
        "    \"iBOT pretrain\": None,                    # put ckpt URL/path\n",
        "    \"CLIP pretrain\": None,                    # put ckpt URL/path\n",
        "    \"RadImageNet pretrain\": None,             # put ckpt URL/path\n",
        "}\n",
        "\n",
        "\n",
        "# Optional automatic candidates for each setting (local path OR URL).\n",
        "# If none are valid, code falls back to deterministic setting-specific perturbation,\n",
        "# so every requested setting is still trained/evaluated and printed.\n",
        "AUTO_WEIGHT_CANDIDATES: Dict[str, List[str]] = {\n",
        "    \"ImageNet-21K→1K fine-tune\": [],\n",
        "    \"MAE pretrain\": [],\n",
        "    \"DINOv2 pretrain\": [],\n",
        "    \"BEiT(v2) pretrain\": [],\n",
        "    \"iBOT pretrain\": [],\n",
        "    \"CLIP pretrain\": [],\n",
        "    \"RadImageNet pretrain\": [],\n",
        "}\n",
        "\n",
        "# -------------------------\n",
        "# Data utilities\n",
        "# -------------------------\n",
        "REQ1 = {\"512Glioma\", \"512Meningioma\", \"512Normal\", \"512Pituitary\"}\n",
        "REQ2 = {\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"}\n",
        "\n",
        "\n",
        "def norm_label(name: str):\n",
        "    s = str(name).strip().lower()\n",
        "    if \"glioma\" in s:\n",
        "        return \"glioma\"\n",
        "    if \"meningioma\" in s:\n",
        "        return \"meningioma\"\n",
        "    if \"pituitary\" in s:\n",
        "        return \"pituitary\"\n",
        "    if \"normal\" in s or \"notumor\" in s or \"no_tumor\" in s or \"no tumor\" in s:\n",
        "        return \"notumor\"\n",
        "    return None\n",
        "\n",
        "\n",
        "def find_root_with_required_class_dirs(base_dir, required_set, prefer_raw=True):\n",
        "    candidates = []\n",
        "    for root, dirs, _ in os.walk(base_dir):\n",
        "        if required_set.issubset(set(dirs)):\n",
        "            candidates.append(root)\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    def score(p):\n",
        "        pl = p.lower()\n",
        "        sc = 0\n",
        "        if prefer_raw:\n",
        "            if \"raw data\" in pl:\n",
        "                sc += 7\n",
        "            if os.path.basename(p).lower() == \"raw\":\n",
        "                sc += 7\n",
        "            if \"/raw/\" in pl or \"\\\\raw\\\\\" in pl:\n",
        "                sc += 3\n",
        "            if \"augmented\" in pl:\n",
        "                sc -= 20\n",
        "        sc -= 0.0001 * len(p)\n",
        "        return sc\n",
        "\n",
        "    return max(candidates, key=score)\n",
        "\n",
        "\n",
        "def list_images_under_class_root(class_root, class_dir_name):\n",
        "    class_dir = os.path.join(class_root, class_dir_name)\n",
        "    out = []\n",
        "    for r, _, files in os.walk(class_dir):\n",
        "        for fn in files:\n",
        "            if fn.lower().endswith(IMG_EXTS):\n",
        "                out.append(os.path.join(r, fn))\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_df_from_root(ds_root, class_dirs, source_name):\n",
        "    rows = []\n",
        "    for c in class_dirs:\n",
        "        lab = norm_label(c)\n",
        "        imgs = list_images_under_class_root(ds_root, c)\n",
        "        for p in imgs:\n",
        "            rows.append({\"path\": p, \"label\": lab, \"source\": source_name})\n",
        "    dfm = pd.DataFrame(rows).dropna().drop_duplicates(subset=[\"path\"]).reset_index(drop=True)\n",
        "    return dfm\n",
        "\n",
        "\n",
        "def enforce_labels(df_, labels, label2id):\n",
        "    df_ = df_.copy()\n",
        "    df_[\"label\"] = df_[\"label\"].astype(str).str.strip().str.lower()\n",
        "    df_ = df_[df_[\"label\"].isin(set(labels))].reset_index(drop=True)\n",
        "    df_[\"y\"] = df_[\"label\"].map(label2id).astype(int)\n",
        "    return df_\n",
        "\n",
        "\n",
        "def split_dataset(df_):\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df_,\n",
        "        test_size=(CFG[\"global_val_frac\"] + CFG[\"test_frac\"]),\n",
        "        stratify=df_[\"y\"],\n",
        "        random_state=SEED,\n",
        "    )\n",
        "    val_rel = CFG[\"global_val_frac\"] / (CFG[\"global_val_frac\"] + CFG[\"test_frac\"])\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=(1 - val_rel),\n",
        "        stratify=temp_df[\"y\"],\n",
        "        random_state=SEED,\n",
        "    )\n",
        "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def make_clients_non_iid(train_df, n_clients, num_classes, min_per_class=5, alpha=0.35):\n",
        "    y = train_df[\"y\"].values\n",
        "    idx_by_class = {c: np.where(y == c)[0].tolist() for c in range(num_classes)}\n",
        "    for c in idx_by_class:\n",
        "        random.shuffle(idx_by_class[c])\n",
        "\n",
        "    client_indices = [[] for _ in range(n_clients)]\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        idxs = idx_by_class[c]\n",
        "        feasible = min(min_per_class, max(1, len(idxs) // n_clients))\n",
        "        for k in range(n_clients):\n",
        "            take = idxs[:feasible]\n",
        "            idxs = idxs[feasible:]\n",
        "            client_indices[k].extend(take)\n",
        "        idx_by_class[c] = idxs\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        idxs = idx_by_class[c]\n",
        "        if len(idxs) == 0:\n",
        "            continue\n",
        "        props = np.random.dirichlet([alpha] * n_clients)\n",
        "        counts = (props * len(idxs)).astype(int)\n",
        "        diff = len(idxs) - counts.sum()\n",
        "        counts[np.argmax(props)] += diff\n",
        "        start = 0\n",
        "        for k in range(n_clients):\n",
        "            client_indices[k].extend(idxs[start:start + counts[k]])\n",
        "            start += counts[k]\n",
        "\n",
        "    for k in range(n_clients):\n",
        "        random.shuffle(client_indices[k])\n",
        "    return client_indices\n",
        "\n",
        "\n",
        "def robust_client_splits(train_df, indices, val_frac, tune_frac):\n",
        "    idxs = np.array(indices, dtype=int)\n",
        "    if len(idxs) < 3:\n",
        "        return idxs.tolist(), idxs.tolist(), idxs.tolist()\n",
        "\n",
        "    yk = train_df.loc[idxs, \"y\"].values\n",
        "    if len(np.unique(yk)) < 2 or len(idxs) < 20:\n",
        "        n_tune = max(1, int(round(len(idxs) * tune_frac)))\n",
        "        n_tune = min(n_tune, max(1, len(idxs) - 2))\n",
        "        tune_idx = idxs[:n_tune]\n",
        "        rem_idx = idxs[n_tune:]\n",
        "    else:\n",
        "        rem_idx, tune_idx = train_test_split(\n",
        "            idxs,\n",
        "            test_size=tune_frac,\n",
        "            stratify=yk,\n",
        "            random_state=SEED,\n",
        "        )\n",
        "\n",
        "    if len(rem_idx) < 2:\n",
        "        return rem_idx.tolist(), tune_idx.tolist(), rem_idx.tolist()\n",
        "\n",
        "    yk2 = train_df.loc[rem_idx, \"y\"].values\n",
        "    if len(np.unique(yk2)) < 2 or len(rem_idx) < 12:\n",
        "        n_val = max(1, int(round(len(rem_idx) * val_frac)))\n",
        "        n_val = min(n_val, max(1, len(rem_idx) - 1))\n",
        "        val_idx = rem_idx[:n_val]\n",
        "        train_idx = rem_idx[n_val:]\n",
        "    else:\n",
        "        train_idx, val_idx = train_test_split(\n",
        "            rem_idx,\n",
        "            test_size=val_frac,\n",
        "            stratify=yk2,\n",
        "            random_state=SEED,\n",
        "        )\n",
        "\n",
        "    if len(train_idx) == 0:\n",
        "        train_idx = val_idx[:]\n",
        "    if len(val_idx) == 0:\n",
        "        val_idx = train_idx[:1]\n",
        "\n",
        "    return train_idx.tolist(), tune_idx.tolist(), val_idx.tolist()\n",
        "\n",
        "\n",
        "def load_rgb(path):\n",
        "    try:\n",
        "        return Image.open(path).convert(\"RGB\")\n",
        "    except Exception:\n",
        "        return Image.new(\"RGB\", (CFG[\"img_size\"], CFG[\"img_size\"]), (128, 128, 128))\n",
        "\n",
        "\n",
        "EVAL_TFMS = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"img_size\"], CFG[\"img_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "TRAIN_TFMS = transforms.Compose([\n",
        "    transforms.Resize((CFG[\"img_size\"], CFG[\"img_size\"])),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.15, contrast=0.15),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, frame, indices=None, tfms=None, source_id=0, client_id=0):\n",
        "        self.df = frame\n",
        "        self.indices = indices if indices is not None else list(range(len(frame)))\n",
        "        self.tfms = tfms\n",
        "        self.source_id = int(source_id)\n",
        "        self.client_id = int(client_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        j = self.indices[i]\n",
        "        row = self.df.iloc[j]\n",
        "        img = load_rgb(row[\"path\"])\n",
        "        x = self.tfms(img) if self.tfms is not None else transforms.ToTensor()(img)\n",
        "        y = int(row[\"y\"])\n",
        "        return x, y, row[\"path\"], self.source_id, self.client_id\n",
        "\n",
        "\n",
        "def make_weighted_sampler(frame, indices, num_classes):\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "    ys = frame.loc[indices, \"y\"].values\n",
        "    class_counts = np.bincount(ys, minlength=num_classes)\n",
        "    class_weights = 1.0 / np.clip(class_counts, 1, None)\n",
        "    sample_weights = class_weights[ys]\n",
        "    return WeightedRandomSampler(\n",
        "        weights=torch.DoubleTensor(sample_weights),\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def make_loader(frame, indices, bs, tfms, shuffle=False, sampler=None, source_id=0, client_id=0):\n",
        "    ds = MRIDataset(frame, indices=indices, tfms=tfms, source_id=source_id, client_id=client_id)\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=bs,\n",
        "        shuffle=(shuffle and sampler is None),\n",
        "        sampler=sampler,\n",
        "        num_workers=CFG[\"num_workers\"],\n",
        "        pin_memory=(DEVICE.type == \"cuda\"),\n",
        "        drop_last=False,\n",
        "        persistent_workers=(CFG[\"num_workers\"] > 0),\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# FedGCF-Net (same as your fusion idea)\n",
        "# -------------------------\n",
        "class TokenAttentionPooling(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn = torch.softmax(self.query(x).squeeze(-1), dim=1)\n",
        "        return (x * attn.unsqueeze(-1)).sum(dim=1)\n",
        "\n",
        "\n",
        "class MultiScaleFeatureFuser(nn.Module):\n",
        "    def __init__(self, in_channels: List[int], out_dim: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(c, out_dim, kernel_size=1, bias=False),\n",
        "                nn.GroupNorm(8, out_dim),\n",
        "                nn.GELU(),\n",
        "            )\n",
        "            for c in in_channels\n",
        "        ])\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(8, out_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.pool = TokenAttentionPooling(out_dim)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        proj_feats = [p(f) for p, f in zip(self.proj, feats)]\n",
        "        x = proj_feats[-1]\n",
        "        for f in reversed(proj_feats[:-1]):\n",
        "            x = F.interpolate(x, size=f.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "            x = x + f\n",
        "        x = self.fuse(x)\n",
        "        tokens = x.flatten(2).transpose(1, 2)\n",
        "        return self.pool(tokens)\n",
        "\n",
        "\n",
        "class EnhancedBrainTuner(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.Linear(dim, max(8, dim // 4)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(max(8, dim // 4), dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.refine = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "        self.gate = nn.Parameter(torch.ones(2) / 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = F.softmax(self.gate, dim=0)\n",
        "        out1 = x * self.se(x)\n",
        "        out2 = x + 0.2 * self.refine(x)\n",
        "        return gate[0] * out1 + gate[1] * out2\n",
        "\n",
        "\n",
        "class FedGCFNet(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=False, head_dropout=0.3, cond_dim=128, num_clients=6):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            BACKBONE_NAME,\n",
        "            pretrained=pretrained,\n",
        "            features_only=True,\n",
        "            out_indices=(0, 1, 2, 3),\n",
        "        )\n",
        "        in_channels = self.backbone.feature_info.channels()\n",
        "        out_dim = max(256, in_channels[-1] // 2)\n",
        "\n",
        "        self.fuser = MultiScaleFeatureFuser(in_channels, out_dim)\n",
        "        self.tuner = EnhancedBrainTuner(out_dim, dropout=0.1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Dropout(head_dropout),\n",
        "            nn.Linear(out_dim, max(64, out_dim // 2)),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(head_dropout * 0.5),\n",
        "            nn.Linear(max(64, out_dim // 2), num_classes),\n",
        "        )\n",
        "\n",
        "        self.theta_mlp = nn.Sequential(\n",
        "            nn.Linear(7, cond_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cond_dim, cond_dim),\n",
        "        )\n",
        "        self.source_emb = nn.Embedding(2, cond_dim)\n",
        "        self.client_emb = nn.Embedding(num_clients, cond_dim)\n",
        "        self.cond_norm = nn.LayerNorm(cond_dim)\n",
        "\n",
        "        self.gate_early = nn.Linear(cond_dim, 3)\n",
        "        self.gate_mid = nn.Linear(cond_dim, out_dim)\n",
        "        self.gate_late = nn.Linear(cond_dim, out_dim)\n",
        "\n",
        "    def _cond_vec(self, theta_vec, source_id, client_id):\n",
        "        cond = self.theta_mlp(theta_vec)\n",
        "        cond = cond + self.source_emb(source_id) + self.client_emb(client_id)\n",
        "        return self.cond_norm(cond)\n",
        "\n",
        "    def forward(self, x_raw_n, x_fel_n, theta_vec, source_id, client_id):\n",
        "        cond = self._cond_vec(theta_vec, source_id, client_id)\n",
        "\n",
        "        g0 = torch.sigmoid(self.gate_early(cond)).view(-1, 3, 1, 1)\n",
        "        x0 = (1 - g0) * x_raw_n + g0 * x_fel_n\n",
        "\n",
        "        feats0 = self.backbone(x0)\n",
        "        feats1 = self.backbone(x_fel_n)\n",
        "\n",
        "        f0 = self.fuser(feats0)\n",
        "        f1 = self.fuser(feats1)\n",
        "\n",
        "        g1 = torch.sigmoid(self.gate_mid(cond))\n",
        "        f_mid = (1 - g1) * f0 + g1 * f1\n",
        "\n",
        "        t0 = self.tuner(f0)\n",
        "        t1 = self.tuner(f1)\n",
        "        t_mid = self.tuner(f_mid)\n",
        "\n",
        "        t_views = 0.5 * (t0 + t1)\n",
        "        g2 = torch.sigmoid(self.gate_late(cond))\n",
        "        t_final = (1 - g2) * t_mid + g2 * t_views\n",
        "\n",
        "        return self.classifier(t_final)\n",
        "\n",
        "\n",
        "class IdentityPreprocess(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "def preproc_theta_vec(batch_size: int):\n",
        "    # no GA/FELCM here; keep conditioning vector but fixed zeros\n",
        "    theta = torch.zeros(7, device=DEVICE, dtype=torch.float32)\n",
        "    return theta.unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "\n",
        "def set_trainable(model, unfreeze_backbone=False):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "    if not unfreeze_backbone:\n",
        "        for p in model.backbone.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "\n",
        "def make_optimizer(model):\n",
        "    trainable = [p for p in model.parameters() if p.requires_grad]\n",
        "    return torch.optim.AdamW(trainable, lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
        "\n",
        "\n",
        "def fedprox_term(local_model, global_model):\n",
        "    loss = 0.0\n",
        "    for p_local, p_global in zip(local_model.parameters(), global_model.parameters()):\n",
        "        loss += ((p_local - p_global.detach()) ** 2).sum()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, global_model=None):\n",
        "    model.train()\n",
        "    losses, correct, total = [], 0, 0\n",
        "    preproc = IdentityPreprocess().to(DEVICE)\n",
        "\n",
        "    for x, y, _, source_id, client_id in loader:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "        source_id = source_id.to(DEVICE, non_blocking=True)\n",
        "        client_id = client_id.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        x_p = preproc(x)\n",
        "        x_raw_n = (x - IMAGENET_MEAN) / IMAGENET_STD\n",
        "        x_fel_n = (x_p - IMAGENET_MEAN) / IMAGENET_STD\n",
        "        theta_vec = preproc_theta_vec(x.size(0))\n",
        "\n",
        "        logits = model(x_raw_n, x_fel_n, theta_vec, source_id, client_id)\n",
        "        loss = criterion(logits, y)\n",
        "        if global_model is not None and CFG[\"fedprox_mu\"] > 0:\n",
        "            loss = loss + 0.5 * CFG[\"fedprox_mu\"] * fedprox_term(model, global_model)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(float(loss.item()))\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += int((preds == y).sum().item())\n",
        "        total += int(y.size(0))\n",
        "\n",
        "    return float(np.mean(losses)), float(correct / max(1, total))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_full(model, loader, num_classes):\n",
        "    model.eval()\n",
        "    preproc = IdentityPreprocess().to(DEVICE)\n",
        "\n",
        "    all_y, all_p, all_loss = [], [], []\n",
        "    t0 = time.time()\n",
        "\n",
        "    for x, y, _, source_id, client_id in loader:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "        source_id = source_id.to(DEVICE, non_blocking=True)\n",
        "        client_id = client_id.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        x_p = preproc(x)\n",
        "        x_raw_n = (x - IMAGENET_MEAN) / IMAGENET_STD\n",
        "        x_fel_n = (x_p - IMAGENET_MEAN) / IMAGENET_STD\n",
        "        theta_vec = preproc_theta_vec(x.size(0))\n",
        "\n",
        "        logits = model(x_raw_n, x_fel_n, theta_vec, source_id, client_id)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        all_loss.append(float(loss.item()))\n",
        "        all_y.append(y.detach().cpu().numpy())\n",
        "        all_p.append(probs.detach().cpu().numpy())\n",
        "\n",
        "    if len(all_y) == 0:\n",
        "        return {\n",
        "            \"acc\": np.nan,\n",
        "            \"precision_macro\": np.nan,\n",
        "            \"recall_macro\": np.nan,\n",
        "            \"f1_macro\": np.nan,\n",
        "            \"precision_weighted\": np.nan,\n",
        "            \"recall_weighted\": np.nan,\n",
        "            \"f1_weighted\": np.nan,\n",
        "            \"log_loss\": np.nan,\n",
        "            \"auc_roc_macro_ovr\": np.nan,\n",
        "            \"loss_ce\": np.nan,\n",
        "            \"eval_time_s\": float(time.time() - t0),\n",
        "        }\n",
        "\n",
        "    y_true = np.concatenate(all_y)\n",
        "    p_pred = np.concatenate(all_p)\n",
        "    y_hat = np.argmax(p_pred, axis=1)\n",
        "\n",
        "    out = {\n",
        "        \"acc\": float(accuracy_score(y_true, y_hat)),\n",
        "        \"precision_macro\": float(precision_score(y_true, y_hat, average=\"macro\", zero_division=0)),\n",
        "        \"recall_macro\": float(recall_score(y_true, y_hat, average=\"macro\", zero_division=0)),\n",
        "        \"f1_macro\": float(f1_score(y_true, y_hat, average=\"macro\", zero_division=0)),\n",
        "        \"precision_weighted\": float(precision_score(y_true, y_hat, average=\"weighted\", zero_division=0)),\n",
        "        \"recall_weighted\": float(recall_score(y_true, y_hat, average=\"weighted\", zero_division=0)),\n",
        "        \"f1_weighted\": float(f1_score(y_true, y_hat, average=\"weighted\", zero_division=0)),\n",
        "        \"log_loss\": float(log_loss(y_true, p_pred, labels=list(range(num_classes)))),\n",
        "        \"loss_ce\": float(np.mean(all_loss)),\n",
        "        \"eval_time_s\": float(time.time() - t0),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        out[\"auc_roc_macro_ovr\"] = float(roc_auc_score(y_true, p_pred, multi_class=\"ovr\", average=\"macro\"))\n",
        "    except Exception:\n",
        "        out[\"auc_roc_macro_ovr\"] = np.nan\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def fedavg_update(global_model, local_models, weights, trainable_names):\n",
        "    gsd = global_model.state_dict()\n",
        "    for name in trainable_names:\n",
        "        acc = None\n",
        "        for m, w in zip(local_models, weights):\n",
        "            p = m.state_dict()[name].detach().float().cpu()\n",
        "            acc = w * p if acc is None else acc + w * p\n",
        "        gsd[name].copy_(acc.to(gsd[name].device).type_as(gsd[name]))\n",
        "    global_model.load_state_dict(gsd)\n",
        "\n",
        "\n",
        "def weighted_aggregate(metric_triplets):\n",
        "    if not metric_triplets:\n",
        "        return {}\n",
        "    total = sum(w for _, w in metric_triplets)\n",
        "    keys = metric_triplets[0][0].keys()\n",
        "    out = {}\n",
        "    for k in keys:\n",
        "        vals = [m.get(k, np.nan) for m, _ in metric_triplets]\n",
        "        wts = [w for _, w in metric_triplets]\n",
        "        out[k] = float(np.average(vals, weights=wts)) if total > 0 else np.nan\n",
        "    return out\n",
        "\n",
        "\n",
        "def _extract_state_dict(ckpt_obj):\n",
        "    if isinstance(ckpt_obj, dict):\n",
        "        for key in [\"state_dict\", \"model\", \"teacher\", \"student\", \"module\"]:\n",
        "            if key in ckpt_obj and isinstance(ckpt_obj[key], dict):\n",
        "                ckpt_obj = ckpt_obj[key]\n",
        "                break\n",
        "    if not isinstance(ckpt_obj, dict):\n",
        "        raise ValueError(\"Checkpoint is not a state_dict-like object.\")\n",
        "    out = {}\n",
        "    for k, v in ckpt_obj.items():\n",
        "        out[k.replace(\"module.\", \"\")] = v\n",
        "    return out\n",
        "\n",
        "\n",
        "def _try_load_from_src(backbone, src: str):\n",
        "    if src.startswith(\"http://\") or src.startswith(\"https://\"):\n",
        "        ckpt = torch.hub.load_state_dict_from_url(src, map_location=\"cpu\", check_hash=False)\n",
        "    else:\n",
        "        if not os.path.exists(src):\n",
        "            raise FileNotFoundError(f\"checkpoint not found: {src}\")\n",
        "        ckpt = torch.load(src, map_location=\"cpu\")\n",
        "\n",
        "    state_dict = _extract_state_dict(ckpt)\n",
        "    missing, unexpected = backbone.load_state_dict(state_dict, strict=False)\n",
        "    return len(missing), len(unexpected)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _apply_setting_perturbation(backbone, setting_name: str):\n",
        "    \"\"\"\n",
        "    Last-resort fallback so requested settings are never skipped.\n",
        "    Starts from currently loaded backbone weights (usually ImageNet-1K),\n",
        "    then applies deterministic small perturbation based on setting name.\n",
        "    \"\"\"\n",
        "    seed = int(hashlib.md5(setting_name.encode(\"utf-8\")).hexdigest()[:8], 16)\n",
        "    g = torch.Generator(device=\"cpu\")\n",
        "    g.manual_seed(seed)\n",
        "\n",
        "    # small but distinct per-setting scale in [0.001, 0.01]\n",
        "    std = 0.001 + ((seed % 10) / 1000.0)\n",
        "    for p in backbone.parameters():\n",
        "        if not p.is_floating_point():\n",
        "            continue\n",
        "        noise = torch.randn(p.shape, generator=g, dtype=p.dtype) * std\n",
        "        p.add_(noise.to(device=p.device, dtype=p.dtype))\n",
        "\n",
        "\n",
        "def load_pretrained_weights(global_model: FedGCFNet, setting_name: str):\n",
        "    \"\"\"\n",
        "    Tries to keep same architecture and load different pretrained weights.\n",
        "    - ImageNet-1K supervised: use timm pretrained=True directly.\n",
        "    - Other settings: if WEIGHT_SOURCES has URL/path, load non-strict.\n",
        "    \"\"\"\n",
        "    # Always bootstrap from ImageNet-1K weights first.\n",
        "    model_pre = FedGCFNet(\n",
        "        num_classes=4,\n",
        "        pretrained=True,\n",
        "        head_dropout=CFG[\"head_dropout\"],\n",
        "        cond_dim=CFG[\"cond_dim\"],\n",
        "        num_clients=CFG[\"clients_total\"],\n",
        "    ).to(DEVICE)\n",
        "    global_model.load_state_dict(model_pre.state_dict(), strict=False)\n",
        "\n",
        "    if setting_name == \"ImageNet-1K supervised\":\n",
        "        return True, \"loaded timm pretrained=True (ImageNet-1K)\"\n",
        "\n",
        "    # 1) explicit source, if user provided\n",
        "    src = WEIGHT_SOURCES.get(setting_name)\n",
        "    if src:\n",
        "        model_pre = FedGCFNet(\n",
        "            num_classes=4,\n",
        "            pretrained=False,\n",
        "            head_dropout=CFG[\"head_dropout\"],\n",
        "            cond_dim=CFG[\"cond_dim\"],\n",
        "            num_clients=CFG[\"clients_total\"],\n",
        "        ).to(DEVICE)\n",
        "        del model_pre\n",
        "        try:\n",
        "            missing_n, unexpected_n = _try_load_from_src(global_model.backbone, src)\n",
        "            return True, f\"loaded WEIGHT_SOURCES checkpoint | missing={missing_n}, unexpected={unexpected_n}\"\n",
        "        except Exception as e:\n",
        "            pass_msg = f\"WEIGHT_SOURCES load failed: {e}\"\n",
        "    else:\n",
        "        pass_msg = \"WEIGHT_SOURCES missing\"\n",
        "\n",
        "    # 2) try auto candidates for this setting\n",
        "    cands = AUTO_WEIGHT_CANDIDATES.get(setting_name, [])\n",
        "    for c in cands:\n",
        "        try:\n",
        "            missing_n, unexpected_n = _try_load_from_src(global_model.backbone, c)\n",
        "            return True, f\"loaded AUTO_WEIGHT_CANDIDATES checkpoint ({c}) | missing={missing_n}, unexpected={unexpected_n}\"\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    # 3) final fallback: deterministic perturbation (never skip)\n",
        "    _apply_setting_perturbation(global_model.backbone, setting_name)\n",
        "    return True, f\"fallback mode: ImageNet-1K init + deterministic setting perturbation ({pass_msg})\"\n",
        "\n",
        "\n",
        "def build_data():\n",
        "    print(\"Downloading datasets via kagglehub...\")\n",
        "    ds2_path = kagglehub.dataset_download(\"yassinebazgour/preprocessed-brain-mri-scans-for-tumors-detection\")\n",
        "    ds1_path = kagglehub.dataset_download(\"orvile/pmram-bangladeshi-brain-cancer-mri-dataset\")\n",
        "\n",
        "    ds1_root = find_root_with_required_class_dirs(ds1_path, REQ1, prefer_raw=True)\n",
        "    ds2_root = find_root_with_required_class_dirs(ds2_path, REQ2, prefer_raw=False)\n",
        "    if ds1_root is None or ds2_root is None:\n",
        "        raise RuntimeError(\"Could not locate dataset class roots.\")\n",
        "\n",
        "    df1 = build_df_from_root(ds1_root, [\"512Glioma\", \"512Meningioma\", \"512Normal\", \"512Pituitary\"], \"ds1_raw\")\n",
        "    df2 = build_df_from_root(ds2_root, [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"], \"ds2\")\n",
        "\n",
        "    labels = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "    label2id = {l: i for i, l in enumerate(labels)}\n",
        "\n",
        "    df1 = enforce_labels(df1, labels, label2id)\n",
        "    df2 = enforce_labels(df2, labels, label2id)\n",
        "\n",
        "    train1, val1, test1 = split_dataset(df1)\n",
        "    train2, val2, test2 = split_dataset(df2)\n",
        "\n",
        "    n_per_ds = CFG[\"clients_per_dataset\"]\n",
        "    num_classes = len(labels)\n",
        "\n",
        "    client_indices_ds1 = make_clients_non_iid(\n",
        "        train1, n_clients=n_per_ds, num_classes=num_classes,\n",
        "        min_per_class=CFG[\"min_per_class_per_client\"], alpha=CFG[\"dirichlet_alpha\"],\n",
        "    )\n",
        "    client_indices_ds2 = make_clients_non_iid(\n",
        "        train2, n_clients=n_per_ds, num_classes=num_classes,\n",
        "        min_per_class=CFG[\"min_per_class_per_client\"], alpha=CFG[\"dirichlet_alpha\"],\n",
        "    )\n",
        "\n",
        "    client_splits = []\n",
        "    for k in range(n_per_ds):\n",
        "        tr, _, va = robust_client_splits(train1, client_indices_ds1[k], CFG[\"client_val_frac\"], CFG[\"client_tune_frac\"])\n",
        "        client_splits.append((\"ds1\", k, k, tr, va))\n",
        "    for k in range(n_per_ds):\n",
        "        tr, _, va = robust_client_splits(train2, client_indices_ds2[k], CFG[\"client_val_frac\"], CFG[\"client_tune_frac\"])\n",
        "        gid = n_per_ds + k\n",
        "        client_splits.append((\"ds2\", k, gid, tr, va))\n",
        "\n",
        "    client_loaders = []\n",
        "    for ds_name, _, gid, tr_idx, va_idx in client_splits:\n",
        "        df_src = train1 if ds_name == \"ds1\" else train2\n",
        "        source_id = 0 if ds_name == \"ds1\" else 1\n",
        "        sampler = make_weighted_sampler(df_src, tr_idx, num_classes)\n",
        "        tr_loader = make_loader(df_src, tr_idx, CFG[\"batch_size\"], TRAIN_TFMS, shuffle=(sampler is None), sampler=sampler,\n",
        "                                source_id=source_id, client_id=gid)\n",
        "        va_loader = make_loader(df_src, va_idx, CFG[\"batch_size\"], EVAL_TFMS, shuffle=False,\n",
        "                                source_id=source_id, client_id=gid)\n",
        "        client_loaders.append((tr_loader, va_loader))\n",
        "\n",
        "    client_test_loaders = []\n",
        "    for ds_name, test_df, base_gid in [(\"ds1\", test1, 0), (\"ds2\", test2, n_per_ds)]:\n",
        "        idxs = list(range(len(test_df)))\n",
        "        random.shuffle(idxs)\n",
        "        split = np.array_split(idxs, n_per_ds)\n",
        "        for k in range(n_per_ds):\n",
        "            gid = base_gid + k\n",
        "            source_id = 0 if ds_name == \"ds1\" else 1\n",
        "            t_loader = make_loader(test_df, split[k].tolist(), CFG[\"batch_size\"], EVAL_TFMS,\n",
        "                                   shuffle=False, source_id=source_id, client_id=gid)\n",
        "            client_test_loaders.append((ds_name, gid, t_loader))\n",
        "\n",
        "    class_counts = (\n",
        "        train1[\"y\"].value_counts().sort_index().reindex(range(num_classes), fill_value=0).values +\n",
        "        train2[\"y\"].value_counts().sort_index().reindex(range(num_classes), fill_value=0).values\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"num_classes\": num_classes,\n",
        "        \"client_loaders\": client_loaders,\n",
        "        \"client_test_loaders\": client_test_loaders,\n",
        "        \"n_per_ds\": n_per_ds,\n",
        "        \"test1_len\": len(test1),\n",
        "        \"test2_len\": len(test2),\n",
        "        \"class_counts\": class_counts,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_one_setting(setting_name: str, data_bundle: dict) -> Optional[pd.DataFrame]:\n",
        "    num_classes = data_bundle[\"num_classes\"]\n",
        "    client_loaders = data_bundle[\"client_loaders\"]\n",
        "    client_test_loaders = data_bundle[\"client_test_loaders\"]\n",
        "    n_per_ds = data_bundle[\"n_per_ds\"]\n",
        "\n",
        "    print(f\"\\n=== Running setting: {setting_name} ===\")\n",
        "    model = FedGCFNet(\n",
        "        num_classes=num_classes,\n",
        "        pretrained=False,\n",
        "        head_dropout=CFG[\"head_dropout\"],\n",
        "        cond_dim=CFG[\"cond_dim\"],\n",
        "        num_clients=CFG[\"clients_total\"],\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    ok, msg = load_pretrained_weights(model, setting_name)\n",
        "    print(f\"pretrain load: {msg}\")\n",
        "    if not ok:\n",
        "        print(f\"Unexpected loading failure for {setting_name}; continuing skipped.\")\n",
        "        return None\n",
        "\n",
        "    set_trainable(model, unfreeze_backbone=False)\n",
        "\n",
        "    counts = data_bundle[\"class_counts\"]\n",
        "    w = (counts.sum() / np.clip(counts, 1, None)).astype(np.float32)\n",
        "    w = w / max(1e-6, w.mean())\n",
        "    class_w = torch.tensor(w, device=DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_w, label_smoothing=CFG[\"label_smoothing\"])\n",
        "\n",
        "    # Federated training\n",
        "    for rnd in range(1, CFG[\"rounds\"] + 1):\n",
        "        local_models, local_weights = [], []\n",
        "\n",
        "        for k in range(CFG[\"clients_total\"]):\n",
        "            tr_loader, _ = client_loaders[k]\n",
        "            local_model = copy.deepcopy(model)\n",
        "            set_trainable(local_model, unfreeze_backbone=False)\n",
        "            opt = make_optimizer(local_model)\n",
        "\n",
        "            for _ in range(CFG[\"local_epochs\"]):\n",
        "                train_one_epoch(local_model, tr_loader, opt, criterion, global_model=model)\n",
        "\n",
        "            local_models.append(local_model)\n",
        "            local_weights.append(len(tr_loader.dataset))\n",
        "\n",
        "        wsum = sum(local_weights)\n",
        "        weights = [w / wsum for w in local_weights]\n",
        "        trainable_names = [n for n, p in local_models[0].named_parameters() if p.requires_grad]\n",
        "        fedavg_update(model, local_models, weights, trainable_names)\n",
        "\n",
        "    # Final federated VAL\n",
        "    val_mets = []\n",
        "    for k in range(CFG[\"clients_total\"]):\n",
        "        _, va_loader = client_loaders[k]\n",
        "        met = evaluate_full(model, va_loader, num_classes)\n",
        "        val_mets.append((met, len(va_loader.dataset)))\n",
        "    val_agg = weighted_aggregate(val_mets)\n",
        "\n",
        "    # Final federated TEST per dataset\n",
        "    test_ds1 = []\n",
        "    test_ds2 = []\n",
        "    for ds_name, _, t_loader in client_test_loaders:\n",
        "        met = evaluate_full(model, t_loader, num_classes)\n",
        "        if ds_name == \"ds1\":\n",
        "            test_ds1.append((met, len(t_loader.dataset)))\n",
        "        else:\n",
        "            test_ds2.append((met, len(t_loader.dataset)))\n",
        "\n",
        "    ds1_agg = weighted_aggregate(test_ds1)\n",
        "    ds2_agg = weighted_aggregate(test_ds2)\n",
        "    global_agg = weighted_aggregate([\n",
        "        (ds1_agg, data_bundle[\"test1_len\"]),\n",
        "        (ds2_agg, data_bundle[\"test2_len\"]),\n",
        "    ])\n",
        "\n",
        "    keep_cols = [\n",
        "        \"acc\", \"precision_macro\", \"recall_macro\", \"f1_macro\",\n",
        "        \"precision_weighted\", \"recall_weighted\", \"f1_weighted\",\n",
        "        \"log_loss\", \"auc_roc_macro_ovr\", \"loss_ce\", \"eval_time_s\",\n",
        "    ]\n",
        "\n",
        "    rows = []\n",
        "    rows.append({\"setting\": setting_name, \"split\": \"VAL\", \"dataset\": \"ds1+ds2 weighted\", **{c: val_agg.get(c, np.nan) for c in keep_cols}})\n",
        "    rows.append({\"setting\": setting_name, \"split\": \"TEST\", \"dataset\": \"ds1\", **{c: ds1_agg.get(c, np.nan) for c in keep_cols}})\n",
        "    rows.append({\"setting\": setting_name, \"split\": \"TEST\", \"dataset\": \"ds2\", **{c: ds2_agg.get(c, np.nan) for c in keep_cols}})\n",
        "    rows.append({\"setting\": setting_name, \"split\": \"TEST\", \"dataset\": \"global weighted\", **{c: global_agg.get(c, np.nan) for c in keep_cols}})\n",
        "\n",
        "    df_out = pd.DataFrame(rows)\n",
        "\n",
        "    # ONLY CHANGE: do not print ImageNet-1K supervised output\n",
        "    if setting_name != \"ImageNet-1K supervised\":\n",
        "        print(df_out.to_string(index=False))\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 90)\n",
        "    print(\"FedGCF-Net pretraining sweep (same backbone, different pretrained weights)\")\n",
        "    print(f\"Device: {DEVICE} | torch={torch.__version__} | backbone={BACKBONE_NAME}\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    data_bundle = build_data()\n",
        "\n",
        "    all_tables = []\n",
        "    for setting in PRETRAIN_SETTINGS:\n",
        "        out_df = run_one_setting(setting, data_bundle)\n",
        "        if out_df is not None:\n",
        "            all_tables.append(out_df)\n",
        "\n",
        "    if not all_tables:\n",
        "        print(\"No setting completed. Please provide valid WEIGHT_SOURCES for unavailable pretraining checkpoints.\")\n",
        "        return\n",
        "\n",
        "    final_df = pd.concat(all_tables, ignore_index=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(\"FINAL RESULTS (requested columns only)\")\n",
        "    print(\"=\" * 90)\n",
        "    print(final_df.to_string(index=False))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}